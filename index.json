{
  "doc/EFCore.PG/README.html": {
    "href": "doc/EFCore.PG/README.html",
    "title": "Npgsql Entity Framework Core provider for PostgreSQL | Npgsql Documentation",
    "keywords": "Npgsql Entity Framework Core provider for PostgreSQL Npgsql.EntityFrameworkCore.PostgreSQL is the open source EF Core provider for PostgreSQL. It allows you to interact with PostgreSQL via the most widely-used .NET O/RM from Microsoft, and use familiar LINQ syntax to express queries. It's built on top of Npgsql. The provider looks and feels just like any other Entity Framework Core provider. Here's a quick sample to get you started: await using var ctx = new BlogContext(); await ctx.Database.EnsureDeletedAsync(); await ctx.Database.EnsureCreatedAsync(); // Insert a Blog ctx.Blogs.Add(new() { Name = \"FooBlog\" }); await ctx.SaveChangesAsync(); // Query all blogs who's name starts with F var fBlogs = await ctx.Blogs.Where(b => b.Name.StartsWith(\"F\")).ToListAsync(); public class BlogContext : DbContext { public DbSet<Blog> Blogs { get; set; } protected override void OnConfiguring(DbContextOptionsBuilder optionsBuilder) => optionsBuilder.UseNpgsql(@\"Host=myserver;Username=mylogin;Password=mypass;Database=mydatabase\"); } public class Blog { public int Id { get; set; } public string Name { get; set; } } Aside from providing general EF Core support for PostgreSQL, the provider also exposes some PostgreSQL-specific capabilities, allowing you to query JSON, array or range columns, as well as many other advanced features. For more information, see the the Npgsql site. For information about EF Core in general, see the EF Core website. Related packages Spatial plugin to work with PostgreSQL PostGIS: Npgsql.EntityFrameworkCore.PostgreSQL.NetTopologySuite NodaTime plugin to use better date/time types with PostgreSQL: Npgsql.EntityFrameworkCore.PostgreSQL.NodaTime The underlying Npgsql ADO.NET provider is Npgsql."
  },
  "doc/EFCore.PG/src/EFCore.PG.NTS/README.html": {
    "href": "doc/EFCore.PG/src/EFCore.PG.NTS/README.html",
    "title": "Npgsql Entity Framework Core provider for PostgreSQL | Npgsql Documentation",
    "keywords": "Npgsql Entity Framework Core provider for PostgreSQL Npgsql.EntityFrameworkCore.PostgreSQL is the open source EF Core provider for PostgreSQL. It allows you to interact with PostgreSQL via the most widely-used .NET O/RM from Microsoft, and use familiar LINQ syntax to express queries. This package is a plugin which allows you to interact with spatial data provided by the PostgreSQL PostGIS extension; PostGIS is a mature, standard extension considered to provide top-of-the-line database spatial features. On the .NET side, the plugin adds support for the types from the NetTopologySuite library, allowing you to read and write them directly to PostgreSQL. To use the plugin, simply add UseNetTopologySuite as below and use NetTopologySuite types in your entity properties: await using var ctx = new BlogContext(); await ctx.Database.EnsureDeletedAsync(); await ctx.Database.EnsureCreatedAsync(); // Insert a Blog ctx.Cities.Add(new() { Name = \"FooCity\", Center = new Point(10, 10) }); await ctx.SaveChangesAsync(); // Query all cities with the given center point var newBlogs = await ctx.Cities.Where(b => b.Center == new Point(10, 10)).ToListAsync(); public class BlogContext : DbContext { public DbSet<City> Cities { get; set; } protected override void OnConfiguring(DbContextOptionsBuilder optionsBuilder) => optionsBuilder.UseNpgsql( @\"Host=myserver;Username=mylogin;Password=mypass;Database=mydatabase\", o => o.UseNetTopologySuite()); } public class City { public int Id { get; set; } public string Name { get; set; } public Point Center { get; set; } } The plugin also supports translating many NetTopologySuite methods and properties into corresponding PostGIS operations. For more information, see the NetTopologySuite plugin documentation page."
  },
  "doc/EFCore.PG/src/EFCore.PG.NodaTime/README.html": {
    "href": "doc/EFCore.PG/src/EFCore.PG.NodaTime/README.html",
    "title": "Npgsql Entity Framework Core provider for PostgreSQL | Npgsql Documentation",
    "keywords": "Npgsql Entity Framework Core provider for PostgreSQL Npgsql.EntityFrameworkCore.PostgreSQL is the open source EF Core provider for PostgreSQL. It allows you to interact with PostgreSQL via the most widely-used .NET O/RM from Microsoft, and use familiar LINQ syntax to express queries. This package is a plugin which allows you to use the NodaTime date/time library when interacting with PostgreSQL; this provides a better and safer API for dealing with date and time data. To use the plugin, simply add UseNodaTime as below and use NodaTime types in your entity properties: await using var ctx = new BlogContext(); await ctx.Database.EnsureDeletedAsync(); await ctx.Database.EnsureCreatedAsync(); // Insert a Blog ctx.Blogs.Add(new() { Name = \"FooBlog\", CreationTime = SystemClock.Instance.GetCurrentInstant() }); await ctx.SaveChangesAsync(); // Query all blogs created in 2020 or after var newBlogs = await ctx.Blogs.Where(b => b.CreationTime >= Instant.FromUtc(2020, 1, 1, 0, 0, 0)).ToListAsync(); public class BlogContext : DbContext { public DbSet<Blog> Blogs { get; set; } protected override void OnConfiguring(DbContextOptionsBuilder optionsBuilder) => optionsBuilder.UseNpgsql( @\"Host=myserver;Username=mylogin;Password=mypass;Database=mydatabase\", o => o.UseNodaTime()); } public class Blog { public int Id { get; set; } public string Name { get; set; } public Instant CreationTime { get; set; } } The plugin also supports translating most NodaTime methods and properties into corresponding PostgreSQL date/time operations. For more information, see the NodaTime plugin documentation page."
  },
  "doc/EFCore.PG/src/EFCore.PG/README.html": {
    "href": "doc/EFCore.PG/src/EFCore.PG/README.html",
    "title": "Npgsql Entity Framework Core provider for PostgreSQL | Npgsql Documentation",
    "keywords": "Npgsql Entity Framework Core provider for PostgreSQL Npgsql.EntityFrameworkCore.PostgreSQL is the open source EF Core provider for PostgreSQL. It allows you to interact with PostgreSQL via the most widely-used .NET O/RM from Microsoft, and use familiar LINQ syntax to express queries. It's built on top of Npgsql. The provider looks and feels just like any other Entity Framework Core provider. Here's a quick sample to get you started: await using var ctx = new BlogContext(); await ctx.Database.EnsureDeletedAsync(); await ctx.Database.EnsureCreatedAsync(); // Insert a Blog ctx.Blogs.Add(new() { Name = \"FooBlog\" }); await ctx.SaveChangesAsync(); // Query all blogs who's name starts with F var fBlogs = await ctx.Blogs.Where(b => b.Name.StartsWith(\"F\")).ToListAsync(); public class BlogContext : DbContext { public DbSet<Blog> Blogs { get; set; } protected override void OnConfiguring(DbContextOptionsBuilder optionsBuilder) => optionsBuilder.UseNpgsql(@\"Host=myserver;Username=mylogin;Password=mypass;Database=mydatabase\"); } public class Blog { public int Id { get; set; } public string Name { get; set; } } Aside from providing general EF Core support for PostgreSQL, the provider also exposes some PostgreSQL-specific capabilities, allowing you to query JSON, array or range columns, as well as many other advanced features. For more information, see the the Npgsql site. For information about EF Core in general, see the EF Core website."
  },
  "doc/Npgsql/README.html": {
    "href": "doc/Npgsql/README.html",
    "title": "Npgsql - the .NET data provider for PostgreSQL | Npgsql Documentation",
    "keywords": "Npgsql - the .NET data provider for PostgreSQL What is Npgsql? Npgsql is the open source .NET data provider for PostgreSQL. It allows you to connect and interact with PostgreSQL server using .NET. For the full documentation, please visit the Npgsql website. For the Entity Framework Core provider that works with this provider, see Npgsql.EntityFrameworkCore.PostgreSQL. Quickstart Here's a basic code snippet to get you started: using Npgsql; var connString = \"Host=myserver;Username=mylogin;Password=mypass;Database=mydatabase\"; var dataSourceBuilder = new NpgsqlDataSourceBuilder(connString); var dataSource = dataSourceBuilder.Build(); var conn = await dataSource.OpenConnectionAsync(); // Insert some data await using (var cmd = new NpgsqlCommand(\"INSERT INTO data (some_field) VALUES (@p)\", conn)) { cmd.Parameters.AddWithValue(\"p\", \"Hello world\"); await cmd.ExecuteNonQueryAsync(); } // Retrieve all rows await using (var cmd = new NpgsqlCommand(\"SELECT some_field FROM data\", conn)) await using (var reader = await cmd.ExecuteReaderAsync()) { while (await reader.ReadAsync()) Console.WriteLine(reader.GetString(0)); } Key features High-performance PostgreSQL driver. Regularly figures in the top contenders on the TechEmpower Web Framework Benchmarks. Full support of most PostgreSQL types, including advanced ones such as arrays, enums, ranges, multiranges, composites, JSON, PostGIS and others. Highly-efficient bulk import/export API. Failover, load balancing and general multi-host support. Great integration with Entity Framework Core via Npgsql.EntityFrameworkCore.PostgreSQL. For the full documentation, please visit the Npgsql website at https://www.npgsql.org."
  },
  "doc/Npgsql/src/Npgsql.DependencyInjection/README.html": {
    "href": "doc/Npgsql/src/Npgsql.DependencyInjection/README.html",
    "title": "| Npgsql Documentation",
    "keywords": "Npgsql is the open source .NET data provider for PostgreSQL. It allows you to connect and interact with PostgreSQL server using .NET. This package helps set up Npgsql in applications using dependency injection, notably ASP.NET applications. It allows easy configuration of your Npgsql connections and registers the appropriate services in your DI container. For example, if using the ASP.NET minimal web API, simply use the following to register Npgsql: var builder = WebApplication.CreateBuilder(args); builder.Services.AddNpgsqlDataSource(\"Host=pg_server;Username=test;Password=test;Database=test\"); This registers a transient NpgsqlConnection which can get injected into your controllers: app.MapGet(\"/\", async (NpgsqlConnection connection) => { await connection.OpenAsync(); await using var command = new NpgsqlCommand(\"SELECT number FROM data LIMIT 1\", connection); return \"Hello World: \" + await command.ExecuteScalarAsync(); }); But wait! If all you want is to execute some simple SQL, just use the singleton NpgsqlDataSource to execute a command directly: app.MapGet(\"/\", async (NpgsqlDataSource dataSource) => { await using var command = dataSource.CreateCommand(\"SELECT number FROM data LIMIT 1\"); return \"Hello World: \" + await command.ExecuteScalarAsync(); }); NpgsqlDataSource can also come in handy when you need more than one connection: app.MapGet(\"/\", async (NpgsqlDataSource dataSource) => { await using var connection1 = await dataSource.OpenConnectionAsync(); await using var connection2 = await dataSource.OpenConnectionAsync(); // Use the two connections... }); Finally, the AddNpgsqlDataSource method also accepts a lambda parameter allowing you to configure aspects of Npgsql beyond the connection string, e.g. to configure UseLoggerFactory and UseNetTopologySuite: var builder = WebApplication.CreateBuilder(args); builder.Services.AddNpgsqlDataSource( \"Host=pg_server;Username=test;Password=test;Database=test\", builder => builder .UseLoggerFactory(loggerFactory) .UseNetTopologySuite()); For more information, see the Npgsql documentation."
  },
  "doc/Npgsql/src/Npgsql.NetTopologySuite/README.html": {
    "href": "doc/Npgsql/src/Npgsql.NetTopologySuite/README.html",
    "title": "| Npgsql Documentation",
    "keywords": "Npgsql is the open source .NET data provider for PostgreSQL. It allows you to connect and interact with PostgreSQL server using .NET. This package is an Npgsql plugin which allows you to interact with spatial data provided by the PostgreSQL PostGIS extension; PostGIS is a mature, standard extension considered to provide top-of-the-line database spatial features. On the .NET side, the plugin adds support for the types from the NetTopologySuite library, allowing you to read and write them directly to PostgreSQL. To use the NetTopologySuite plugin, add a dependency on this package and create a NpgsqlDataSource. using Npgsql; using NetTopologySuite.Geometries; var dataSourceBuilder = new NpgsqlDataSourceBuilder(ConnectionString); dataSourceBuilder.UseNetTopologySuite(); var dataSource = dataSourceBuilder.Build(); var conn = await dataSource.OpenConnectionAsync(); var point = new Point(new Coordinate(1d, 1d)); conn.ExecuteNonQuery(\"CREATE TEMP TABLE data (geom GEOMETRY)\"); using (var cmd = new NpgsqlCommand(\"INSERT INTO data (geom) VALUES (@p)\", conn)) { cmd.Parameters.AddWithValue(\"@p\", point); cmd.ExecuteNonQuery(); } using (var cmd = new NpgsqlCommand(\"SELECT geom FROM data\", conn)) using (var reader = cmd.ExecuteReader()) { reader.Read(); Assert.That(reader[0], Is.EqualTo(point)); } For more information, visit the NetTopologySuite plugin documentation page."
  },
  "doc/Npgsql/src/Npgsql.NodaTime/README.html": {
    "href": "doc/Npgsql/src/Npgsql.NodaTime/README.html",
    "title": "| Npgsql Documentation",
    "keywords": "Npgsql is the open source .NET data provider for PostgreSQL. It allows you to connect and interact with PostgreSQL server using .NET. This package is an Npgsql plugin which allows you to use the NodaTime date/time library when interacting with PostgreSQL; this provides a better and safer API for dealing with date and time data. To use the NodaTime plugin, add a dependency on this package and create a NpgsqlDataSource. Once this is done, you can use NodaTime types when interacting with PostgreSQL, just as you would use e.g. DateTime: using Npgsql; var dataSourceBuilder = new NpgsqlDataSourceBuilder(ConnectionString); dataSourceBuilder.UseNodaTime(); var dataSource = dataSourceBuilder.Build(); var conn = await dataSource.OpenConnectionAsync(); // Write NodaTime Instant to PostgreSQL \"timestamp with time zone\" (UTC) using (var cmd = new NpgsqlCommand(@\"INSERT INTO mytable (my_timestamptz) VALUES (@p)\", conn)) { cmd.Parameters.Add(new NpgsqlParameter(\"p\", Instant.FromUtc(2011, 1, 1, 10, 30))); cmd.ExecuteNonQuery(); } // Read timestamp back from the database as an Instant using (var cmd = new NpgsqlCommand(@\"SELECT my_timestamptz FROM mytable\", conn)) using (var reader = cmd.ExecuteReader()) { reader.Read(); var instant = reader.GetFieldValue<Instant>(0); } For more information, visit the NodaTime plugin documentation page."
  },
  "doc/Npgsql/src/Npgsql.OpenTelemetry/README.html": {
    "href": "doc/Npgsql/src/Npgsql.OpenTelemetry/README.html",
    "title": "| Npgsql Documentation",
    "keywords": "Npgsql is the open source .NET data provider for PostgreSQL. It allows you to connect and interact with PostgreSQL server using .NET. This package helps set up Npgsql's support for OpenTelemetry tracing, which allows you to observe database commands as they are being executed. You can drop the following code snippet in your application's startup, and you should start seeing tracing information on the console: using var tracerProvider = Sdk.CreateTracerProviderBuilder() .SetResourceBuilder(ResourceBuilder.CreateDefault().AddService(\"npgsql-tester\")) .SetSampler(new AlwaysOnSampler()) // This optional activates tracing for your application, if you trace your own activities: .AddSource(\"MyApp\") // This activates up Npgsql's tracing: .AddNpgsql() // This prints tracing data to the console: .AddConsoleExporter() .Build(); Once this is done, you should start seeing Npgsql trace data appearing in your application's console. At this point, you can look into exporting your trace data to a more useful destination: systems such as Zipkin or Jaeger can efficiently collect and store your data, and provide user interfaces for querying and exploring it. For more information, visit the diagnostics documentation page."
  },
  "doc/Npgsql/src/Npgsql.SourceGenerators/AnalyzerReleases.Shipped.html": {
    "href": "doc/Npgsql/src/Npgsql.SourceGenerators/AnalyzerReleases.Shipped.html",
    "title": "| Npgsql Documentation",
    "keywords": ""
  },
  "doc/Npgsql/src/Npgsql.SourceGenerators/AnalyzerReleases.Unshipped.html": {
    "href": "doc/Npgsql/src/Npgsql.SourceGenerators/AnalyzerReleases.Unshipped.html",
    "title": "| Npgsql Documentation",
    "keywords": "New Rules Rule ID Category Severity Notes PGXXXX Internal Error"
  },
  "doc/Npgsql/src/Npgsql/README.html": {
    "href": "doc/Npgsql/src/Npgsql/README.html",
    "title": "| Npgsql Documentation",
    "keywords": "Npgsql is the open source .NET data provider for PostgreSQL. It allows you to connect and interact with PostgreSQL server using .NET. Quickstart Here's a basic code snippet to get you started: var connString = \"Host=myserver;Username=mylogin;Password=mypass;Database=mydatabase\"; await using var conn = new NpgsqlConnection(connString); await conn.OpenAsync(); // Insert some data await using (var cmd = new NpgsqlCommand(\"INSERT INTO data (some_field) VALUES (@p)\", conn)) { cmd.Parameters.AddWithValue(\"p\", \"Hello world\"); await cmd.ExecuteNonQueryAsync(); } // Retrieve all rows await using (var cmd = new NpgsqlCommand(\"SELECT some_field FROM data\", conn)) await using (var reader = await cmd.ExecuteReaderAsync()) { while (await reader.ReadAsync()) Console.WriteLine(reader.GetString(0)); } Key features High-performance PostgreSQL driver. Regularly figures in the top contenders on the TechEmpower Web Framework Benchmarks. Full support of most PostgreSQL types, including advanced ones such as arrays, enums, ranges, multiranges, composites, JSON, PostGIS and others. Highly-efficient bulk import/export API. Failover, load balancing and general multi-host support. Great integration with Entity Framework Core via Npgsql.EntityFrameworkCore.PostgreSQL. For the full documentation, please visit the Npgsql website. Related packages The Entity Framework Core provider that works with this provider is Npgsql.EntityFrameworkCore.PostgreSQL. Spatial plugin to work with PostgreSQL PostGIS: Npgsql.NetTopologySuite NodaTime plugin to use better date/time types with PostgreSQL: Npgsql.NodaTime OpenTelemetry support can be set up with Npgsql.OpenTelemetry"
  },
  "doc/README.html": {
    "href": "doc/README.html",
    "title": "| Npgsql Documentation",
    "keywords": "This is the documentation repo for Npgsql. It contains conceptual documentation articles for Npgsql, Npgsql.EntityFrameworkCore.PostgreSQL (AKA EFCore.PG) and EntityFramework6.Npgsql (AKA EF6.PG). Note that to properly work, docfx expects to also find the Npgsql and EFCore.PG repos cloned in the repo root - it extracts API documentation from them. A Github Actions workflow automatically clones the appropriate repository, rebuilds the entire documentation and pushes the results to live."
  },
  "doc/conceptual/EF6.PG/index.html": {
    "href": "doc/conceptual/EF6.PG/index.html",
    "title": "Entity Framework 6 | Npgsql Documentation",
    "keywords": "Npgsql has an Entity Framework 6 provider. You can use it by installing the EntityFramework6.Npgsql nuget. Basic Configuration Configuration for an Entity Framework application can be specified in a config file (app.config/web.config) or through code. The latter is known as code-based configuration. Code-based To use Entity Framework with Npgsql, define a class that inherits from DbConfiguration in the same assembly as your class inheriting DbContext. Ensure that you configure provider services, a provider factory, a default connection factory as shown below: using Npgsql; using System.Data.Entity; class NpgSqlConfiguration : DbConfiguration { public NpgSqlConfiguration() { var name = \"Npgsql\"; SetProviderFactory(providerInvariantName: name, providerFactory: NpgsqlFactory.Instance); SetProviderServices(providerInvariantName: name, provider: NpgsqlServices.Instance); SetDefaultConnectionFactory(connectionFactory: new NpgsqlConnectionFactory()); } } Config file When installing EntityFramework6.Npgsql nuget package, the relevant sections in App.config / Web.config are usually automatically updated. You typically only have to add your connectionString with the correct providerName. <configuration> <connectionStrings> <add name=\"BlogDbContext\" connectionString=\"Server=localhost;port=5432;Database=Blog;User Id=postgres;Password=postgres;\" providerName=\"Npgsql\" /> </connectionStrings> <entityFramework> <providers> <provider invariantName=\"Npgsql\" type=\"Npgsql.NpgsqlServices, EntityFramework6.Npgsql\" /> </providers> <!-- setting the default connection factory is optional --> <defaultConnectionFactory type=\"Npgsql.NpgsqlConnectionFactory, EntityFramework6.Npgsql\" /> </entityFramework> <system.data> <DbProviderFactories> <add name=\"Npgsql Provider\" invariant=\"Npgsql\" description=\".NET Framework Data Provider for PostgreSQL\" type=\"Npgsql.NpgsqlFactory, Npgsql, Version=4.1.3.0, Culture=neutral, PublicKeyToken=5d8b90d52f46fda7\" /> </DbProviderFactories> </system.data> </configuration> Guid Support Npgsql EF migrations support uses uuid_generate_v4() function to generate guids. In order to have access to this function, you have to install the extension uuid-ossp through the following command: create extension \"uuid-ossp\"; If you don't have this extension installed, when you run Npgsql migrations you will get the following error message: ERROR: function uuid_generate_v4() does not exist If the database is being created by Npgsql Migrations, you will need to run the create extension command in the template1 database. This way, when the new database is created, the extension will be installed already. Optimistic Concurrency EntityFramework supports optimistic concurrency, through the system column xmin. To use this column as the concurrency token, some customization is needed. The following code will setup Department.Version to map to xmin, while the SqlGenerator will generate CREATE/ALTER TABLE statements omitting system columns. public class Department { public string Version { get; private set; } } [DbConfigurationType(typeof(Configuration))] public class UniversityDbContext : DbContext { public DbSet<Department> Departments { get; set; } protected override void OnModelCreating(DbModelBuilder modelBuilder) { modelBuilder.Entity<Department>() .Property(p => p.Version) .HasColumnName(\"xmin\") .HasColumnType(\"text\") .IsConcurrencyToken() .HasDatabaseGeneratedOption(DatabaseGeneratedOption.Computed); base.OnModelCreating(modelBuilder); } } internal class Configuration : DbConfiguration { public Configuration() { SetMigrationSqlGenerator(\"Npgsql\", () => new SqlGenerator()); } } public class SqlGenerator : NpgsqlMigrationSqlGenerator { private readonly string[] systemColumnNames = { \"oid\", \"tableoid\", \"xmin\", \"cmin\", \"xmax\", \"cmax\", \"ctid\" }; protected override void Convert(CreateTableOperation createTableOperation) { var systemColumns = createTableOperation.Columns.Where(x => systemColumnNames.Contains(x.Name)).ToArray(); foreach (var systemColumn in systemColumns) createTableOperation.Columns.Remove(systemColumn); base.Convert(createTableOperation); } } Template Database When the Entity Framework 6 provider creates a database, it issues a simple CREATE DATABASE command. In PostgreSQL, this implicitly uses template1 as the template - anything existing in template1 will be copied to your new database. If you wish to change the database used as a template, you can specify the EF Template Database connection string parameter. For more info see the PostgreSQL docs. Customizing DataReader Behavior You can use an Entity Framework 6 IDbCommandInterceptor to wrap the DataReader instance returned by Npgsql when Entity Framework executes queries. This is possible using a DbConfiguration class. Example use cases: Forcing all returned DateTime and DateTimeOffset values to be in the UTC timezone. Preventing accidental insertion of DateTime values having DateTimeKind.Unspecified. Forcing all postgres date/time types to be returned to Entity Framework as DateTimeOffset. [DbConfigurationType(typeof(AppDbContextConfiguration))] public class AppDbContext : DbContext { // ... } public class AppDbContextConfiguration : DbConfiguration { public AppDbContextConfiguration() { this.AddInterceptor(new MyEntityFrameworkInterceptor()); } } class MyEntityFrameworkInterceptor : DbCommandInterceptor { public override void ReaderExecuted( DbCommand command, DbCommandInterceptionContext<DbDataReader> interceptionContext) { if (interceptionContext.Result == null) return; interceptionContext.Result = new WrappingDbDataReader(interceptionContext.Result); } public override void ScalarExecuted( DbCommand command, DbCommandInterceptionContext<object> interceptionContext) { interceptionContext.Result = ModifyReturnValues(interceptionContext.Result); } static object ModifyReturnValues(object result) { // Transform and then return result; } } class WrappingDbDataReader : DbDataReader, IDataReader { // Wrap an existing DbDataReader, proxy all calls to the underlying instance, // modify return values and/or parameters as needed... public WrappingDbDataReader(DbDataReader reader) { } }"
  },
  "doc/conceptual/EFCore.PG/index.html": {
    "href": "doc/conceptual/EFCore.PG/index.html",
    "title": "Npgsql Entity Framework Core Provider | Npgsql Documentation",
    "keywords": "Npgsql Entity Framework Core Provider Npgsql has an Entity Framework (EF) Core provider. It behaves like other EF Core providers (e.g. SQL Server), so the general EF Core docs apply here as well. If you're just getting started with EF Core, those docs are the best place to start. Development happens in the Npgsql.EntityFrameworkCore.PostgreSQL repository, all issues should be reported there. Configuring the project file To use the Npgsql EF Core provider, add a dependency on Npgsql.EntityFrameworkCore.PostgreSQL. You can follow the instructions in the general EF Core Getting Started docs. Below is a .csproj file for a console application that uses the Npgsql EF Core provider: <Project Sdk=\"Microsoft.NET.Sdk\"> <PropertyGroup> <TargetFramework>net8.0</TargetFramework> </PropertyGroup> <ItemGroup> <PackageReference Include=\"Npgsql.EntityFrameworkCore.PostgreSQL\" Version=\"8.0.4\" /> </ItemGroup> </Project> Defining a model and a DbContext Let's say you want to store blogs and their posts in their database; you can model these as .NET types as follows: public class Blog { public int BlogId { get; set; } public string Url { get; set; } public List<Post> Posts { get; set; } } public class Post { public int PostId { get; set; } public string Title { get; set; } public string Content { get; set; } public int BlogId { get; set; } public Blog Blog { get; set; } } You then define a DbContext type which you'll use to interact with the database: OnConfiguring DbContext pooling ASP.NET / DI Using OnConfiguring() to configure your context is the easiest way to get started, but is discouraged for most production applications: public class BloggingContext : DbContext { public DbSet<Blog> Blogs { get; set; } public DbSet<Post> Posts { get; set; } protected override void OnConfiguring(DbContextOptionsBuilder optionsBuilder) => optionsBuilder.UseNpgsql(\"<connection string>\"); } // At the point where you need to perform a database operation: using var context = new BloggingContext(); // Use the context... var dbContextFactory = new PooledDbContextFactory<BloggingContext>( new DbContextOptionsBuilder<BloggingContext>() .UseNpgsql(\"<connection string>\") .Options); // At the point where you need to perform a database operation: using var context = dbContextFactory.CreateDbContext(); // Use the context... When using ASP.NET - or any application with dependency injection - the context instance will be injected into your code. Use the following to configure EF with your DI container: var builder = WebApplication.CreateBuilder(args); builder.Services.AddDbContextPool<BloggingContext>(opt => opt.UseNpgsql(builder.Configuration.GetConnectionString(\"BloggingContext\"))); public class BloggingContext(DbContextOptions<BloggingContext> options) : DbContext(options) { public DbSet<Blog> Blogs { get; set; } public DbSet<Post> Posts { get; set; } } For more information on getting started with EF, consult the EF getting started documentation. Additional Npgsql configuration The Npgsql EF provider is built on top of the lower-level Npgsql ADO.NET provider (docs); these two separate components support various options you may want to configure. If you're using EF 9.0 or above, the UseNpgsql() is a single point where you can configure everything related to Npgsql. For example: builder.Services.AddDbContextPool<BloggingContext>(opt => opt.UseNpgsql( builder.Configuration.GetConnectionString(\"BloggingContext\"), o => o .SetPostgresVersion(13, 0) .UseNodaTime() .MapEnum<Mood>(\"mood\"))); The above configures the EF provider to produce SQL for PostgreSQL version 13 (avoiding newer incompatible features), adds a plugin allowing use of NodaTime for date/time type mapping, and maps a .NET enum type. Note that the last two also require configuration at the lower-level ADO.NET layer, which the code above does for you automatically. If you need to configure something at the lower-level ADO.NET layer, use ConfigureDataSource() as follows: builder.Services.AddDbContextPool<BloggingContext>(opt => opt.UseNpgsql( builder.Configuration.GetConnectionString(\"BloggingContext\"), o => o.ConfigureDataSource(dataSourceBuilder => dataSourceBuilder.UseClientCertificate(certificate)))); ConfigureDataSource() provides access to a lower-level NpgsqlDataSourceBuilder which you can use to configure all aspects of the Npgsql ADO.NET provider. Warning The EF provider internally creates an NpgsqlDataSource and uses that; for most configuration (e.g. connection string), the provider knows to switch between NpgsqlDataSources automatically. However, it's not possible to detect configuration differences within the ConfigureDataSource(); as a result, avoid performing varying configuration inside ConfigureDataSource(), since you may get the wrong NpgsqlDataSource. If you find yourself needing to vary Npgsql ADO.NET configuration, create an external NpgsqlDataSource yourself with the desired configuration and pass that to UseNpgsql() as described below. Using an external NpgsqlDataSource If you're using a version of EF prior to 9.0, the above configuration methods aren't available. You can still create an NpgsqlDataSource yourself, and then pass it EF's UseNpgsql(): var dataSourceBuilder = new NpgsqlDataSourceBuilder(builder.Configuration.GetConnectionString(\"BloggingContext\")); dataSourceBuilder.MapEnum<Mood>(); dataSourceBuilder.UseNodaTime(); var dataSource = dataSourceBuilder.Build(); builder.Services.AddDbContextPool<BloggingContext>(opt => opt.UseNpgsql(dataSource)); Using an Existing Database (Database-First) The Npgsql EF Core provider also supports reverse-engineering a code model from an existing PostgreSQL database (\"database-first\"). To do so, use dotnet CLI to execute the following: dotnet ef dbcontext scaffold \"Host=my_host;Database=my_db;Username=my_user;Password=my_pw\" Npgsql.EntityFrameworkCore.PostgreSQL"
  },
  "doc/conceptual/EFCore.PG/mapping/array.html": {
    "href": "doc/conceptual/EFCore.PG/mapping/array.html",
    "title": "Array Type Mapping | Npgsql Documentation",
    "keywords": "Array Type Mapping PostgreSQL has the unique feature of supporting array data types. This allow you to conveniently and efficiently store several values in a single column, where in other database you'd typically resort to concatenating the values in a string or defining another table with a one-to-many relationship. Note Although PostgreSQL supports multidimensional arrays, these aren't yet supported by the EF Core provider. Mapping arrays Simply define a regular .NET array or List<> property: public class Post { public int Id { get; set; } public string Name { get; set; } public string[] Tags { get; set; } public List<string> AlternativeTags { get; set; } } The provider will create text[] columns for the above two properties, and will properly detect changes in them - if you load an array and change one of its elements, calling SaveChanges will automatically update the row in the database accordingly. Operation translation The provider can also translate CLR array operations to the corresponding SQL operation; this allows you to efficiently work with arrays by evaluating operations in the database and avoids pulling all the data. The following table lists the range operations that currently get translated; all these translations work both for .NET arrays (int[]) and for generic Lists (List<int>). If you run into a missing operation, please open an issue. .NET SQL Notes array[0] array[1] array.Length / list.Count cardinality(array) array.Skip(2) array[3,] Added in 8.0 array.Take(2) array[,2] Added in 8.0 array.Skip(1).Take(2) array[2,3] Added in 8.0 array1 == array2 array1 = array2 array1.SequenceEqual(array2) array1 = array2 arrayNonColumn.Contains(element) element = ANY(arrayNonColumn) Can use regular index arrayColumn.Contains(element) arrayColumn @> ARRAY[element] Can use GIN index array.Append(element) array_append(array, element) Added in 6.0 array1.Concat(array2) array1 || array2 Added in 6.0 array.IndexOf(element) array_position(array, element) - 1 Added in 6.0 array.IndexOf(element, startIndex) array_position(array, element, startIndex + 1) - 1 Added in 6.0 String.Join(separator, array) array_to_string(array, separator, '') Added in 6.0 array.Any() cardinality(array) > 0 array1.Intersect(array2).Any() array1 && array2 Added in 8.0 array1.Any(i => array2.Contains(i)) array1 && array2 array1.All(i => array2.Contains(i)) array1 <@ array2 array.Any(s => EF.Functions.Like(string, s)) string LIKE ANY (array) array.Any(s => EF.Functions.ILike(string, s)) string ILIKE ANY (array) array.All(s => EF.Functions.Like(string, s)) string LIKE ALL (array) array.All(s => EF.Functions.ILike(string, s)) string ILIKE ALL (array) EF.Functions.ArrayAgg(values) array_agg(values) Added in 7.0, See Aggregate functions."
  },
  "doc/conceptual/EFCore.PG/mapping/enum.html": {
    "href": "doc/conceptual/EFCore.PG/mapping/enum.html",
    "title": "Enum Type Mapping | Npgsql Documentation",
    "keywords": "Enum Type Mapping By default, any enum properties in your model will be mapped to database integers. EF Core 2.1 also allows you to map these to strings in the database with value converters. However, the Npgsql provider also allows you to map your CLR enums to database enum types. This option, unique to PostgreSQL, provides the best of both worlds: the enum is internally stored in the database as a number (minimal storage), but is handled like a string (more usable, no need to remember numeric values) and has type safety. Setting up your enum with EF Note Enum mapping has changed considerably in EF 9.0. If you're using EF 9.0 or above, simply call MapEnum inside your UseNpgsql invocation. With a connection string With an external NpgsqlDataSource If you're passing a connection string to UseNpgsql, simply add the MapEnum call as follows: builder.Services.AddDbContext<MyContext>(options => options.UseNpgsql( \"<connection string>\", o => o.MapEnum<Mood>(\"mood\"))); This configures all aspects of Npgsql to use your Mood enum - both at the EF and the lower-level Npgsql layer - and ensures that the enum is created in the database in EF migrations. If you're creating an external NpgsqlDataSource and passing it to UseNpgsql, you must make sure to map your enum on that data independently of the EF-level setup: var dataSourceBuilder = new NpgsqlDataSourceBuilder(\"<connection string>\"); dataSourceBuilder.MapEnum<Mood>(); var dataSource = dataSourceBuilder.Build(); builder.Services.AddDbContext<MyContext>(options => options.UseNpgsql( dataSource, o => o.MapEnum<Mood>(\"mood\"))); Older EF versions On versions of EF prior to 9.0, enum setup is more involved and consists of several steps; enum mapping has to be done at the lower-level Npgsql layer, and also requires explicit configuration in the EF model for creation in the database via migrations. Creating your database enum First, you must specify the PostgreSQL enum type on your model, just like you would with tables, sequences or other databases objects: protected override void OnModelCreating(ModelBuilder builder) => builder.HasPostgresEnum<Mood>(); This causes the EF Core provider to create your enum type, mood, with two labels: happy and sad. This will cause the appropriate migration to be created. Mapping your enum Even if your database enum is created, Npgsql has to know about it, and especially about your CLR enum type that should be mapped to it: NpgsqlDataSource Without NpgsqlDatasource Since version 7.0, NpgsqlDataSource is the recommended way to use Npgsql. When using NpgsqlDataSource, map your enum when building your data source: // Call MapEnum() when building your data source: var dataSourceBuilder = new NpgsqlDataSourceBuilder(/* connection string */); dataSourceBuilder.MapEnum<Mood>(); var dataSource = dataSourceBuilder.Build(); builder.Services.AddDbContext<MyContext>(options => options.UseNpgsql(dataSource)); Since version 7.0, NpgsqlDataSource is the recommended way to use Npgsql. However, if you're not yet using NpgsqlDataSource, map enums by adding the following code, before any EF Core operations take place. An appropriate place for this is in the static constructor on your DbContext class: static MyDbContext() => NpgsqlConnection.GlobalTypeMapper.MapEnum<Mood>(); Note If you have multiple context types, all MapEnum invocations must be done before any of them is used; this means that the code cannot be in your static constructors, but must be moved to the program start. This code lets Npgsql know that your CLR enum type, Mood, should be mapped to a database enum called mood. Note that if your enum is in a custom schema (not public), you must specify that schema in the call to MapEnum. Using enum properties Once your enum is properly set up with EF, you can use your CLR enum type just like any other property: public class Blog { public int Id { get; set; } public Mood Mood { get; set; } } using (var ctx = new MyDbContext()) { // Insert ctx.Blogs.Add(new Blog { Mood = Mood.Happy }); ctx.Blogs.SaveChanges(); // Query var blog = ctx.Blogs.Single(b => b.Mood == Mood.Happy); } Altering enum definitions The Npgsql provider only allow adding new values to existing enums, and the appropriate migrations will be automatically created as you add values to your CLR enum type. However, PostgreSQL itself doesn't support removing enum values (since these may be in use), and while renaming values is supported, it isn't automatically done by the provider to avoid using unreliable detection heuristics. Renaming an enum value can be done by including raw SQL in your migrations as follows: migrationBuilder.Sql(\"ALTER TYPE mood RENAME VALUE 'happy' TO 'thrilled';\"); As always, test your migrations carefully before running them on production databases. Scaffolding from an existing database If you're creating your model from an existing database, the provider will recognize enums in your database, and scaffold the appropriate HasPostgresEnum() lines in your model. However, the scaffolding process has no knowledge of your CLR type, and will therefore skip your enum columns (warnings will be logged). You will have to create the CLR type and perform the proper setup as described above. In the future it may be possible to scaffold the actual enum type (and with it the properties), but this isn't supported at the moment."
  },
  "doc/conceptual/EFCore.PG/mapping/full-text-search.html": {
    "href": "doc/conceptual/EFCore.PG/mapping/full-text-search.html",
    "title": "Full Text Search | Npgsql Documentation",
    "keywords": "Full Text Search PostgreSQL has built-in support for full-text search, which allows you to conveniently and efficiently query natural language documents. Mapping PostgreSQL full text search types are mapped onto .NET types built-in to Npgsql. The tsvector type is mapped to NpgsqlTsVector and tsquery is mapped to NpgsqlTsQuery. This means you can use properties of type NpgsqlTsVector directly in your model to create tsvector columns. The NpgsqlTsQuery type on the other hand, is used in LINQ queries. public class Product { public int Id { get; set; } public string Title { get; set; } public string Description { get; set; } public NpgsqlTsVector SearchVector { get; set; } } Setting up and querying a full text search index on an entity As the PostgreSQL documentation explains, full-text search requires an index to run efficiently. This section will show two ways to do this, each having its benefits and drawbacks. Please read the PostgreSQL docs for more information on the two different approaches. Method 1: tsvector column This method adds a tsvector column to your table, that is automatically updated when the row is modified. First, add an NpgsqlTsVector property to your entity: public class Product { public int Id { get; set; } public string Name { get; set; } public string Description { get; set; } public NpgsqlTsVector SearchVector { get; set; } } Setting up the column to be auto-updated depends on your PostgreSQL version. On PostgreSQL 12 and above, the column can be a simple generated column, and version 5.0.0 contains sugar for setting that up. In previous versions, you must manually set up database triggers that update the column instead. PostgreSQL 12+ Older Versions Note The below only works on PostgreSQL 12 and version 5.0.0 of the EF Core provider. The following will set up a generated tsvector column, over which you can easily create an index: protected override void OnModelCreating(ModelBuilder modelBuilder) { modelBuilder.Entity<Product>() .HasGeneratedTsVectorColumn( p => p.SearchVector, \"english\", // Text search config p => new { p.Name, p.Description }) // Included properties .HasIndex(p => p.SearchVector) .HasMethod(\"GIN\"); // Index method on the search vector (GIN or GIST) } First, modify the OnModelCreating() of your context class to add an index as follows: protected override void OnModelCreating(ModelBuilder modelBuilder) { modelBuilder.Entity<Product>() .HasIndex(p => p.SearchVector) .HasMethod(\"GIN\"); // Index method on the search vector (GIN or GIST) } Now generate a migration (dotnet ef migrations add ....), and open it with your favorite editor, adding the following: public partial class CreateProductTable : Migration { protected override void Up(MigrationBuilder migrationBuilder) { // Migrations for creation of the column and the index will appear here, all we need to do is set up the trigger to update the column: migrationBuilder.Sql( @\"CREATE TRIGGER product_search_vector_update BEFORE INSERT OR UPDATE ON \"\"Products\"\" FOR EACH ROW EXECUTE PROCEDURE tsvector_update_trigger(\"\"SearchVector\"\", 'pg_catalog.english', \"\"Name\"\", \"\"Description\"\");\"); // If you were adding a tsvector to an existing table, you should populate the column using an UPDATE // migrationBuilder.Sql(\"UPDATE \\\"Products\\\" SET \\\"Name\\\" = \\\"Name\\\";\"); } protected override void Down(MigrationBuilder migrationBuilder) { // Migrations for dropping of the column and the index will appear here, all we need to do is drop the trigger: migrationBuilder.Sql(\"DROP TRIGGER product_search_vector_update\"); } } Once your auto-updated tsvector column is set up, any inserts or updates on the Products table will now update the SearchVector column and maintain it automatically. You can query it as follows: var context = new ProductDbContext(); var npgsql = context.Products .Where(p => p.SearchVector.Matches(\"Npgsql\")) .ToList(); Method 2: Expression index Version 5.0.0 of the provider includes sugar for defining the appropriate expression index; if you're using an older version, you'll have to define a raw SQL migration yourself. Version 5.0.0 Older Versions modelBuilder.Entity<Blog>() .HasIndex(b => new { b.Title, b.Description }) .HasMethod(\"GIN\") .IsTsVectorExpressionIndex(\"english\"); Create a migration which will contain the index creation SQL (dotnet ef migrations add ...). At this point, open the generated migration with your editor and add the following: protected override void Up(MigrationBuilder migrationBuilder) { migrationBuilder.Sql(@\"CREATE INDEX fts_idx ON \"\"Product\"\" USING GIN (to_tsvector('english', \"\"Name\"\" || ' ' || \"\"Description\"\"));\"); } protected override void Down(MigrationBuilder migrationBuilder) migrationBuilder.Sql(@\"DROP INDEX fts_idx;\"); } Once the index is created on the Title and Description columns, you can query as follows: var context = new ProductDbContext(); var npgsql = context.Products .Where(p => EF.Functions.ToTsVector(\"english\", p.Title + \" \" + p.Description) .Matches(\"Npgsql\")) .ToList(); Computed column over JSON columns Starting with 7.0, the provider can also create computed tsvector columns over JSON columns. Simply use HasGeneratedTsVectorColumn() as shown above, and when applied to JSON columns, the provider will automatically generate json_to_tsvector/jsonb_to_tsvector as appropriate. Note that this will pass the filter all to these functions, meaning that all values in the JSON document will be included. To customize the filter - or to create the computed column on older versions of the provider - simply specify the function yourself via HasComputedColumnSql. Operation translation Almost all PostgreSQL full text search functions can be called through LINQ queries. All supported EF Core LINQ methods are defined in extension classes in the Microsoft.EntityFrameworkCore namespace, so simply referencing the Npgsql provider will light up these methods. The following table lists all supported operations; if an operation you need is missing, please open an issue to request for it. .NET SQL EF.Functions.ToTsVector(string) to_tsvector(string) EF.Functions.ToTsVector(\"english\", string) to_tsvector('english'::regconfig, string) EF.Functions.ToTsQuery(string) to_tsquery(string) EF.Functions.ToTsQuery(\"english\", string ) to_tsquery('english'::regconfig, string) EF.Functions.PlainToTsQuery(string) plainto_tsquery(string) EF.Functions.PlainToTsQuery(\"english\", string) plainto_tsquery('english'::regconfig, string) EF.Functions.PhraseToTsQuery(string) phraseto_tsquery(string) EF.Functions.PhraseToTsQuery(\"english\", string) phraseto_tsquery('english'::regconfig, string) EF.Functions.WebSearchToTsQuery(string) websearch_to_tsquery(string) EF.Functions.WebSearchToTsQuery(\"english\", string) websearch_to_tsquery('english'::regconfig, string) EF.functions.ArrayToTsVector(new[] { \"a\", \"b\" }) array_to_tsvector(ARRAY['a', 'b']) NpgsqlTsVector.Parse(string) CAST(string AS tsvector) NpgsqlTsQuery.Parse(string) CAST(queryString AS tsquery) tsvector.Matches(string) tsvector @@ plainto_tsquery(string) tsvector.Matches(tsquery) tsvector @@ tsquery tsquery1.And(tsquery2) tsquery1 && tsquery2 tsquery1.Or(tsquery2) tsquery1 || tsquery2 tsquery.ToNegative() !! tsquery tsquery1.Contains(tsquery2) tsquery1 @> tsquery2 tsquery1.IscontainedIn(tsquery2) tsquery1 <@ tsquery2 tsquery.GetNodeCount() numnode(query) tsquery.GetQueryTree() querytree(query) tsquery.GetResultHeadline(\"a b c\") ts_headline('a b c', query) tsquery.GetResultHeadline(\"a b c\", \"MinWords=1, MaxWords=2\") ts_headline('a b c', query, 'MinWords=1, MaxWords=2') tsquery.Rewrite(targetQuery, substituteQuery) ts_rewrite(to_tsquery(tsquery), to_tsquery(targetQuery), to_tsquery(substituteQuery)) tsquery1.ToPhrase(tsquery2) tsquery_phrase(tsquery1, tsquery2) tsquery1.ToPhrase(tsquery2, distance) tsquery_phrase(tsquery1, tsquery2, distance) tsvector1.Concat(tsvector2) tsvector1 || tsvector2 tsvector.Delete(\"x\") ts_delete(tsvector, 'x') tsvector.Delete(new[] { \"x\", \"y\" }) ts_delete(tsvector, ARRAY['x', 'y']) tsvector.Filter(new[] { \"x\", \"y\" }) ts_filter(tsvector, ARRAY['x', 'y']) tsvector.GetLength() length(tsvector) tsvector.Rank(tsquery) ts_rank(tsvector, tsquery) tsvector.RankCoverDensity(tsquery) ts_rank_cd(tsvector, tsquery) tsvector.SetWeight(NpgsqlTsVector.Lexeme.Weight.A) setweight(tsvector, 'A') tsvector.ToStripped() strip(tsvector) EF.Functions.Unaccent(string) unaccent(string) EF.Functions.Unaccent(regdictionary, string) unaccent(regdictionary, string)"
  },
  "doc/conceptual/EFCore.PG/mapping/general.html": {
    "href": "doc/conceptual/EFCore.PG/mapping/general.html",
    "title": "Type mapping | Npgsql Documentation",
    "keywords": "Type mapping The EF Core provider transparently maps the types supported by Npgsql at the ADO.NET level - see the Npgsql ADO type mapping page. This means that you can use PostgreSQL-specific types, such as inet or circle, directly in your entities. Simply define your properties just as if they were a simple type, such as a string: public class MyEntity { public int Id { get; set; } public string Name { get; set; } public IPAddress IPAddress { get; set; } public NpgsqlCircle Circle { get; set; } public int[] SomeInts { get; set; } } Special types such as arrays and enums have their own documentation pages with more details. PostgreSQL composite types, while supported at the ADO.NET level, aren't yet supported in the EF Core provider. This is tracked by #22. Explicitly specifying data types In some cases, your .NET property type can be mapped to several PostgreSQL data types; a good example is a string, which will be mapped to text by default, but can also be mapped to jsonb. You can use either Data Annotation attributes or the Fluent API to configure the PostgreSQL data type: Data Annotations Fluent API [Column(TypeName=\"jsonb\")] public string SomeStringProperty { get; set; } builder.Entity<Blog>() .Property(b => b.SomeStringProperty) .HasColumnType(\"jsonb\");"
  },
  "doc/conceptual/EFCore.PG/mapping/json.html": {
    "href": "doc/conceptual/EFCore.PG/mapping/json.html",
    "title": "JSON Mapping | Npgsql Documentation",
    "keywords": "JSON Mapping Note Version 8.0 of the Npgsql provider introduced support for EF's JSON columns, using ToJson(). That is the recommended way to map POCOs going forward. PostgreSQL has rich, built-in support for storing JSON columns and efficiently performing complex queries operations on them. Newcomers can read more about the PostgreSQL support on the JSON types page, and on the functions and operators page. Note that the below mapping mechanisms support both the jsonb and json types, although the former is almost always preferred for efficiency reasons. The Npgsql EF Core provider allows you to map PostgreSQL JSON columns in three different ways: As simple strings As EF owned entities As System.Text.Json DOM types (JsonDocument or JsonElement, see docs) As strongly-typed user-defined types (POCOs) (deprecated) String mapping The simplest form of mapping to JSON is via a regular string property, just like an ordinary text column: Data Annotations Fluent API public class SomeEntity { public int Id { get; set; } [Column(TypeName = \"jsonb\")] public string Customer { get; set; } } class MyContext : DbContext { public DbSet<SomeEntity> SomeEntities { get; set; } protected override void OnModelCreating(ModelBuilder modelBuilder) { modelBuilder.Entity<SomeEntity>() .Property(b => b.Customer) .HasColumnType(\"jsonb\"); } } public class SomeEntity { public int Id { get; set; } public string Customer { get; set; } } With string mapping, the EF Core provider will save and load properties to database JSON columns, but will not do any further serialization or parsing - it's the developer's responsibility to handle the JSON contents, possibly using System.Text.Json to parse them. This mapping approach is more limited compared to the others. POCO mapping If your column JSON contains documents with a stable schema, you can map them to your own .NET types (or POCOs); EF will use System.Text.Json APIs under the hood to serialize instances of your types to JSON documents before sending them to the database, and to deserialize documents coming back from the database. This effectively allows mapping an arbitrary .NET type - or object graph - to a single column in the database. EF 7.0 introduced the \"JSON Columns\" feature, which maps a database JSON column via EF's \"owned entity\" mapping concept, using ToJson(). In this approach, EF fully models the types within the JSON document - just like it models regular tables and columns - and uses that information to perform better queries and updates. Full support for ToJson has been added to version 8.0 of the Npgsql EF provider. As an alternative, prior to version 8.0, the Npgsql EF provider has supported JSON POCO mapping by simply delegating serialization/deserialization to System.Text.Json; in this model, EF itself model the contents of the JSON document, and cannot take that structure into account for queries and updates. This approach can now be considered deprecated as it allows for less powerful mapping and supports less query types; using ToJson() is now the recommended way to map POCOs to JSON. ToJson (owned entity mapping) Npgsql's support for ToJson() is fully aligned with the general EF support; see the EF documentation for more information. To get you started quickly, assume that we have the following Customer type, with a Details property that we want to map to a single JSON column in the database: public class Customer { public int Id { get; set; } public CustomerDetails Details { get; set; } } public class CustomerDetails // Map to a JSON column in the table { public string Name { get; set; } public int Age { get; set; } public List<Order> Orders { get; set; } } public class Order // Part of the JSON column { public decimal Price { get; set; } public string ShippingAddress { get; set; } } To instruct EF to map CustomerDetails - and within it, Order - to a JSON column, configure it as follows: protected override void OnModelCreating(ModelBuilder modelBuilder) { modelBuilder.Entity<Customer>() .OwnsOne(c => c.Details, d => { d.ToJson(); d.OwnsMany(d => d.Orders); }); } At this point you can interact with the Customer just like you would normally, and EF will seamlessly serialize and deserialize it to a JSON column in the database. You can also perform LINQ queries which reference properties inside the JSON document, and these will get translated to SQL. Traditional POCO mapping (deprecated) Before version 8.0 introduced support for EF's ToJson (owned entity mapping), the provider had its own support for JSON POCO mapping, by simply delegating serialization/deserialization to System.Text.Json; in this model, EF itself model the contents of the JSON document, and cannot take that structure into account for queries and updates. This approach can now be considered deprecated as it allows for less powerful mapping and supports less query types; using ToJson() is now the recommended way to map POCOs to JSON. To use traditional POCO mapping, configure a property a mapping to map to a jsonb column as follows: Data Annotations Fluent API public class Customer { public int Id { get; set; } [Column(TypeName = \"jsonb\")] public CustomerDetails Details { get; set; } } public class CustomerDetails // Mapped to a JSON column in the table { public string Name { get; set; } public int Age { get; set; } public Order[] Orders { get; set; } } public class Order // Part of the JSON column { public decimal Price { get; set; } public string ShippingAddress { get; set; } } class MyContext : DbContext { public DbSet<SomeEntity> SomeEntities { get; set; } protected override void OnModelCreating(ModelBuilder modelBuilder) { modelBuilder.Entity<SomeEntity>() .Property(b => b.Customer) .HasColumnType(\"jsonb\"); } } public class SomeEntity // Mapped to a database table { public int Id { get; set; } public Customer Customer { get; set; } } public class Customer // Mapped to a JSON column in the table { public string Name { get; set; } public int Age { get; set; } public Order[] Orders { get; set; } } public class Order // Part of the JSON column { [JsonPropertyName(\"OrderPrice\")] // Controls the JSON property name public decimal Price { get; set; } public string ShippingAddress { get; set; } } Note that when using this mapping, only limited forms of LINQ querying is supported; it's recommended to switch to ToJson() for full LINQ querying capabilities. The querying supported by traditional POCO mapping is documented below. JsonDocument DOM mapping If your column JSON schema isn't stable, a strongly-typed POCO mapping may not be appropriate. The Npgsql provider also allows you to map the DOM document type provided by System.Text.Json APIs. public class SomeEntity : IDisposable { public int Id { get; set; } public JsonDocument Customer { get; set; } public void Dispose() => Customer?.Dispose(); } Note that neither a data annotation nor the fluent API are required, as JsonDocument is automatically recognized and mapped to jsonb. Note also that JsonDocument is disposable, so the entity type is made disposable as well; not dispose the JsonDocument will result in the memory not being returned to the pool, which will increase GC impact across various parts of the framework. Once a document is loaded from the database, you can traverse it: var someEntity = context.Entities.First(); Console.WriteLine(someEntity.Customer.RootElement.GetProperty(\"Orders\")[0].GetProperty(\"Price\").GetInt32()); Note that when using this mapping, only limited forms of LINQ querying is supported; see below for more details. Querying JSON columns (traditional JSON and DOM) Note The below does not apply if you are using ToJson (owned entity mapping). ToJson supports Saving and loading documents these documents wouldn't be much use without the ability to query them. You can express your queries via the same LINQ constructs you are already using in EF Core: Classic POCO Mapping JsonDocument Mapping var joes = context.CustomerEntries .Where(e => e.Customer.Name == \"Joe\") .ToList(); var joes = context.CustomerEntries .Where(e => e.Customer.RootElement.GetProperty(\"Name\").GetString() == \"Joe\") .ToList(); The provider will recognize the traversal of a JSON document, and translate it to the correspond PostgreSQL JSON traversal operator, producing the following PostgreSQL-specific SQL: SELECT c.\"\"Id\"\", c.\"\"Customer\"\" FROM \"\"CustomerEntries\"\" AS c WHERE c.\"\"Customer\"\"->>'Name' = 'Joe' If indexes are set up properly, this can result in very efficient, server evaluation of searches with database JSON documents. The following expression types and functions are translated: POCO Mapping JsonDocument Mapping .NET SQL customer.Name customer->>'Name' customer.Orders[1].Price customer#>>'{Orders,0,Price}'[1] customer.Orders.Length (or Count) jsonb_array_length(customer->'Orders') EF.Functions.JsonContains(customer, @\"{\"\"Name\"\": \"\"Joe\"\", \"\"Age\"\": 25}\")1 customer @> '{\"Name\": \"Joe\", \"Age\": 25}' EF.Functions.JsonContained(@\"{\"\"Name\"\": \"\"Joe\"\", \"\"Age\"\": 25}\", e.Customer)1 '{\"Name\": \"Joe\", \"Age\": 25}' <@ customer EF.Functions.JsonExists(e.Customer, \"Age\") customer ? 'Age' EF.Functions.JsonExistsAny(e.Customer, \"Age\", \"Address\") customer ?| ARRAY['Age','Address'] EF.Functions.JsonExistsAll(e.Customer, \"Age\", \"Address\") customer ?& ARRAY['Age','Address'] EF.Functions.JsonTypeof(e.Customer.Age) jsonb_typeof(customer->'Age') .NET SQL customer.RootElement.GetProperty(\"Name\").GetString() customer->>'Name' = 'Joe' customer.RootElement.GetProperty(\"Orders\")[1].GetProperty(\"Price\").GetInt32() customer#>>'{Orders,0,Price}'[1] = 8 customer.RootElement.GetProperty(\"Orders\").GetArrayLength() jsonb_array_length(customer->'Orders' EF.Functions.JsonContains(customer, @\"{\"\"Name\"\": \"\"Joe\"\", \"\"Age\"\": 25}\")1 customer @> '{\"Name\": \"Joe\", \"Age\": 25}' EF.Functions.JsonContained(@\"{\"\"Name\"\": \"\"Joe\"\", \"\"Age\"\": 25}\", customer)1 '{\"Name\": \"Joe\", \"Age\": 25}' <@ customer EF.Functions.JsonExists(customer, \"Age\") customer ? 'Age' EF.Functions.JsonExistsAny(customer, \"Age\", \"Address\") customer ?| ARRAY['Age','Address'] EF.Functions.JsonExistsAll(customer, \"Age\", \"Address\") customer ?& ARRAY['Age','Address'] EF.Functions.JsonTypeof(customer.GetProperty(\"Age\")) == \"number\" jsonb_typeof(customer->'Age') = 'number' 1 JSON functions which accept a .NET object will not accept .NET scalar values. For example, to pass a scalar to JsonContains wrap it in a JsonElement or alternatively wrap it in a string. Note: a root level JSON string value requires quotes and escaping @\"\"\"Joe\"\"\", just as any nested JSON string value would. Indexing JSON columns Note A section on indices will be added. In the meantime consult the PostgreSQL documentation and other guides on the Internet."
  },
  "doc/conceptual/EFCore.PG/mapping/nodatime.html": {
    "href": "doc/conceptual/EFCore.PG/mapping/nodatime.html",
    "title": "Date/Time Mapping with NodaTime | Npgsql Documentation",
    "keywords": "Date/Time Mapping with NodaTime What is NodaTime By default, the PostgreSQL date/time types are mapped to the built-in .NET types (DateTime, TimeSpan). Unfortunately, these built-in types are flawed in many ways. The NodaTime library was created to solve many of these problems, and if your application handles dates and times in anything but the most basic way, you should consider using it. To learn more read this blog post by Jon Skeet. Beyond NodaTime's general advantages, some specific advantages NodaTime for PostgreSQL date/time mapping include: NodaTime defines some types which are missing from the BCL, such as LocalDate, LocalTime, and OffsetTime. These cleanly correspond to PostgreSQL date, time and timetz. Period is much more suitable for mapping PostgreSQL interval than TimeSpan. NodaTime types can fully represent PostgreSQL's microsecond precision, and can represent dates outside the BCL's date limit (1AD-9999AD). Setup To set up the NodaTime plugin, add the Npgsql.EntityFrameworkCore.PostgreSQL.NodaTime nuget to your project. Then, configure the NodaTime plugin as follows: EF 9.0, with a connection string With an external NpgsqlDataSource Older EF versions, with a connection string If you're passing a connection string to UseNpgsql, simply add the UseNodaTime call as follows: builder.Services.AddDbContext<MyContext>(options => options.UseNpgsql( \"<connection string>\", o => o.UseNodaTime())); This configures all aspects of Npgsql to use the NodaTime plugin - both at the EF and the lower-level Npgsql layer. If you're creating an external NpgsqlDataSource and passing it to UseNpgsql, you must call UseNodaTime on your NpgsqlDataSourceBuilder independently of the EF-level setup: var dataSourceBuilder = new NpgsqlDataSourceBuilder(\"<connection string>\"); dataSourceBuilder.UseNodaTime(); var dataSource = dataSourceBuilder.Build(); builder.Services.AddDbContext<MyContext>(options => options.UseNpgsql( dataSource, o => o.UseNodaTime())); // Configure UseNodaTime at the ADO.NET level. // This code must be placed at the beginning of your application, before any other Npgsql API is called; an appropriate place for this is in the static constructor on your DbContext class: static MyDbContext() => NpgsqlConnection.GlobalTypeMapper.UseNodaTime(); // Then, when configuring EF Core with UseNpgsql(), call UseNodaTime(): builder.Services.AddDbContext<MyContext>(options => options.UseNpgsql(\"<connection string>\", o => o.UseNodaTime())); The above sets up all the necessary mappings and operation translators. You can now use NodaTime types as regular properties in your entities, and even perform some operations: public class Post { public int Id { get; set; } public string Name { get; set; } public Instant CreationTime { get; set; } } var recentPosts = context.Posts.Where(p => p.CreationTime > someInstant); Operation translation The provider knows how to translate many members and methods on mapped NodaTime types. For example, the following query will be translated to SQL and evaluated server-side: // Get all events which occurred on a Monday var mondayEvents = context.Events.Where(p => p.SomeDate.DayOfWeek == DayOfWeek.Monday); // Get all events which occurred before the year 2000 var oldEvents = context.Events.Where(p => p.SomeDate.Year < 2000); Following is the list of supported NodaTime translations; If an operation you need is missing, please open an issue to request for it. Note Most translations on ZonedDateTime and Period were added in version 6.0 .NET SQL Notes SystemClock.Instance.GetCurrentInstant() now() LocalDateTime.Date date_trunc('day', timestamp) LocalDateTime.Second (also LocalTime, ZonedDateTime) date_part('second', timestamp)::INT LocalDateTime.Minute (also LocalTime, ZonedDateTime) date_part('minute', timestamp)::INT LocalDateTime.Hour (also LocalTime, ZonedDateTime) date_part('hour', timestamp)::INT LocalDateTime.Day, (also LocalDate, ZonedDateTime) date_part('day', timestamp)::INT LocalDateTime.Month (also LocalDate, ZonedDateTime) date_part('month', timestamp)::INT LocalDateTime.Year (also LocalDate, ZonedDateTime) date_part('year', timestamp)::INT LocalDateTime.DayOfWeek (also LocalDate, ZonedDateTime) floor(date_part('dow', timestamp))::INT LocalDateTime.DayOfYear (also LocalDate, ZonedDateTime) date_part('doy', timestamp)::INT Period.Seconds (also Duration) date_part('second', interval)::INT Period.Minutes (also Duration) date_part('minute', interval)::INT Period.Hours (also Duration) date_part('hour', interval)::INT Period.Days (also Duration) date_part('day', interval)::INT Period.Months date_part('month', interval)::INT Period.Years date_part('year', interval)::INT Period.FromSeconds make_interval(seconds => int) Period.FromMinutes make_interval(minutes => int) Period.FromHours make_interval(hours => int) Period.FromDays make_interval(days => int) Period.FromWeeks make_interval(weeks => int) Period.FromMonths make_interval(months => int) Period.FromYears make_interval(years => int) Duration.TotalMilliseconds date_part('epoch', interval) / 0.001 Duration.TotalSeconds date_part('epoch', interval) Duration.TotalMinutes date_part('epoch', interval) / 60.0 Duration.TotalDays date_part('epoch', interval) / 86400.0 Duration.TotalHours date_part('epoch', interval) / 3600.0 ZonedDateTime.LocalDateTime timestamptz AT TIME ZONE 'UTC' Added in 6.0 DateInterval.Length upper(daterange) - lower(daterange) Added in 6.0 DateInterval.Start lower(daterange) Added in 6.0 DateInterval.End upper(daterange) - INTERVAL 'P1D' Added in 6.0 DateInterval.Contains(LocalDate) daterange @> date Added in 6.0 DateInterval.Contains(DateInterval) daterange @> daterange Added in 6.0 DateInterval.Intersection(DateInterval) daterange * daterange Added in 6.0 DateInterval.Union(DateInterval) daterange + daterange Added in 6.0 Instant.InZone(DateTimeZoneProviders.Tzdb[\"Europe/Berlin\"]).LocalDateTime timestamptz AT TIME ZONE 'Europe/Berlin' Added in 6.0 LocalDateTime.InZoneLeniently(DateTimeZoneProviders.Tzdb[\"Europe/Berlin\"]).ToInstant() timestamp AT TIME ZONE 'Europe/Berlin' Added in 6.0 ZonedDateTime.ToInstant No PG operation (.NET-side conversion from ZonedDateTime to Instant only) Added in 6.0 Instant.InUtc No PG operation (.NET-side conversion from Instant to ZonedDateTime only) Added in 6.0 Instant.ToDateTimeUtc No PG operation (.NET-side conversion from Instant to UTC DateTime only) Added in 6.0 EF.Functions.Sum(periods) sum(periods) Added in 7.0, see Aggregate functions. EF.Functions.Sum(durations) sum(durations) Added in 7.0, see Aggregate functions. EF.Functions.Average(periods) avg(durations) Added in 7.0, see Aggregate functions. EF.Functions.Average(durations) avg(durations) Added in 7.0, see Aggregate functions. In addition to the above, most arithmetic operators are also translated (e.g. LocalDate + Period)."
  },
  "doc/conceptual/EFCore.PG/mapping/nts.html": {
    "href": "doc/conceptual/EFCore.PG/mapping/nts.html",
    "title": "Spatial Mapping with NetTopologySuite | Npgsql Documentation",
    "keywords": "Spatial Mapping with NetTopologySuite Note It's recommended that you start by reading the general Entity Framework Core docs on spatial support. PostgreSQL supports spatial data and operations via the PostGIS extension, which is a mature and feature-rich database spatial implementation. .NET doesn't provide a standard spatial library, but NetTopologySuite is a leading spatial library. The Npgsql EF Core provider has a plugin which allows you to map the NTS types to PostGIS columns, allowing seamless reading and writing. This is the recommended way to interact with spatial types in Npgsql. Note that the EF Core NetTopologySuite plugin depends on the Npgsql ADO.NET NetTopology plugin, which provides NetTopologySuite support at the lower level. Setup To use the NetTopologySuite plugin, add the Npgsql.EntityFrameworkCore.PostgreSQL.NetTopologySuite nuget to your project. Then, configure the NetTopologySuite plugin as follows: EF 9.0, with a connection string With an external NpgsqlDataSource Older EF versions, with a connection string If you're passing a connection string to UseNpgsql, simply add the UseNetTopologySuite call as follows: builder.Services.AddDbContext<MyContext>(options => options.UseNpgsql( \"<connection string>\", o => o.UseNetTopologySuite())); This configures all aspects of Npgsql to use the NetTopologySuite plugin - both at the EF and the lower-level Npgsql layer. If you're creating an external NpgsqlDataSource and passing it to UseNpgsql, you must call UseNetTopologySuite on your NpgsqlDataSourceBuilder independently of the EF-level setup: var dataSourceBuilder = new NpgsqlDataSourceBuilder(\"<connection string>\"); dataSourceBuilder.UseNetTopologySuite(); var dataSource = dataSourceBuilder.Build(); builder.Services.AddDbContext<MyContext>(options => options.UseNpgsql( dataSource, o => o.UseNetTopologySuite())); // Configure NetTopologySuite at the ADO.NET level. // This code must be placed at the beginning of your application, before any other Npgsql API is called; an appropriate place for this is in the static constructor on your DbContext class: static MyDbContext() => NpgsqlConnection.GlobalTypeMapper.UseNetTopologySuite(); // Then, when configuring EF Core with UseNpgsql(), call UseNetTopologySuite(): builder.Services.AddDbContext<MyContext>(options => options.UseNpgsql(\"<connection string>\", o => o.UseNetTopologySuite())); The above sets up all the necessary EF mappings and operation translators. If you're using EF 6.0, you also need to make sure that the PostGIS extension is installed in your database (later versions do this automatically). Add the following to your DbContext: protected override void OnModelCreating(ModelBuilder builder) { builder.HasPostgresExtension(\"postgis\"); } At this point spatial support is set up. You can now use NetTopologySuite types as regular properties in your entities, and even perform some operations: public class City { public int Id { get; set; } public string Name { get; set; } public Point Location { get; set; } } var nearbyCities = context.Cities.Where(c => c.Location.Distance(somePoint) < 100); Constraining your type names With the code above, the provider will create a database column of type geometry. This is perfectly fine, but be aware that this type accepts any geometry type (point, polygon...), with any coordinate system (XY, XYZ...). It's good practice to constrain the column to the exact type of data you will be storing, but unfortunately the provider isn't aware of your required coordinate system and therefore can't do that for you. Consider explicitly specifying your column types on your properties as follows: [Column(TypeName=\"geometry (point)\")] public Point Location { get; set; } This will constrain your column to XY points only. The same can be done via the fluent API with HasColumnType(). Geography (geodetic) support PostGIS has two types: geometry (for Cartesian coordinates) and geography (for geodetic or spherical coordinates). You can read about the geometry/geography distinction in the PostGIS docs or in this blog post. In a nutshell, geography is much more accurate when doing calculations over long distances, but is more expensive computationally and supports only a small subset of the spatial operations supported by geometry. The Npgsql provider will be default map all NetTopologySuite types to PostGIS geometry. However, you can instruct it to map certain properties to geography instead: protected override void OnModelCreating(ModelBuilder builder) { builder.Entity<City>().Property(b => b.Location).HasColumnType(\"geography (point)\"); } or via an attribute: public class City { public int Id { get; set; } public string Name { get; set; } [Column(TypeName=\"geography\")] public Point Location { get; set; } } Once you do this, your column will be created as geography, and spatial operations will behave as expected. Operation translation The following table lists NetTopologySuite operations which are translated to PostGIS SQL operations. This allows you to use these NetTopologySuite methods and members efficiently - evaluation will happen on the server side. Since evaluation happens at the server, table data doesn't need to be transferred to the client (saving bandwidth), and in some cases indexes can be used to speed things up. Note that the plugin is far from covering all spatial operations. If an operation you need is missing, please open an issue to request for it. .NET SQL Notes geom.Area() ST_Area(geom) geom.AsBinary() ST_AsBinary(geom) geom.AsText() ST_AsText(geom) geom.Boundary ST_Boundary(geom) geom.Buffer(d) ST_Buffer(geom,d) geom.Centroid ST_Centroid(geom) geom1.Contains(geom2) ST_Contains(geom1, geom2) geomCollection.Count ST_NumGeometries(geom1) linestring.Count ST_NumPoints(linestring) geom1.ConvexHull() ST_ConvexHull(geom1) geom1.Covers(geom2) ST_Covers(geom1, geom2) geom1.CoveredBy(geom2) ST_CoveredBy(geom1, geom2) geom1.Crosses(geom2) ST_Crosses(geom1, geom2) geom1.Difference(geom2) ST_Difference(geom1, geom2) geom1.Dimension ST_Dimension(geom1) geom1.Disjoint(geom2) ST_Disjoint(geom1, geom2) geom1.Distance(geom2) ST_Distance(geom1, geom2) EF.Functions.DistanceKnn(geom1, geom2) geom1 <-> geom2 Added in 6.0 EF.Functions.Distance(geom1, geom2, useSpheriod) ST_Distance(geom1, geom2, useSpheriod) Added in 6.0 geom1.Envelope ST_Envelope(geom1) geom1.ExactEquals(geom2) ST_OrderingEquals(geom1, geom2) lineString.EndPoint ST_EndPoint(lineString) polygon.ExteriorRing ST_ExteriorRing(polygon) geom1.Equals(geom2) geom1 = geom2 geom1.Polygon.EqualsExact(geom2) geom1 = geom2 geom1.EqualsTopologically(geom2) ST_Equals(geom1, geom2) EF.Functions.Force2D ST_Force2D(geom) Added in 6.0 geom.GeometryType GeometryType(geom) geomCollection.GetGeometryN(i) ST_GeometryN(geomCollection, i) linestring.GetPointN(i) ST_PointN(linestring, i) geom1.Intersection(geom2) ST_Intersection(geom1, geom2) geom1.Intersects(geom2) ST_Intersects(geom1, geom2) geom.InteriorPoint ST_PointOnSurface(geom) lineString.IsClosed() ST_IsClosed(lineString) geomCollection.IsEmpty() ST_IsEmpty(geomCollection) linestring.IsRing ST_IsRing(linestring) geom.IsWithinDistance(geom2,d) ST_DWithin(geom1, geom2, d) EF.Functions.IsWithinDistance(geom1, geom2, d, useSpheriod) ST_DWithin(geom1, geom2, d, useSpheriod) Added in 6.0 geom.IsSimple() ST_IsSimple(geom) geom.IsValid() ST_IsValid(geom) lineString.Length ST_Length(lineString) geom.Normalized ST_Normalize(geom) geomCollection.NumGeometries ST_NumGeometries(geomCollection) polygon.NumInteriorRings ST_NumInteriorRings(polygon) lineString.NumPoints ST_NumPoints(lineString) geom1.Overlaps(geom2) ST_Overlaps(geom1, geom2) geom.PointOnSurface ST_PointOnSurface(geom) geom1.Relate(geom2) ST_Relate(geom1, geom2) geom.Reverse() ST_Reverse(geom) geom1.SRID ST_SRID(geom1) lineString.StartPoint ST_StartPoint(lineString) geom1.SymmetricDifference(geom2) ST_SymDifference(geom1, geom2) geom.ToBinary() ST_AsBinary(geom) geom.ToText() ST_AsText(geom) geom1.Touches(geom2) ST_Touches(geom1, geom2) EF.Functions.Transform(geom, srid) ST_Transform(geom, srid) geom1.Union(geom2) ST_Union(geom1, geom2) geom1.Within(geom2) ST_Within(geom1, geom2) point.M ST_M(point) point.X ST_X(point) point.Y ST_Y(point) point.Z ST_Z(point) UnaryUnionOp.Union(geometries) ST_Union(geometries) Added in 7.0, see Aggregate functions. GeometryCombiner.Combine(geometries) ST_Collect(geometries) Added in 7.0, see Aggregate functions. EnvelopeCombiner.CombineAsGeometry(geometries) ST_Extent(geometries)::geometry Added in 7.0, see Aggregate functions. ConvexHull.Create(geometries) ST_ConvexHull(geometries) Added in 7.0, see Aggregate functions."
  },
  "doc/conceptual/EFCore.PG/mapping/range.html": {
    "href": "doc/conceptual/EFCore.PG/mapping/range.html",
    "title": "Ranges and Multiranges | Npgsql Documentation",
    "keywords": "Ranges and Multiranges PostgreSQL has the unique feature of supporting range data types. Ranges represent a range of numbers, dates or other data types, and allow you to easily query ranges which contain a value, perform set operations (e.g. query ranges which contain other ranges), and other similar operations. The range operations supported by PostgreSQL are listed in this page. The Npgsql EF Core provider allows you to seamlessly map PostgreSQL ranges, and even perform operations on them that get translated to SQL for server evaluation. In addition, PostgreSQL 14 introduced multiranges, which are basically sorted arrays of non-overlapping ranges with set-theoretic operations defined over them. Most range operators also work on multiranges, and they have a few functions of their own. Multirange support in the EF Core provider was introduced in version 6.0.0. Ranges Npgsql maps PostgreSQL ranges to the generic CLR type NpgsqlRange<T>: public class Event { public int Id { get; set; } public string Name { get; set; } public NpgsqlRange<DateTime> Duration { get; set; } } This will create a column of type daterange in your database. You can similarly have properties of type NpgsqlRange<int>, NpgsqlRange<long>, etc. User-defined ranges PostgreSQL comes with 6 built-in ranges: int4range, int8range, numrange, tsrange, tstzrange, daterange; these can be used simply by adding the appropriate NpgsqlRange<T> property in your entities as shown above. You can also define your own range types over arbitrary types, and use those in EF Core as well. To make the EF Core type mapper aware of your user-defined range, call the MapRange() method in your context's OnConfiguring() method as follows: protected override void OnConfiguring(DbContextOptionsBuilder optionsBuilder) => optionsBuilder.UseNpgsql( \"<connection_string>\", options => options.MapRange<float>(\"floatrange\")); This allows you to have properties of type NpgsqlRange<float>, which will be mapped to PostgreSQL floatrange. The above does not create the floatrange type for you. In order to do that, include the following in your context's OnModelCreating(): protected override void OnModelCreating(ModelBuilder modelBuilder) => modelBuilder.HasPostgresRange(\"floatrange\", \"real\"); This will cause the appropriate CREATE TYPE ... AS RANGE statement to be generated in your migrations, ensuring that your range is created and ready for use. Note that HasPostgresRange() supports additional parameters as supported by PostgreSQL CREATE TYPE. Multiranges Note This feature was introduced in version 6.0 Npgsql maps arrays or lists of NpgsqlRange<T> to PostgreSQL multiranges: public class Store { public int Id { get; set; } public string Name { get; set; } public NpgsqlRange<DateTime>[] OpeningTimes { get; set; } } Operation translation Ranges can be queried via extensions methods on NpgsqlRange: var events = context.Events.Where(p => p.Duration.Contains(someDate)); This will translate to an SQL operation using the PostgreSQL @> operator, evaluating at the server and saving you from transferring the entire Events table to the client. Note that you can (and probably should) create indexes to make this operation more efficient, see the PostgreSQL docs for more info. The following table lists the range operations that currently get translated. Most operations work on both ranges and multiranges (starting with version 6.0.0); the multirange version is omitted for brevity. If you run into a missing operation, please open an issue. .NET SQL range.LowerBound lower(range) range.UpperBound upper(range) range.LowerBoundIsInclusive lower_inc(range) range.UpperBoundIsInclusive upper_inc(range) range.LowerBoundIsInfinite lower_inf(range) range.UpperBoundIsInfinite upper_inf(range) range.IsEmpty isempty(range) multirange.Any() NOT is_empty(multirange) range.Contains(i) range @> i range1.Contains(range2) range @> range2 range1.ContainedBy(range2) range1 <@ range2 range1.Overlaps(range2) range1 && range2 range1.IsStrictlyLeftOf(range2) range1 << range2 range1.IsStrictlyRightOf(range2) range1 >> range2 range1.DoesNotExtendLeftOf(range2) range1 &> range2 range1.DoesNotExtendRightOf(range2) range1 <& range2 range1.IsAdjacentTo(range2) range1 -|- range2 range1.Union(range2) range1 + range2 range1.Intersect(range2) range1 * range2 range1.Except(range2) range1 - range2 range1.Merge(range2) range_merge(range1, range2) multirange.Merge() range_merge(multirange) ranges.RangeAgg() range_agg(ranges) ranges.RangeIntersectAgg() range_intersect_agg(ranges) multiranges.RangeIntersectAgg() range_intersect_agg(multiranges) Added in 7.0, See Aggregate functions."
  },
  "doc/conceptual/EFCore.PG/mapping/translations.html": {
    "href": "doc/conceptual/EFCore.PG/mapping/translations.html",
    "title": "Translations | Npgsql Documentation",
    "keywords": "Translations Entity Framework Core allows providers to translate query expressions to SQL for database evaluation. For example, PostgreSQL supports regular expression operations, and the Npgsql EF Core provider automatically translates .NET's Regex.IsMatch to use this feature. Since evaluation happens at the server, table data doesn't need to be transferred to the client (saving bandwidth), and in some cases indexes can be used to speed things up. The same C# code on other providers will trigger client evaluation. The Npgsql-specific translations are listed below. Some areas, such as full-text search, have their own pages in this section which list additional translations. String functions .NET SQL Notes EF.Functions.Collate(operand, collation) operand COLLATE collation Added in 5.0 EF.Functions.Like(matchExpression, pattern) matchExpression LIKE pattern EF.Functions.Like(matchExpression, pattern, escapeCharacter) matchExpression LIKE pattern ESCAPE escapeCharacter EF.Functions.ILike(matchExpression, pattern) matchExpression ILIKE pattern EF.Functions.ILike(matchExpression, pattern, escapeCharacter) matchExpression ILIKE pattern ESCAPE escapeCharacter string.Compare(strA, strB) CASE WHEN strA = strB THEN 0 ... END string.Concat(str0, str1) str0 || str1 string.IsNullOrEmpty(value) value IS NULL OR value = '' string.IsNullOrWhiteSpace(value) value IS NULL OR btrim(value, E' \\t\\n\\r') = '' stringValue.CompareTo(strB) CASE WHEN stringValue = strB THEN 0 ... END stringValue.Contains(value) stringValue LIKE %value% stringValue.EndsWith(value) stringValue LIKE '%' || value stringValue.FirstOrDefault() substr(stringValue, 1, 1) Added in 5.0 stringValue.IndexOf(value) strpos(stringValue, value) - 1 stringValue.LastOrDefault() substr(stringValue, length(stringValue), 1) Added in 5.0 stringValue.Length length(stringValue) stringValue.PadLeft(length) lpad(stringValue, length) stringValue.PadLeft(length, char) lpad(stringValue, length, char) stringValue.PadRight(length) rpad(stringValue, length) stringValue.PadRight(length, char) rpad(stringValue, length, char) stringValue.Replace(oldValue, newValue) replace(stringValue, oldValue, newValue) stringValue.StartsWith(value) stringValue LIKE value || '%' stringValue.Substring(startIndex, length) substr(stringValue, startIndex + 1, @length) stringValue.ToLower() lower(stringValue) stringValue.ToUpper() upper(stringValue) stringValue.Trim() btrim(stringValue) stringValue.Trim(trimChar) btrim(stringValue, trimChar) stringValue.TrimEnd() rtrim(stringValue) stringValue.TrimEnd(trimChar) rtrim(stringValue, trimChar) stringValue.TrimStart() ltrim(stringValue) stringValue.TrimStart(trimChar) ltrim(stringValue, trimChar) EF.Functions.Reverse(value) reverse(value) Regex.IsMatch(stringValue, \"^A+\") stringValue ~ '^A+' (with options) Regex.IsMatch(stringValue, \"^A+\", regexOptions) stringValue ~ '^A+' (with options) string.Join(\", \", a, b) concat_ws(', ', a, b) Added in 7.0 (previously array_to_string) string.Join(\", \", array) array_to_string(array, ', ', '') string.Join(\", \", agg_strings) string_agg(agg_strings, ', ') Added in 7.0, see Aggregate functions. EF.Functions.StringToArray(s, \"|\") string_agg(s, '|') Added in 8.0 EF.Functions.StringToArray(s, \"|\", \"FOO\") string_agg(s, '|', 'FOO') Added in 8.0 Date and time functions Note Some of the operations below depend on the concept of a \"local time zone\" (e.g. DateTime.Today). While in .NET this is the machine time zone where .NET is running, the corresponding PostgreSQL translations use the TimeZone connection parameter as the local time zone. Since version 6.0, many of the below DateTime translations are also supported on DateTimeOffset. See also Npgsql's NodaTime support, which is a better and safer way of interacting with date/time data. .NET SQL Notes DateTime.UtcNow (6.0+) now() See 6.0 release notes DateTime.Now (6.0+) now()::timestamp See 6.0 release notes DateTime.Today (6.0+) date_trunc('day', now()::timestamp) See 6.0 release notes DateTime.UtcNow (legacy) now() AT TIME ZONE 'UTC' See 6.0 release notes DateTime.Now (legacy) now() See 6.0 release notes DateTime.Today (legacy) date_trunc('day', now()) See 6.0 release notes dateTime.AddDays(1) dateTime + INTERVAL '1 days' dateTime.AddHours(value) dateTime + INTERVAL '1 hours' dateTime.AddMinutes(1) dateTime + INTERVAL '1 minutes' dateTime.AddMonths(1) dateTime + INTERVAL '1 months' dateTime.AddSeconds(1) dateTime + INTERVAL '1 seconds' dateTime.AddYears(1) dateTime + INTERVAL '1 years' dateTime.Date date_trunc('day', dateTime) dateTime.Day date_part('day', dateTime)::INT dateTime.DayOfWeek floor(date_part('dow', dateTime))::INT dateTime.DayOfYear date_part('doy', dateTime)::INT dateTime.Hour date_part('hour', dateTime)::INT dateTime.Minute date_part('minute', dateTime)::INT dateTime.Month date_part('month', dateTime)::INT dateTime.Second date_part('second', dateTime)::INT dateTime.Year date_part('year', dateTime)::INT dateTime.ToUniversalTime dateTime::timestamptz Added in 6.0 dateTime.ToLocalTime dateTime::timestamp Added in 6.0 dateTimeOffset.DateTime dateTimeOffset AT TIME ZONE 'UTC' Added in 6.0 dateTimeOffset.UtcDateTime No PG operation (.NET-side conversion from DateTimeOffset to DateTime only) Added in 6.0 dateTimeOffset.LocalDateTime dateTimeOffset::timestamp Added in 6.0 timeSpan.Days floor(date_part('day', timeSpan))::INT timeSpan.Hours floor(date_part('hour', timeSpan))::INT timeSpan.Minutes floor(date_part('minute', timeSpan))::INT timeSpan.Seconds floor(date_part('second', timeSpan))::INT timeSpan.Milliseconds floor(date_part('millisecond', timeSpan))::INT timeSpan.Milliseconds floor(date_part('millisecond', timeSpan))::INT timeSpan.TotalMilliseconds date_part('epoch', interval) / 0.001 Added in 6.0 timeSpan.TotalSeconds date_part('epoch', interval) Added in 6.0 timeSpan.TotalMinutes date_part('epoch', interval) / 60.0 Added in 6.0 timeSpan.TotalDays date_part('epoch', interval) / 86400.0 Added in 6.0 timeSpan.TotalHours date_part('epoch', interval) / 3600.0 Added in 6.0 dateTime1 - dateTime2 dateTime1 - dateTime2 TimeZoneInfo.ConvertTimeBySystemTimeZoneId(utcDateTime, timezone) utcDateTime AT TIME ZONE timezone Added in 6.0, only for timestamptz columns TimeZoneInfo.ConvertTimeToUtc(nonUtcDateTime) nonUtcDateTime::timestamptz Added in 6.0, only for timestamp columns DateTime.SpecifyKind(utcDateTime, DateTimeKind.Unspecified) utcDateTime AT TIME ZONE 'UTC' Added in 6.0, only for timestamptz columns DateTime.SpecifyKind(nonUtcDateTime, DateTimeKind.Utc) nonUtcDateTime AT TIME ZONE 'UTC' Added in 6.0, only for timestamp columns new DateTime(year, month, day) make_date(year, month, day) new DateTime(y, m, d, h, m, s) make_timestamp(y, m, d, h, m, s) new DateTime(y, m, d, h, m, s, kind) make_timestamp or make_timestamptz, based on kind Added in 6.0 EF.Functions.Sum(timespans) sum(timespans) Added in 7.0, see Aggregate functions. EF.Functions.Average(timespans) avg(timespans) Added in 7.0, see Aggregate functions. Miscellaneous functions .NET SQL collection.Contains(item) item IN collection enumValue.HasFlag(flag) enumValue & flag = flag Guid.NewGuid() uuid_generate_v4(), or gen_random_uuid() on PostgreSQL 13 with EF Core 5 and above. nullable.GetValueOrDefault() coalesce(nullable, 0) nullable.GetValueOrDefault(defaultValue) coalesce(nullable, defaultValue) Binary functions .NET SQL Notes bytes[i] get_byte(bytes, i) Added in 5.0 bytes.Contains(value) position(value IN bytes) > 0 Added in 5.0 bytes.Length length(@bytes) Added in 5.0 bytes1.SequenceEqual(bytes2) @bytes = @second Added in 5.0 Math functions .NET SQL Notes Math.Abs(value) abs(value) Math.Acos(d) acos(d) Math.Asin(d) asin(d) Math.Atan(d) atan(d) Math.Atan2(y, x) atan2(y, x) Math.Ceiling(d) ceiling(d) Math.Cos(d) cos(d) Math.Exp(d) exp(d) Math.Floor(d) floor(d) Math.Log(d) ln(d) Math.Log10(d) log(d) Math.Max(x, y) greatest(x, y) Math.Min(x, y) least(x, y) Math.Pow(x, y) power(x, y) Math.Round(d) round(d) Math.Round(d, decimals) round(d, decimals) Math.Sin(a) sin(a) Math.Sign(value) sign(value)::int Math.Sqrt(d) sqrt(d) Math.Tan(a) tan(a) Math.Truncate(d) trunc(d) EF.Functions.Random() random() Added in 6.0 See also Aggregate statistics functions. Row value comparisons The following allow expressing comparisons over SQL row values. This are particularly useful for implementing efficient pagination, see the EF Core docs for more information. Note All of the below were introduced in version 7.0 of the provider. .NET SQL EF.Functions.GreaterThan(ValueTuple.Create(a, b), ValueTuple.Create(c, d)) (a, b) > (c, d) EF.Functions.LessThan(ValueTuple.Create(a, b), ValueTuple.Create(c, d)) (a, b) < (c, d) EF.Functions.GreaterThanOrEqual(ValueTuple.Create(a, b), ValueTuple.Create(c, d)) (a, b) >= (c, d) EF.Functions.LessThanOrEqual(ValueTuple.Create(a, b), ValueTuple.Create(c, d)) (a, b) <= (c, d) ValueTuple.Create(a, b).Equals(ValueTuple.Create(c, d)) (a, b) = (c, d) !ValueTuple.Create(a, b).Equals(ValueTuple.Create(c, d)) (a, b) <> (c, d) Network functions Note As of Npgsql 8.0, IPAddress and NpgsqlCidr are implicitly convertible to NpgsqlInet, and so can be used with the functions below which accept inet. .NET SQL IPAddress.Parse(string) CAST(string AS inet) PhysicalAddress.Parse(string) CAST(string AS macaddr) EF.Functions.LessThan(net1, net2) net1 < net2 EF.Functions.LessThanOrEqual(net1, net2) net1 <= net2 EF.Functions.GreaterThan(net1, net2) net1 > net2 EF.Functions.GreaterThanOrEqual(net1, net2) net1 >= net2 EF.Functions.ContainedBy(inet1, inet2) inet1 << inet2 EF.Functions.ContainedByOrEqual(inet1, inet2) inet1 <<= inet2 EF.Functions.Contains(inet1, inet2) inet1 >> inet2 EF.Functions.ContainsOrEqual(inet1, inet2) inet1 >>= inet2 EF.Functions.ContainsOrContainedBy(inet1, inet2) inet1 && inet2 EF.Functions.BitwiseNot(net) ~net1 EF.Functions.BitwiseAnd(net1, net2) net1 & net2 EF.Functions.BitwiseOr(net1, net2) net1 | net2 EF.Functions.Add(inet, int) inet + int EF.Functions.Subtract(inet, int) inet - int EF.Functions.Subtract(inet1, inet2) inet1 - inet2 EF.Functions.Abbreviate(inet) abbrev(inet) EF.Functions.Abbreviate(cidr) abbrev(cidr) EF.Functions.Broadcast(inet) broadcast(inet) EF.Functions.Family(inet) family(inet) EF.Functions.Host(inet) host(inet) EF.Functions.HostMark(inet) hostmask(inet) EF.Functions.MaskLength(inet) masklen(inet) EF.Functions.Netmask(inet) netmask(inet) EF.Functions.Network(inet) network(inet) EF.Functions.SetMaskLength(inet) set_masklen(inet) EF.Functions.SetMaskLength(cidr) set_masklen(cidr) EF.Functions.Text(inet) text(inet) EF.Functions.SameFamily(inet1, inet2) inet_same_family(inet1, inet2) EF.Functions.Merge(inet1, inet2) inet_merge(inet1, inet2) EF.Functions.Truncate(macaddr) trunc(macaddr) EF.Functions.Set7BitMac8(macaddr8) macaddr8_set7bit(macaddr8) Trigram functions The below translations provide functionality for determining the similarity of alphanumeric text based on trigram matching, using the pg_trgm extension which is bundled with standard PostgreSQL distributions. All the below parameters are strings. Note Prior to version 6.0, to use these translations, your project must depend on the Npgsql.EntityFrameworkCore.PostgreSQL.Trigrams package, and call UseTrigrams() in your OnModelConfiguring. .NET SQL EF.Functions.TrigramsShow(s) show_trgm(s) EF.Functions.TrigramsSimilarity(s1, s2) similarity(s1, s2) EF.Functions.TrigramsWordSimilarity(s1, s2) word_similarity(s1, s2) EF.Functions.TrigramsStrictWordSimilarity(s1, s2) strict_word_similarity(s1, s2) EF.Functions.TrigramsAreSimilar(s1, s2) s1 % s2 EF.Functions.TrigramsAreWordSimilar(s1, s2) s1 <% s2 EF.Functions.TrigramsAreNotWordSimilar(s1, s2) s1 %> s2 EF.Functions.TrigramsAreStrictWordSimilar(s1, s2) s1 <<% s2 EF.Functions.TrigramsAreNotStrictWordSimilar(s1, s2) s1 %>> s2 EF.Functions.TrigramsSimilarityDistance(s1, s2) s1 <-> s2 EF.Functions.TrigramsWordSimilarityDistance(s1, s2) s1 <<-> s2 EF.Functions.TrigramsWordSimilarityDistanceInverted(s1, s2) s1 <->> s2 EF.Functions.TrigramsStrictWordSimilarityDistance(s1, s2) s1 <<<-> s2 EF.Functions.TrigramsStrictWordSimilarityDistanceInverted(s1, s2) s1 <->>> s2 LTree functions The below translations are for working with label trees from the PostgreSQL ltree extension. Use the <xref:Microsoft.EntityFrameworkCore.LTree> type to represent ltree and invoke methods on it in EF Core LINQ queries. Note LTree support was introduced in version 6.0 of the provider, and requires PostgreSQL 13 or later. .NET SQL ltree1.IsAncestorOf(ltree2) ltree1 @> ltree2 ltree1.IsDescendantOf(ltree2) ltree1 <@ ltree2 ltree.MatchesLQuery(lquery) ltree ~ lquery ltree.MatchesLTxtQuery(ltxtquery) ltree @ ltxtquery lqueries.Any(q => ltree.MatchesLQuery(q)) ltree ? lqueries ltrees.Any(t => t.IsAncestorOf(ltree)) ltrees @> ltree ltrees.Any(t => t.IsDescendantOf(ltree)) ltrees <@ ltree ltrees.Any(t => t.MatchesLQuery(lquery)) ltrees ~ ltree ltrees.Any(t => t.MatchesLTxtQuery(ltxtquery)) ltrees @ ltxtquery ltrees.Any(t => lqueries.Any(q => t.MatchesLQuery(q))) ltrees ? lqueries ltrees.FirstOrDefault(l => l.IsAncestorOf(ltree)) ltrees ?@> ltree ltrees.FirstOrDefault(l => l.IsDescendantOf(ltree)) ltrees ?<@ ltree ltrees.FirstOrDefault(l => l.MatchesLQuery(lquery)) ltrees ?~ ltree ltrees.FirstOrDefault(l => l.MatchesLTxtQuery(ltxtquery)) ltrees ?@ ltree ltree.Subtree(0, 1) subltree(ltree, 0, 1) ltree.Subpath(0, 1) sublpath(ltree, 0, 1) ltree.Subpath(2) sublpath(ltree, 2) ltree.NLevel nlevel(ltree) ltree.Index(subpath) index(ltree, subpath) ltree.Index(subpath, 2) index(ltree, subpath, 2) LTree.LongestCommonAncestor(ltree1, ltree2) lca(index(ltree1, ltree2) Aggregate functions The PostgreSQL aggregate functions are documented here. Note All the below aggregate functions were added in version 7.0. .NET SQL string.Join(\", \", agg_strings) string_agg(agg_strings, ', ') EF.Functions.ArrayAgg(values) array_agg(values) EF.Functions.JsonbAgg(values) jsonb_agg(values) EF.Functions.JsonAgg(values) json_agg(values) EF.Functions.Sum(timespans) sum(timespans) EF.Functions.Average(timespans) avg(timespans) EF.Functions.JsonObjectAgg(tuple_of_2) json_object_agg(tuple_of_2.first, tuple_of_2.second) ranges.RangeAgg() range_agg(ranges) ranges.RangeIntersectAgg() range_intersect_agg(ranges) multiranges.RangeIntersectAgg() range_intersect_agg(multiranges) EF.Functions.StandardDeviationSample(values) stddev_samp(values) EF.Functions.StandardDeviationPopulation(values) stddev_pop(values) EF.Functions.VarianceSample(values) var_samp(values) EF.Functions.VariancePopulation(values) var_pop(values) EF.Functions.Correlation(tuple) corr(tuple_of_2.first, tuple_of_2.second) EF.Functions.CovariancePopulation(tuple) covar_pop(tuple_of_2.first, tuple_of_2.second) EF.Functions.CovarianceSample(tuple) covar_samp(tuple_of_2.first, tuple_of_2.second) EF.Functions.RegrAverageX(tuple) regr_avgx(tuple_of_2.first, tuple_of_2.second) EF.Functions.RegrAverageY(tuple) regr_avgy(tuple_of_2.first, tuple_of_2.second) EF.Functions.RegrCount(tuple) regr_count(tuple_of_2.first, tuple_of_2.second) EF.Functions.RegrIntercept(tuple) regr_intercept(tuple_of_2.first, tuple_of_2.second) EF.Functions.RegrR2(tuple) regr_r2(tuple_of_2.first, tuple_of_2.second) EF.Functions.RegrSlope(tuple) regr_slope(tuple_of_2.first, tuple_of_2.second) EF.Functions.RegrSXX(tuple) regr_sxx(tuple_of_2.first, tuple_of_2.second) EF.Functions.RegrSXY(tuple) regr_sxy(tuple_of_2.first, tuple_of_2.second) Aggregate functions can be used as follows: var query = ctx.Set<Customer>() .GroupBy(c => c.City) .Select( g => new { City = g.Key, Companies = EF.Functions.ArrayAgg(g.Select(c => c.ContactName)) }); To use functions accepting a tuple_of_2, project out from the group as follows: var query = ctx.Set<Customer>() .GroupBy(c => c.City) .Select( g => new { City = g.Key, Companies = EF.Functions.JsonObjectAgg(g.Select(c => ValueTuple.Create(c.CompanyName, c.ContactName))) });"
  },
  "doc/conceptual/EFCore.PG/misc/collations-and-case-sensitivity.html": {
    "href": "doc/conceptual/EFCore.PG/misc/collations-and-case-sensitivity.html",
    "title": "Collations and Case Sensitivity | Npgsql Documentation",
    "keywords": "Collations and Case Sensitivity Note This feature is introduced in EF Core 5.0. It's recommended that you start by reading the general Entity Framework Core docs on collations and case sensitivity. PostgreSQL is a case-sensitive database by default, but provides various possibilities for performing case-insensitive operations and working with collations. Unfortunately, full collation support is recent and somewhat incomplete, so you may need to carefully review your options below and pick the one which suits you. PostgreSQL collations While PostgreSQL has supported collations for a long time, supported was limited to \"deterministic\" collations, which did not allow for case-insensitive or accent-insensitive operations. PostgreSQL 12 introduced non-deterministic ICU collations, so it is now possible to use collations in a more flexible way. Read more about PostgreSQL collation support in the documentation. Note It is not yet possible to use pattern matching operators such as LIKE on columns with a non-deterministic collation. Creating a collation In PostgreSQL, collations are first-class, named database objects which can be created and dropped, just like tables. To create a collation, place the following in your context's OnModelCreating: modelBuilder.HasCollation(\"my_collation\", locale: \"en-u-ks-primary\", provider: \"icu\", deterministic: false); This creates a collation with the name my_collation: this is an arbitrary name you can choose, which you will be specifying later when assigning the collation to columns. The rest of the parameters instruct PostgreSQL to create a non-deterministic, case-insensitive ICU collation. ICU collations are very powerful, and allow you to specify precise rules with regards to case, accents and other textual aspects. Consult the ICU docs for more information on supported features and keywords. Column collation Once a collation has been created in your database, you can specify it on columns: protected override void OnModelCreating(ModelBuilder modelBuilder) { modelBuilder.HasCollation(\"my_collation\", locale: \"en-u-ks-primary\", provider: \"icu\", deterministic: false); modelBuilder.Entity<Customer>().Property(c => c.Name) .UseCollation(\"my_collation\"); } This will cause all textual operators on this column to be case-insensitive. Database collation PostgreSQL also allows you to specify collations at the database level, when it is created: protected override void OnModelCreating(ModelBuilder modelBuilder) { modelBuilder.UseCollation(\"<collation_name>\"); } Unfortunately, the database collation is quite limited in PostgreSQL; it notably does not support non-deterministic collations (e.g. case-insensitive ones). To work around this limitation, you can use EF Core's pre-convention model configuration feature: protected override void ConfigureConventions(ModelConfigurationBuilder configurationBuilder) { configurationBuilder.Properties<string>().UseCollation(\"my_collation\"); } All columns created with this configuration will automatically have their collation specified accordingly, and all existing columns will be altered. The end result of the above is very similar to specifying a database collation: instead of telling PostgreSQL to implicit apply a collation to all columns, EF Core will do the same for you in its migrations. The citext type The older PostgreSQL method for performing case-insensitive text operations is the citext type; it is similar to the text type, but operators are functions between citext values are implicitly case-insensitive. The PostgreSQL docs provide more information on this type. citext is available in a PostgreSQL-bundled extension, so you'll first have to install it: modelBuilder.HasPostgresExtension(\"citext\"); Specifying that a column should use citext is simply a matter of setting the column's type: Data Annotations Fluent API public class Blog { public int Id { get; set; } [Column(TypeName = \"citext\")] public string Name { get; set; } } protected override void OnModelCreating(ModelBuilder modelBuilder) { modelBuilder.Entity<Blog>().Property(b => b.Name) .HasColumnType(\"citext\"); } Some limitations (others are listed in the PostgreSQL docs): While citext allows case-insensitive comparisons, it doesn't handle other aspects of collations, such as accents. Several PostgreSQL text functions are overloaded to work with citext as expected, but others aren't. Using a function that isn't overloaded will result in a regular, case-sensitive match. Unlike collations, citext does not allow the same column to be compared case-sensitively in some queries, and and insensitively in others. ILIKE ILIKE is a PostgreSQL-specific operator that works just like LIKE, but is case-insensitive. If you only need to perform case-insensitive LIKE pattern matching, then this could be sufficient. The provider exposes this via EF.Functions.ILike: var results = ctx.Blogs .Where(b => EF.Functions.ILike(b.Name, \"a%b\")) .ToList();"
  },
  "doc/conceptual/EFCore.PG/misc/database-creation.html": {
    "href": "doc/conceptual/EFCore.PG/misc/database-creation.html",
    "title": "Database Creation | Npgsql Documentation",
    "keywords": "Database Creation Specifying the administrative db When the Npgsql EF Core provider creates or deletes a database (EnsureCreated(), EnsureDeleted()), it must connect to an administrative database which already exists (with PostgreSQL you always have to be connected to some database, even when creating/deleting another database). Up to now the postgres database was used, which is supposed to always be present. However, there are some PostgreSQL-like databases where the postgres database is not available. For these cases you can specify the administrative database as follows: protected override void OnConfiguring(DbContextOptionsBuilder optionsBuilder) => optionsBuilder.UseNpgsql( \"<connection_string>\", options => options.UseAdminDatabase(\"my_admin_db\")); Using a database template When creating a new database, PostgreSQL allows specifying another \"template database\" which will be copied as the basis for the new one. This can be useful for including database entities which are not managed by Entity Framework Core. You can trigger this by using HasDatabaseTemplate in your context's OnModelCreating: protected override void OnModelCreating(ModelBuilder modelBuilder) => modelBuilder.UseDatabaseTemplate(\"my_template_db\"); Setting a tablespace PostgreSQL allows you to locate your database in different parts of your filesystem, via tablespaces. The Npgsql EF Core provider allows you to specify your database's namespace: protected override void OnModelCreating(ModelBuilder modelBuilder) => modelBuilder.UseTablespace(\"my_tablespace\"); You must have created your tablespace prior to this via the CREATE TABLESPACE command - the Npgsql EF Core provider does not do this for you. Note also that specifying a tablespace on specific tables is not supported."
  },
  "doc/conceptual/EFCore.PG/misc/other.html": {
    "href": "doc/conceptual/EFCore.PG/misc/other.html",
    "title": "Other | Npgsql Documentation",
    "keywords": "Other PostgreSQL extensions The Npgsql EF Core provider allows you to specify PostgreSQL extensions that should be set up in your database. Simply use HasPostgresExtension in your context's OnModelCreating method: protected override void OnModelCreating(ModelBuilder modelBuilder) => modelBuilder.HasPostgresExtension(\"hstore\"); Execution Strategy The Npgsql EF Core provider provides a retrying execution strategy, which will attempt to detect most transient PostgreSQL/network errors and will automatically retry your operation. To enable, place the following code in your context's OnModelConfiguring: protected override void OnConfiguring(DbContextOptionsBuilder optionsBuilder) => optionsBuilder.UseNpgsql( \"<connection_string>\", options => options.EnableRetryOnFailure()); This strategy relies on the IsTransient property of NpgsqlException. Certificate authentication The Npgsql allows you to provide a callback for verifying the server-provided certificates, and to provide a callback for providing certificates to the server. The latter, if properly set up on the PostgreSQL side, allows you to do client certificate authentication - see the Npgsql docs and also the PostgreSQL docs on setting this up. The Npgsql EF Core provider allows you to set these two callbacks on the DbContextOptionsBuilder as follows: protected override void OnConfiguring(DbContextOptionsBuilder optionsBuilder) => optionsBuilder.UseNpgsql( \"<connection_string>\", options => { options.RemoteCertificateValidationCallback(MyCallback1); options.ProvideClientCertificatesCallback(MyCallback2); }); You may also consider passing Trust Server Certificate=true in your connection string to make Npgsql accept whatever certificate your PostgreSQL provides (useful for self-signed certificates). Caution When specifying the options via OnConfiguring, make sure that the callbacks you pass in are static methods. Passing in instance methods causes EF Core to create a new service provider for each context instance, which can degrade performance in a significant way. CockroachDB Interleave In Parent If you're using CockroachDB, the Npgsql EF Core provider exposes its \"interleave in parent\" feature. Use the following code: protected override void OnModelCreating(ModelBuilder modelBuilder) => modelBuilder.Entity<Customer>() .UseCockroachDbInterleaveInParent( typeof(ParentEntityType), new List<string> { \"prefix_column_1\", \"prefix_column_2\" });"
  },
  "doc/conceptual/EFCore.PG/modeling/concurrency.html": {
    "href": "doc/conceptual/EFCore.PG/modeling/concurrency.html",
    "title": "Concurrency Tokens | Npgsql Documentation",
    "keywords": "Concurrency Tokens Note Please read the general Entity Framework Core docs on concurrency tokens. Entity Framework Core supports the concept of optimistic concurrency - a property on your entity is designated as a concurrency token, and EF Core detects concurrent modifications by checking whether that token has changed since the entity was read. The PostgreSQL xmin system column Although applications can update concurrency tokens themselves, we frequently rely on the database automatically updating a column on update - a \"last modified\" timestamp, an SQL Server rowversion, etc. Unfortunately PostgreSQL doesn't have such auto-updating columns - but there is one feature that can be used for concurrency token. All PostgreSQL tables have a set of implicit and hidden system columns, among which xmin holds the ID of the latest updating transaction. Since this value automatically gets updated every time the row is changed, it is ideal for use as a concurrency token. Starting with version 7.0, you can map a uint property to the PostgreSQL xmin system column using the standard EF Core mechanisms: Data Annotations Fluent API public class SomeEntity { public int Id { get; set; } [Timestamp] public uint Version { get; set; } } class MyContext : DbContext { public DbSet<SomeEntity> SomeEntities { get; set; } protected override void OnModelCreating(ModelBuilder modelBuilder) { modelBuilder.Entity<SomeEntity>() .Property(b => b.Version) .IsRowVersion(); } } public class SomeEntity { public int Id { get; set; } public uint Version { get; set; } } In older version of the provider, use the following instead: protected override void OnModelCreating(ModelBuilder modelBuilder) { modelBuilder.Entity<Blog>().UseXminAsConcurrencyToken(); }"
  },
  "doc/conceptual/EFCore.PG/modeling/generated-properties.html": {
    "href": "doc/conceptual/EFCore.PG/modeling/generated-properties.html",
    "title": "Value Generation | Npgsql Documentation",
    "keywords": "Value Generation Caution In 3.0.0, the default value generation strategy has changed from the older SERIAL columns to the newer IDENTITY columns. Read the information below carefully if you are migrating an existing database. Note It's recommended that you start by reading the general Entity Framework Core docs on generated properties. Identity and serial columns (auto-increment) Introduction Since PostgreSQL 10, the standard way to define auto-incrementing columns is \"identity columns\". Prior to version 10, \"serial columns\" were used, which are less SQL-compliant and generally more difficult to manage. For more information on these, see this blog post. Note that since PostgreSQL 10, both support smallint, integer and bigint as their data type. The Npgsql EF Core provider allows you to choose which of the above you want on a property-by-property basis, or globally on your model. The following \"value generation strategies\" are available: Identity by default: an identity column whose values are by default generated at the database, but you can still override this behavior by providing values from your application. This will generate the clause GENERATED BY DEFAULT AS IDENTITY on your column, and is the default value generation strategy. Identity always: an identity column whose values are always generated at the database - you cannot provide values from your application. This will generate the clause GENERATED ALWAYS AS IDENTITY on your column. Serial: the traditional PostgreSQL serial column. This will create the column with the serial datatype. Recommended only if you are using an older PostgreSQL version. Sequence HiLo: See below The default value generation strategy is \"identity by default\". In other words, when EF decides that an int property should be value generated (e.g. because it's named Id, or because you explicitly specified ValueGeneratedOnAdd on it), the Npgsql provider will automatically map it to an identity column. Defining the default strategy for the entire model You can easily control the value generation strategy for the entire model. For example, to opt out of the change to identity columns, simply place the following in your context's OnModelCreating(): protected override void OnModelCreating(ModelBuilder modelBuilder) => modelBuilder.UseIdentityByDefaultColumns(); Defining the strategy for a single property Regardless of the model default, you can define a value-generation strategy on a property-by-property basis: protected override void OnModelCreating(ModelBuilder modelBuilder) => modelBuilder.Entity<Blog>().Property(b => b.Id).UseIdentityAlwaysColumn(); Identity sequence options Identity columns have a standard sequence, managed behind the scenes by PostgreSQL; you can customize the sequence options for these. For example, the following makes the column values start at 100: protected override void OnModelCreating(ModelBuilder modelBuilder) => modelBuilder.Entity<Blog>().Property(b => b.Id) .HasIdentityOptions(startValue: 100); This can be especially useful when seeding data. Seeded data must explicitly specify all columns - including database-generated ones - but the backing sequence for identity columns isn't aware that the values are in use, and will generate conflicting values. This technique allows to start your identity sequence at a value higher than all seeded data values. Another strategy is to seed negative values only, allowing your identity column to start at 1. It is not possible to specify sequence options for serial columns, but you can set up a sequence separately and configure the column's default value (see sequence-driven columns). Standard sequence-driven columns While identity and serial columns set up a sequence for you behind the scenes, sometimes you may want to manage sequence creation yourself. For example, you may want to have multiple columns drawing their default values from a single sequence. Adding a sequence to your model is described in the general EF Core documentation; once the sequence is specified, you can simply set a column's default value to extract the next value from that sequence. Note that the SQL used to fetch the next value from a sequence differs across databases (see the PostgreSQL docs). Your models' OnModelCreating should look like this: protected override void OnModelCreating(ModelBuilder modelBuilder) { modelBuilder.HasSequence<int>(\"OrderNumbers\") .StartsAt(1000) .IncrementsBy(5); modelBuilder.Entity<Order>() .Property(o => o.OrderNo) .HasDefaultValueSql(\"nextval('\\\"OrderNumbers\\\"')\"); } HiLo Autoincrement Generation One disadvantage of database-generated values is that these values must be read back from the database after a row is inserted. If you're saving multiple related entities, this means you must perform multiple round-trips as the first entity's generated key must be read before writing the second one. One solution to this problem is HiLo value generation: rather than relying on the database to generate each and every value, the application \"allocates\" a range of values, which it can then populate directly on new entities without any additional round-trips. When the range is exhausted, a new range is allocated. In practical terms, this uses a sequence that increments by some large value (100 by default), allowing the application to insert 100 rows autonomously. To use HiLo, specify UseHiLo on a property in your model's OnModelCreating: protected override void OnModelCreating(ModelBuilder modelBuilder) => modelBuilder.Entity<Blog>() .Property(b => b.Id) .UseHiLo(); You can also make your model use HiLo everywhere: protected override void OnModelCreating(ModelBuilder modelBuilder) => modelBuilder.UseHiLo(); GUID/UUID Generation By default, for GUID key properties, a GUID is generated client-side by the EF provider and sent to the database. From version 9.0 and onwards, these GUIDs are sequential (version 7), which are more optimized for database indexes (before version 9.0, these GUIDs were random). To have the provider generate GUIDs client-side for non-key properties, configure them as follows: protected override void OnModelCreating(ModelBuilder modelBuilder) { modelBuilder .Entity<Blog>() .Property(b => b.SomeGuidProperty) .HasValueGenerator<NpgsqlSequentialGuidValueGenerator>(); } If you prefer to generate values in the database instead, you can do so by specifying HasDefaultValueSql on your property, and call the function to generate the value in the SQL expression. Which function to use depends on your PostgreSQL version: PG 13+ Older protected override void OnModelCreating(ModelBuilder modelBuilder) { modelBuilder .Entity<Blog>() .Property(e => e.SomeGuidProperty) .HasDefaultValueSql(\"gen_random_uuid()\"); } Versions of PostgreSQL prior to 13 don't include any GUID/UUID generation functions, but extensions such as uuid-ossp or pgcrypto exist to fill thie gap. This can be done by placing the following code in your model's OnModelCreating: protected override void OnModelCreating(ModelBuilder modelBuilder) { modelBuilder.HasPostgresExtension(\"uuid-ossp\"); modelBuilder .Entity<Blog>() .Property(e => e.SomeGuidProperty) .HasDefaultValueSql(\"uuid_generate_v4()\"); } See the PostgreSQL docs on UUID for more details. Note Generating Guid values in the database causes an additional network roundtrip when a principal and a dependent are inserted in the same SaveChanges, as the principal's key needs to be fetched before inserting the dependent's. Timestamp generation In many scenarios, it's useful to have a column containing the timestamp when the row was originally created. To do this, add a DateTime property to your entity type (or Instant if using NodaTime) , and configure its default with HasDefaultValueSql as follows: protected override void OnModelCreating(ModelBuilder modelBuilder) { modelBuilder .Entity<Blog>() .Property(e => e.SomeDateTimeProperty) .HasDefaultValueSql(\"now()\"); } In other scenarios, a \"last updated\" is needed, which is automatically updated every time is modified. Unfortunately, while PostgreSQL supports generated columns, the use of functions such as now() isn't supported. It's still possible to use database trigger to set this up; triggers can be managed by adding raw SQL to your migrations, as follows: CREATE FUNCTION \"Blogs_Update_Timestamp_Function\"() RETURNS TRIGGER LANGUAGE PLPGSQL AS $$ BEGIN NEW.\"Timestamp\" := now(); RETURN NEW; END; $$; CREATE TRIGGER \"UpdateTimestamp\" BEFORE INSERT OR UPDATE ON \"Blogs\" FOR EACH ROW EXECUTE FUNCTION \"Blogs_Update_Timestamp_Function\"(); Computed Columns Note This feature works only on PostgreSQL 12 or above. PostgreSQL 12 added support for stored generated columns, and Npgsql feature supports that feature as well: Version 5.0 Version 3.x protected override void OnModelCreating(ModelBuilder modelBuilder) { modelBuilder.Entity<Person>() .Property(p => p.DisplayName) .HasComputedColumnSql(@\"\"\"FirstName\"\" || ' ' || \"\"LastName\"\"\", stored: true); } protected override void OnModelCreating(ModelBuilder modelBuilder) { modelBuilder.Entity<Person>() .Property(p => p.DisplayName) .HasComputedColumnSql(@\"\"\"FirstName\"\" || ' ' || \"\"LastName\"\"\"); } This will cause the following migration SQL to be generated: ALTER TABLE \"\"Person\"\" ADD \"\"DisplayName\"\" text GENERATED ALWAYS AS (\"\"FirstName\"\" || ' ' || \"\"LastName\"\") STORED; Note that this is a stored column - it is computed once when the row is updated, and takes space on disk. Virtual computed columns, which are computed on each select, are not yet supported by PostgreSQL."
  },
  "doc/conceptual/EFCore.PG/modeling/indexes.html": {
    "href": "doc/conceptual/EFCore.PG/modeling/indexes.html",
    "title": "Indexes | Npgsql Documentation",
    "keywords": "Indexes PostgreSQL and the Npgsql provider support the standard index modeling described in the EF Core docs. This page describes some supported PostgreSQL-specific features. Covering indexes (INCLUDE) PostgreSQL supports covering indexes, which allow you to include \"non-key\" columns in your indexes. This allows you to perform index-only scans and can provide a significant performance boost: protected override void OnModelCreating(ModelBuilder modelBuilder) => modelBuilder.Entity<Blog>() .HasIndex(b => b.Id) .IncludeProperties(b => b.Name); This will create an index for searching on Id, but containing also the column Name, so that reading the latter will not involve accessing the table. The SQL generated is as follows: CREATE INDEX \"IX_Blog_Id\" ON blogs (\"Id\") INCLUDE (\"Name\"); Treating nulls as non-distinct Note This feature was introduced in version 7.0, and is available starting with PostgreSQL 15. By default, when you create a unique index, PostgreSQL treats null values as distinct; this means that a unique index can contain multiple null values in a column. When creating an index, you can also instruct PostgreSQL that nulls should be treated as non-distinct; this causes a unique constraint violation to be raised if a column contains multiple null values: protected override void OnModelCreating(ModelBuilder modelBuilder) => modelBuilder.Entity<Blog>() .IsUnique() .AreNullsDistinct(false); Index methods PostgreSQL supports a number of index methods, or types. These are specified at index creation time via the USING <method> clause, see the PostgreSQL docs for CREATE INDEX and this page for information on the different types. The Npgsql EF Core provider allows you to specify the index method to be used by calling HasMethod() on your index in your context's OnModelCreating method: protected override void OnModelCreating(ModelBuilder modelBuilder) => modelBuilder.Entity<Blog>() .HasIndex(b => b.Url) .HasMethod(\"gin\"); Index operator classes PostgreSQL allows you to specify operator classes on your indexes, to allow tweaking how the index should work. Use the following code to specify an operator class: protected override void OnConfiguring(DbContextOptionsBuilder builder) => modelBuilder.Entity<Blog>() .HasIndex(b => new { b.Id, b.Name }) .HasOperators(null, \"text_pattern_ops\"); Note that each operator class is used for the corresponding index column, by order. In the example above, the text_pattern_ops class will be used for the Name column, while the Id column will use the default class (unspecified), producing the following SQL: CREATE INDEX \"IX_blogs_Id_Name\" ON blogs (\"Id\", \"Name\" text_pattern_ops); Storage parameters PostgreSQL allows configuring indexes with storage parameters, which can tweak their behaviors in various ways; which storage parameters are available depends on the chosen index method. See the PostgreSQL documentation for more information. To configure a storage parameter on an index, use the following code: protected override void OnModelCreating(ModelBuilder modelBuilder) => modelBuilder.Entity<Blog>() .HasIndex(b => b.Url) .HasStorageParameter(\"fillfactor\", 70); Creating indexes concurrently Creating an index can interfere with regular operation of a database. Normally PostgreSQL locks the table to be indexed against writes and performs the entire index build with a single scan of the table. Other transactions can still read the table, but if they try to insert, update, or delete rows in the table they will block until the index build is finished. This could have a severe effect if the system is a live production database. Very large tables can take many hours to be indexed, and even for smaller tables, an index build can lock out writers for periods that are unacceptably long for a production system. The EF provider allows you to specify that an index should be created concurrently, partially mitigating the above issues: protected override void OnModelCreating(ModelBuilder modelBuilder) => modelBuilder.Entity<Blog>() .HasIndex(b => b.Url) .IsCreatedConcurrently(); Caution Do not enable this feature before reading the PostgreSQL documentation and understanding the full implications of concurrent index creation. Note Prior to version 5.0, IsCreatedConcurrently erroneously defaulted to false - explicitly pass true to configure the index for concurrent creation"
  },
  "doc/conceptual/EFCore.PG/modeling/tables.html": {
    "href": "doc/conceptual/EFCore.PG/modeling/tables.html",
    "title": "Tables | Npgsql Documentation",
    "keywords": "Tables Naming By default, EF Core will map to tables and columns named exactly after your .NET classes and properties, so an entity type named BlogPost will be mapped to a PostgreSQL table called BlogPost. While there's nothing wrong with that, the PostgreSQL world tends towards snake_case naming instead. In addition, any upper-case letters in unquoted identifiers are automatically converted to lower-case identifiers, so the Npgsql provider generates quotes around all such identifiers. Starting with 3.0.0, you can use the EFCore.NamingConventions plugin to automatically set all your table and column names to snake_case instead: protected override void OnConfiguring(DbContextOptionsBuilder optionsBuilder) => optionsBuilder .UseNpgsql(...) .UseSnakeCaseNamingConvention(); public class Customer { public int Id { get; set; } public string FullName { get; set; } } This will cause cleaner SQL such as the following to be generated: CREATE TABLE customers ( id integer NOT NULL GENERATED BY DEFAULT AS IDENTITY, full_name text NULL, CONSTRAINT \"PK_customers\" PRIMARY KEY (id); SELECT c.id, c.full_name FROM customers AS c WHERE c.full_name = 'John Doe'; See the plugin documentation for more details, Storage parameters PostgreSQL allows configuring tables with storage parameters, which can tweak storage behavior in various ways; see the PostgreSQL documentation for more information. To configure a storage parameter on a table, use the following code: protected override void OnModelCreating(ModelBuilder modelBuilder) => modelBuilder.Entity<Blog>().HasStorageParameter(\"fillfactor\", 70);"
  },
  "doc/conceptual/EFCore.PG/release-notes/1.1.html": {
    "href": "doc/conceptual/EFCore.PG/release-notes/1.1.html",
    "title": "Migrating to 1.1 | Npgsql Documentation",
    "keywords": "Migrating to 1.1 Version 1.1.0 of the Npgsql Entity Framework Core provider has been released and is available on nuget. This version works with version 1.1.0 of Entity Framework Core, and contains some new Npgsql features as well. Note that if you're using the command-line tools, you'll have to modify your tools section as described in the EF Core release post: \"tools\": { \"Microsoft.EntityFrameworkCore.Tools.DotNet\": \"1.0.0-preview3-final\" }, New Features Aside from general EF Core features, version 1.1.0 of the Npgsql provider contains the following: Hilo key generation (#5). This can be a much more efficient way to generate autoincrement key values. PostgreSQL array mapping (#15). This allows you to have plain CLR arrays on your entities, and have those arrays mapped to native PostgreSQL array columns. Optimistic concurrency with PostgreSQL's xmin column (#19). Simply specify .UseXminAsConcurrencyToken() on an entity to start using this, see the EF docs for more details. Cleanup of how serial (autoincrement) and generated GUID/UUID columns are managed. Here's the full list of issues. Please report any problems to https://github.com/npgsql/Npgsql.EntityFrameworkCore.PostgreSQL. Upgrading from 1.0.x If you've used 1.0.x without migrations, you can simply upgrade and everything should just work. Unfortunately, if you already have migrations from 1.0.x you'll have to do some manual fixups because of some bad decisions that were previously made. If deleting your old migrations and starting over (e.g. non-production database) is an option, you may wish to do so. The following are instructions for fixing up 1.0.x migrations. First, Npgsql 1.0.x used a problematic method to identify serial (autoincrement) columns in migrations. If you look at your migration code you'll see .Annotation(\"Npgsql:ValueGeneratedOnAdd\", true) on various columns. Unfortunately this annotation is also present on non-serial columns, e.g. columns with default values. This causes various issues and has been replaced in 1.1. However, you'll have to manually remove .Annotation(\"Npgsql:ValueGeneratedOnAdd\", true), and replace it with .Annotation(\"Npgsql:ValueGenerationStrategy\", NpgsqlValueGenerationStrategy.SerialColumn) but only on columns which should be serial (e.g. not on columns with defaults). If you attempt to run a migration that has the old annotation, Npgsql will throw an exception and refuse to run your migrations. Unfortunately, this change will cause some incorrect changes the first time you add a migration after the upgrade. To avoid this, simply add a dummy migration right after upgrading to 1.1 and then delete the two new files generated for the dummy migration, but keep the changes made to your ModelSnapshot.cs. From this point on everything should be fine. Make sure you have no pending changes to your model before doing this!. Apologies for this problematic upgrade procedure, it should at least keep things clean going forward. Contributors Thank you very much to the following people who have contributed to the individual 1.1.x. releases. Milestone 1.1.1 Contributor Assigned issues @roji 8 Milestone 1.1.0 Contributor Assigned issues @roji 11"
  },
  "doc/conceptual/EFCore.PG/release-notes/2.0.html": {
    "href": "doc/conceptual/EFCore.PG/release-notes/2.0.html",
    "title": "2.0 Release Notes | Npgsql Documentation",
    "keywords": "2.0 Release Notes Version 2.0.0 of the Npgsql Entity Framework Core provider has been released and is available on nuget.org. This version works with version 2.0.0 of Entity Framework Core, and contains some new Npgsql features as well. New Features Aside from general EF Core features new in 2.0.0, the Npgsql provider contains the following major new features: PostgreSQL array operation translation (#120). While array properties have been supported since 1.1, operations on those arrays where client-evaluated. Version 2.0 will now translate array indexing, .Contains(), .SequenceEquals() and .Length. See the array mapping docs for more details. A retrying execution strategy (#155), which will automatically retry operations on exceptions which are considered transient. PostgreSQL extensions are now included in scaffolded models (#102). More LINQ operations are translated to SQL, and more database scenarios are scaffolded correctly (see the docs). Here's the full list of issues. Please report any problems to https://github.com/npgsql/Npgsql.EntityFrameworkCore.PostgreSQL. Upgrading from 1.x Previously an Npgsql.EntityFrameworkCore.PostgreSQL.Design nuget package existed alongside the main package. Its contents have been merged into the main Npgsql.EntityFrameworkCore.PostgreSQL and no new version has been released. Specifying versions when specifying PostgreSQL extensions on your model is no longer supported - this was a very rarely-used feature which interfered with extension scaffolding. Contributors Thank you very much to the following people who have contributed to the individual 2.0.x. releases. Milestone 2.0.2 Contributor Assigned issues @roji 4 Milestone 2.0.1 Contributor Assigned issues @roji 5 Milestone 2.0.0 Contributor Assigned issues @roji 16"
  },
  "doc/conceptual/EFCore.PG/release-notes/2.1.html": {
    "href": "doc/conceptual/EFCore.PG/release-notes/2.1.html",
    "title": "2.1 Release Notes | Npgsql Documentation",
    "keywords": "2.1 Release Notes Version 2.1.0 of the Npgsql Entity Framework Core provider has been released and is available on nuget. This version works with version 2.1.0 of Entity Framework Core, and contains some new Npgsql features as well. Thanks to @rwasef1830 and @austindrenski for their valuable contributions. New Features Aside from general EF Core features new in 2.1.0, the Npgsql provider contains the following major new features: Improved Spatial Support (PostGIS) Previous versions have allowed basic usage of PostGIS's spatial types via built-in Npgsql types such as NpgsqlPoint, NpgsqlLineString, etc. These types were limited in many ways, and no operation translation was supported. If you want to calculate, say, the distance between two points, you had to drop down to raw SQL. No more! Thanks to a new plugin infrastructure, the EF Core provider now has full-class support for PostGIS. You can now use the NetTopologySuite spatial library to map PostGIS types - NetTopologySuite's types are more complete, and best of all, the provider knows how to translate its operations to SQL. This allows you to write the following code: var nearbyCities = context.Cities.Where(c => c.Location.Distance(somePoint) < 100); See the full documentation for more information. Thanks to @YohDeadfall for implementing support for this at the ADO level. Full text search PostgreSQL has a powerful feature for efficient natural-language search across multiple columns and tables, see the PostgreSQL docs for more info. The EF Core provider now supports full-text search, allowing you to use .NET functions in your LINQ queries which will get translated to efficient PostgreSQL natural-language search queries. Read the full documentation for more information. Many thanks to @rwasef1830 for contributing this feature. NodaTime date/time support NodaTime is a powerful alternative to .NET's built-in date/time types, such as DateTime. The built-in types are flawed in many ways: they have problematic support for timezones, don't have a date-only or time-only types, and promote problematic programming but not making the right distinctions. If your application handles dates and times in anything but the most basic way, you should seriously consider using NodaTime. To learn more read this blog post by Jon Skeet. Thanks to a new plugin infrastructure, it is now possible to set up the EF Core provider to use NodaTime's types instead of the built-in .NET ones; instead of having DateTime properties on your entities, you can now have Instant properties instead. See the full documentation for more information. PostgreSQL 10 IDENTITY columns PostgreSQL 10 introduced a new IDENTITY column as an alternative to traditional SERIAL columns, and these are now supported by the EF Core provider. IDENTITY columns conform to the SQL standard and are in general safer than SERIAL columns, read this blog post for more info. It's recommended that all new projects use IDENTITY, but Npgsql even provides seamless migration of your existing SERIAL-based model!. Read the full documentation for more information. Enum support It is now possible to map your CLR enums to native PostgreSQL enums. This is a unique PostgreSQL feature that provides the best of both worlds: the enum is internally stored in the database as a number (minimal storage), but is handled like a string (more usable, no need to remember numeric values) and has type safety. See the full documentation for more information. Range support PostgreSQL supports native range types, which allow you to represent ranges of dates, ints and other data types in a single column. You can then efficiently perform queries on these types from LINQ, e.g. select all rows where a given date falls in the row's date range. See the full documentation for more information. Many thanks to @austindrenski for contributing the operation translations. Other notable features Several improvements have been made to the way arrays are mapped. For example, you can now map List<T> to PostgreSQL array (previously only T[] was supported) (#392). In addition, change tracking now works for arrays, so EF Core will automatically detect when you change an element's array and will update the corresponding database column when saving. PostgreSQL's built-in range types can now be mapped (#63), head over to the PostgreSQL docs to find out more about range types. Note that user-defined range types are not supported, if you're interested let us know at #329. Properties of type char are now supported and will be mapped to character(1) in PostgreSQL (#374). Identifiers in generated SQL will only be quoted if needed (#327). This should make it much easier to read. You can now use client certificate authentication and provide a server certificate validation callback (#270). See the doc for usage instructions. Added support for PostgreSQL 10 sequences with type int and smallint (#301). You can now specify the tablespace when creating your databases (#332). Here's the full list of issues. Please report any problems on https://github.com/npgsql/Npgsql.EntityFrameworkCore.PostgreSQL. Breaking changes Caution The provider's classes have been moved from the namespace Microsoft.EntityFrameworkCore to Npgsql.EntityFrameworkCore.PostgreSQL. As a result, any migrations already generated in your project (as well as the mode snapshot) will have to be manually updated to use the new namespace. You will need to add using Npgsql.EntityFrameworkCore.PostgreSQL.Metadata to all the relevant files. Columns of type timestamp with time zone/timestamptz will now be scaffolded as DateTime properties, and not DateTimeOffset properties. The general use of timestamp with time zone/timestamptz is discouraged (this type does not store the timezone in the database), consider using timestamp without time zone/timestamp instead. If you're specifying index methods with ForNpgsqlHasMethod(), then you will have to fix migrations which generate those indexes. In these migrations, you will find code such as .Annotation(\"Npgsql:Npgsql:IndexMethod\", \"gin\"). You must remove the extra Npgsql:, leaving .Annotation(\"Npgsql:IndexMethod\", \"gin\"). Specifying versions when specifying PostgreSQL extensions on your model is no longer supported - this was a very rarely-used feature which interfered with extension scaffolding. If you're still referencing the nuget package Npgsql.EntityFrameworkCore.PostgreSQL.Design, please remove it - it's no longer needed or up to date. Contributors Thank you very much to the following people who have contributed to the individual 2.1.x. releases. Milestone 2.1.2 Contributor Assigned issues @roji 6 @austindrenski 3 @rwasef1830 1 Milestone 2.1.11 Contributor Assigned issues @roji 1 Milestone 2.1.1.1 Contributor Assigned issues @roji 4 Milestone 2.1.1 Contributor Assigned issues @roji 9 Milestone 2.1.0 Contributor Assigned issues @roji 24 @austindrenski 1 @rwasef1830 1"
  },
  "doc/conceptual/EFCore.PG/release-notes/2.2.html": {
    "href": "doc/conceptual/EFCore.PG/release-notes/2.2.html",
    "title": "2.2 Release Notes | Npgsql Documentation",
    "keywords": "2.2 Release Notes Version 2.2.0 of the Npgsql Entity Framework Core provider has been released and is available on nuget. This version works with version 2.2.0 of Entity Framework Core, and contains some new Npgsql features as well. This release was result of hard work by @roji, @austindrenski, @yohdeadfall and @khellang. New Features Aside from general EF Core features new in 2.2.0, the Npgsql EF Core provider contains the following major new features: PostgreSQL 11 covering indexes PostgreSQL 11 introduced covering indexes feature, which allow you to include \"non-key\" columns in your indexes. This allows you to perform index-only scans and can provide a significant performance boost. Support has been added in (#697): protected override void OnConfiguring(DbContextOptionsBuilder builder) => modelBuilder.Entity<Blog>() .ForNpgsqlHasIndex(b => b.Id) .ForNpgsqlInclude(b => b.Name); This will create an index for searching on Id, but containing also the column Name, so that reading the latter will not involve accessing the table. See the documentation for more details. Thanks to @khellang for contributing this! PostgreSQL user-defined ranges The provider already supported PostgreSQL range types, but prior to 2.2 that support was limited to the built-in range types which come with PostgreSQL. #329 extends that support to range types which you define: protected override void OnConfiguring(DbContextOptionsBuilder builder) => builder.UseNpgsql(\"...\", b => b.MapRange<float>(\"floatrange\")); protected override void OnModelCreating(ModelBuilder builder) => builder.ForNpgsqlHasRange(\"floatrange\", \"real\"); This will make the provider create a PostgreSQL range called floatrange, over the PostgreSQL type real. Any property with type NpgsqlRange<float> will be seamlessly mapped to it. See the documentation for more details. Seeding for Npgsql-specific types When using some Npgsql-specific types, it wasn't possible to seed values for those types. With EF Core support for seeding any type, #667 allows seeding values for network, bit and range types (more are coming). PostgreSQL index operator classes PostgreSQL allows you to specify operator classes on your indexes, to allow tweaking how the index should work. #481 adds support for managing these. See the documentation for more details. Thanks to @khellang for contributing this! Other features Various issues with enum and range types were fixed, including upper/lower case, quoting and schema management. Many new SQL translations were added, so more of your LINQ expressions can run in the database. We'll be working on our documentation to make these more discoverable. The full list of issues for this release is available here. Contributors Thank you very much to the following people who have contributed to the individual 2.2.x. releases. Milestone 2.2.6 Contributor Assigned issues @roji 5 Milestone 2.2.4 Contributor Assigned issues @roji 3 @austindrenski 1 Milestone 2.2.0 Contributor Assigned issues @austindrenski 15 @roji 8"
  },
  "doc/conceptual/EFCore.PG/release-notes/3.1.html": {
    "href": "doc/conceptual/EFCore.PG/release-notes/3.1.html",
    "title": "3.1 Release Notes | Npgsql Documentation",
    "keywords": "3.1 Release Notes Version 3.1 of the Npgsql Entity Framework Core provider has been released and is available on nuget. This version works with version 3.1 of Entity Framework Core, and brings new Npgsql features in addition to the general EF Core changes. Caution Use these versions with care and do not deploy to production without thorough testing. Read the breaking changes section below. The full list of issues for this release is available here. New Features Aside from general EF Core features new in 3.1, the Npgsql EF Core provider contains the following major new features: It is now possible to map POCOs to JSON columns and query them, see the docs for more info (#981) PostgreSQL identity columns are now the default - see breaking changes below) (#804) Sequence options can now be managed on identity columns (#819) Support has been added for the upcoming PostgreSQL 12 generated columns feature (#939) A plugin is now available to automatically make all your tables and columns be snake_case! See EFCore.NamingConventions. Index sort options (ascending/descending, null sort order...) can now be specified (#326) Indexes can now be created concurrently (#967) Views are now reverse-engineered (#878) Array value converters help mapping with PostgreSQL arrays (e.g. map enum arrays to integer arrays) (#1031). Breaking changes Default value generation strategy is now IDENTITY Caution This is a major change, and upgrading to 3.1 with an existing database will cause a non-trivial schema migration to be generated. Read the following carefully. The default value generation strategy has changed from the older SERIAL columns to the newer IDENTITY columns, introduced in PostgreSQL 10. When producing a migration with 3.1, the Npgsql provider will attempt to automatically generate SQL to alter your tables and convert serial columns to identity ones. This is a sensitive, one-time migration operation that should be done with care, and carefully tested before deployment to production. If you wish to opt out of using IDENTITY columns and continue using SERIAL columns, set the following on your model: protected override void OnModelCreating(ModelBuilder modelBuilder) => modelBuilder.UseSerialColumns(); If you are using a PostgreSQL version older than 10, setting the compatibility mode is a better way to switch the default back to SERIAL columns: protected override void OnConfiguring(DbContextOptionsBuilder optionsBuilder) => optionsBuilder.UseNpgsql(\"...\", o => o.SetPostgresVersion(9, 6)); More information is available in the page about generated properties. Contributors Thank you very much to the following people who have contributed to the individual 3.x releases. Milestone 3.1.11 Contributor Assigned issues @roji 5 Milestone 3.1.7 Contributor Assigned issues @roji 4 Milestone 3.1.4 Contributor Assigned issues @roji 4 Milestone 3.1.3 Contributor Assigned issues @roji 1 Milestone 3.1.2 Contributor Assigned issues @roji 3 @grzybsonssg 1 Milestone 3.1.1.2 Contributor Assigned issues @roji 1 Milestone 3.1.1.1 Contributor Assigned issues @roji 1 Milestone 3.1.1 Contributor Assigned issues @roji 6 Milestone 3.1.0 Contributor Assigned issues @roji 15 @austindrenski 1 @ins0mniaque 1 @MarioPrabowo 1 Milestone 3.0.1 Contributor Assigned issues @roji 1 Milestone 3.0.0 Contributor Assigned issues @roji 25 @austindrenski 2 @YohDeadfall 1"
  },
  "doc/conceptual/EFCore.PG/release-notes/5.0.html": {
    "href": "doc/conceptual/EFCore.PG/release-notes/5.0.html",
    "title": "5.0 Release Notes | Npgsql Documentation",
    "keywords": "5.0 Release Notes Version 5.0 of the Npgsql Entity Framework Core provider has been released and is available on nuget. This version works with version 5.0 of Entity Framework Core, and brings new Npgsql features in addition to the general EF Core changes. New Features Aside from general EF Core features new in 5.0, here is a non-exhaustive list of features added in the Npgsql EF Core provider: Full support for PostgreSQL collations. See the docs for more info. It is now much easier to create indexes for full-text search (#1097, #1253). See the docs for more info. Sequences are now automatically updated after seeding data to avoid duplicate values (#367). The [JsonPropertyName] attribute is now respected when mapping to JSON (#1419). Improved support for PostgreSQL arrays: Mapping List<T> is now fully supported (#395). Mapping arrays of nullable value types (e.g. int?[]) is now supported (#1025). Query translations: Precise, exhaustive documentation has been added on which functions can be used in LINQ queries. Contains over arrays now uses indexes in various scenarios, improving performance (#1372). Various members of TimeSpan are now translated and can be used in queries (#328). Support for unaccent (#1530). Thanks @cloudlucky. Various translations over byte array properties have been added (#1225, #1226. Improved null semantics for PostgreSQL-specific operations results in tighter and more efficient SQL (#1142). Better support for NodaTime: It is now possible to map NodaTime Duration to PostgreSQL interval, previously only Period could be mapped (#1567. Some additional arithmetic operation are now translated (#1565). It is now possible to add labels to existing enums (but not remove or modify existing ones). The provider will generate the appropriate migrations (#1182). The full list of issues for this release is available here. Breaking changes Please consult the EF Core 5.0 breaking changes page as well - the below lists only changes specific to the Npgsql provider. Computed columns must now explicitly be configured as stored (#1336) Version 3.1 of the provider added support for PostgreSQL generated columns (#939), referred to as computed columns in EF Core. EF Core 5.0 adds support for specifying whether a computed column is virtual (computed when fetched), or stored (persisted on disk and computed when modified). Aligning with most databases, EF Core by default creates virtual computed columns, but these are currently unsupported by PostgreSQL. As a result, you must explicitly specify on all computed columns that they are stored: modelBuilder.Entity<Customer>() .Property(p => p.FullName) .HasComputedColumnSql(\"...\", stored: true); If you have existing migrations which were created with previous versions of EF Core, the column definitions in those also need to be fixed up with stored: true. IsCreatedConcurrently defaults to true (#1212) Previously, when IsCreatedConcurrently was used to configure an index without parameters, the default was false - this has changed to true. Note that indexes are never configured for for concurrent creation unless IsCreatedConcurrently is specified. Contributors A big thank you to all the following people who contributed to the 5.0 release! Milestone 5.0.10 Contributor Assigned issues @roji 3 @dmitrynovik 1 Milestone 5.0.7 Contributor Assigned issues @roji 5 @nathan-c 1 Milestone 5.0.6 Contributor Assigned issues @roji 2 @kakone 1 Milestone 5.0.5.1 Contributor Assigned issues @fsibilla 1 @roji 1 Milestone 5.0.5 Contributor Assigned issues @roji 8 @DanielAdolfsson 1 Milestone 5.0.2 Contributor Assigned issues @roji 6 Milestone 5.0.1 Contributor Assigned issues @roji 4 @akilin 1 Milestone 5.0.0 Contributor Assigned issues @roji 50 @artfulsage 1 @cloudlucky 1 @plamen-i 1 @Quogu 1"
  },
  "doc/conceptual/EFCore.PG/release-notes/6.0.html": {
    "href": "doc/conceptual/EFCore.PG/release-notes/6.0.html",
    "title": "6.0 Release Notes | Npgsql Documentation",
    "keywords": "6.0 Release Notes The release candidate of Npgsql Entity Framework Core provider version 6.0 has been released and is available on nuget. This version works with version 6.0 of Entity Framework Core, and brings new Npgsql features in addition to the general EF Core changes. Npgsql 6.0 brings some major breaking changes and is not a simple in-place upgrade. Carefully read the breaking change notes below and upgrade with care. New features Timestamp rationalization and improvements Support for timestamp with time zone and timestamp without time zone has been rationalized and simplified, and aligned with PostgreSQL best practices. In particular, the \"UTC everywhere\" pattern is much better supported via the PostgreSQL timestamp with time zone type, which is the recommended way to handle timestamps. A detailed explanation is available in this blog post, below is a summary of the main improvements. UTC timestamps have been cleanly separated from non-UTC timestamps, aligning with the PostgreSQL types. The former are represented by timestamp with time zone and DateTime with Kind UTC, the latter by timestamp without time zone and DateTime with Kind Local or Unspecified. It is recommended to use UTC timestamps where possible. Npgsql no longer performs any implicit timezone conversions when reading or writing any timestamp value - the value in the database is what you get, and the machine timezone no longer plays any role when reading/writing values. Npgsql no longer supports date/time representations which cannot be fully round-tripped to the database. If it can't be fully stored as-is, you can't write it. A compatibility switch enables opting out of the new behavior, to maintain backwards compatibility. This change introduces significant breaking changes (see below), although a compatibility flag can be used to opt out and revert to the previous behavior. Other date/time improvements include: Support for the new .NET DateOnly and TimeOnly types. Most DateTimeOffset members and methods are now translated. Many NodaTime translations have been added for ZonedDateTime, Period, DateInterval and others. PostgreSQL tstzrange is now mapped to NodaTime Interval, and PostgreSQL daterange is now mapped to NodaTime DateInterval. Most methods on these types are translated (#1998, #2059). Other new features The provider is now fully annotated for nullable reference types. Full support for the PostgreSQL 14 multirange type, mapped to arrays or lists of NpgsqlRange<T> (#1963). Includes translation of all major operators and functions, see the updated docs. Support for PostgreSQL 14 column compression methods (#2062). Support for the PostgreSQL ltree type, which represents labels of data stored in a hierarchical tree-like structure. Requires PostgreSQL 13 and above. Multiple spatial translations have been added for NetTopologySuite (DistanceKnn, <->, ST_Force2D, ST_Distance and ST_DWithin with spheriod). More translations and better type inference for arrays (#2026). The full list of issues for this release is available here. Breaking changes Major changes to timestamp mapping Note It is possible to opt out of these changes to maintain backwards compatibility, see below. Quick summary In many cases, it makes sense to store UTC timestamps in the database. To do this, migrate your timestamp without time zone columns to timestamp with time zone (see migration notes below), and always use either DateTime with Kind=Utc or DateTimeOffset with offset 0. If using NodaTime (recommended), use either Instant or ZonedDateTime with time zone UTC. To store non-UTC timestamps, use DateTime with Kind=Unspecified and add explicit configuration to your properties to be timestamp without time zone. If using NodaTime (recommended), use LocalDateTime (no explicit column configuration is required). Detailed notes The below notes will use the PostgreSQL aliases timestamptz to refer to timestamp with time zone, and timestamp to refer to timestamp without time zone. Note that timestamp with time zone represents a UTC timestamp and does not store a timezone in the database. DateTime properties now map to timestamptz by default, instead of to timestamp; this follows the recommended practice of storing UTC timestamps by default, but will cause the first migration to change your column type. If the intention is to store point-in-time or UTC timestamps, it's recommended to allow the migration to occur (see migration notes below). If the column really should store non-UTC timestamps (local or unspecified), explicitly set the column type back to timestamp. This is usually discouraged, but can be a temporary solution before transitioning to timestamptz. It is no longer possible to write DateTime with Kinds Local or Unspecified to timestamptz properties (which are the default for DateTime). Previously, Npgsql allowed writing those, performing timezone conversions from local to UTC. To write to timestamptz, provide a UTC DateTime. Similarly, it is no longer possible to write DateTime with Kind UTC to a timestamp column. timestamptz values are now read back as DateTime with Kind=UTC, without any conversions; these were previously returned as local DateTime, converted to the local machine's timezone. When reading timestamptz values as DateTimeOffset, UTC values (offset 0) are always returned. It is no longer possible to write DateTimeOffset with offsets other than 0 (UTC), since these cannot be represented in PostgreSQL. These were previously implicitly converted to UTC before sending. See the Npgsql ADO.NET docs for additional lower-level changes to timestamp handling. NodaTime changes Properties with type Instant are now mapped to timestamptz columns, and not to timestamp, since they represent a universally agreed-upon point in time. This follows the recommended practice, but will cause the first migration to change your column type. If the intention is to store point-in-time or UTC timestamps, it's recommended to allow the migration to occur (see migration notes below). If the column really should store non-UTC timestamps (local or unspecified), change the property's type to LocalDateTime instead; this will maintain the mapping to timestamp. This is usually discouraged, but can be a temporary solution before transitioning to timestamptz. When reading timestamptz as ZonedDateTime or OffsetDateTime, UTC values are always returned. Previously, local values based on the PostgreSQL TimeZone parameter were returned. Migrating columns from timestamp to timestamptz As a result of the above changes, the first migration created after upgrading to 6.0 will alter the columns for all DateTime and Instant properties from timestamp to timestamptz. If these columns are meant to store point-in-time or UTC timestamps (the recommended practice), then it's best to let this migration proceed; but care must be taken. As a starting point, let's assume your existing timestamp column has the timestamp 2020-01-01 12:00:00: SELECT \"CreatedOn\", pg_typeof(\"CreatedOn\") AS type FROM \"Blogs\"; Results in: CreatedOn | type ---------------------+----------------------------- 2020-01-01 12:00:00 | timestamp without time zone The migration generated by version 6.0 will cause the following SQL to be generated: ALTER TABLE \"Blogs\" ALTER COLUMN \"CreatedOn\" TYPE timestamp with time zone; When converting the timestamp without time zone column to timestamp with time zone, PostgreSQL will assume that existing values are local timestamps, and will convert them to UTC based on the TimeZone parameter. Performing the above query will result in something like: CreatedOn | type ------------------------+-------------------------- 2020-01-01 12:00:00+02 | timestamp with time zone This means that your new timestamptz column now contains 10:00 UTC, which is probably not what you want: if the original values were in fact UTC values, you need them to be preserved as-is, changing only the column type. To do this, edit your migration and add the following to the top of your migration's Up and Down methods: migrationBuilder.Sql(\"SET TimeZone='UTC';\"); This will ensure that no time zone conversions will be applied when converting the columns: CreatedOn | type ------------------------+-------------------------- 2020-01-01 14:00:00+02 | timestamp with time zone Changing timestamp seed data When switching from timestamp without time zone to timezone with time zone, you may have have non-UTC timestamp literals in your seeding configuration: modelBuilder.Entity<Blog>().HasData(new Blog { Id = 1, Timestamp = new DateTime(2020, 1, 1, 0, 0, 0) }); If so, you'll have to change these to be UTC. In addition, all migrations code since the change must be modified in the same way, to only seed UTC DateTime into the column; the model snapshot should be changed as well. Opting out of the new timestamp mapping logic The changes described above are far-reaching, and may break applications in various ways. You can upgrade to version 6.0 but opt out of the new mapping by enabling the Npgsql.EnableLegacyTimestampBehavior AppContext switch. To do this and revert to the legacy timestamp behavior, add the following to your context's constructor, before any Npgsql or EF Core operations are invoked: AppContext.SetSwitch(\"Npgsql.EnableLegacyTimestampBehavior\", true); NodaTime: tstzrange and daterange are mapped to Interval and DateInterval by default When using NodaTime, PostgreSQL tstzrange columns are scaffolded as Interval properties instead of NpgsqlRange<Instant> (#4070), and daterange columns are scaffolded as DateInterval properties of NpgsqlRange<LocalDateTime> (#1998). Date/time min/max values are now converted to PostgreSQL infinity values by default PostgreSQL has special infinity and -infinity values for timestamps and dates, which are later and earlier than other value. Npgsql has supported mapping DateTime.MaxValue and MinValue to these infinity values via an Convert Infinity DateTime connection string parameter, which was disabled by default. This behavior is now on by default, since DateTime.MaxValue and MinValue are very rarely used as actual timestamps/dates, and the Convert Infinity DateTime parameter has been removed. To disable infinity conversions, add the following at the start of your application: AppContext.SetSwitch(\"Npgsql.DisableDateTimeInfinityConversions\", true); See the date/time documentation for more details. Value converters for array/list properties need to use a special new API Previously, it was possible to configure value converters for array/list properties with the general EF Core API: modelBuilder.Entity<SomeEntity>.Property(e => e.ValueConvertedArray) .HasConversion(w => w.Select(x => x.Value).ToArray(), v => v.Select(x => new IntWrapper(x)).ToArray()); This is no longer possible and will cause an exception to be thrown. Instead, use the new HasPostgresArrayConversion API, providing conversion lambdas for the array's elements: modelBuilder.Entity<SomeEntity>.Property(e => e.ValueConvertedArray) .HasPostgresArrayConversion(w => w.Value, v => new IntWrapper(v)); Arrays/lists over ranges are mapped to PG14 multiranges PostgreSQL 14 introduced a new multirange type, which is very similar to an array of ranges but supports various range-related operations efficiently. The provider now maps arrays and lists of NpgsqlRange to these new types by default. To map them to old-style arrays over ranges in PostgreSQL, configure the column type explicitly: Data Annotations Fluent API [Column(TypeName = \"int4range[]\")] public NpgsqlRange<int>[] SomeArrayOverIntNpgsqlRange { get; set;} protected override void OnModelCreating(ModelBuilder modelBuilder) { modelBuilder .Entity<Blog>() .Property(e => e.SomeArrayOverIntNpgsqlRange) .HasColumnType(\"int4range[]\"); } Trigrams and FuzzyStringMatch plugins are now built-in The Npgsql.EntityFrameworkCore.PostgreSQL.Trigrams and Npgsql.EntityFrameworkCore.PostgreSQL.FuzzyStringMatch plugins have been integrated into the main provider; as a result, there are no 6.0 versions of these nuget packages - simply remove the package references from your project when upgrading to 6.0.0. Contributors A big thank you to all the following people who contributed to the 6.0 release! Milestone 6.0.8 Contributor Assigned issues @roji 3 Milestone 6.0.7 Contributor Assigned issues @roji 5 Milestone 6.0.6 Contributor Assigned issues @roji 2 Milestone 6.0.5 Contributor Assigned issues @roji 5 Milestone 6.0.4 Contributor Assigned issues @roji 6 @kourosko 1 Milestone 6.0.3 Contributor Assigned issues @roji 8 @rus-art 1 Milestone 6.0.2 Contributor Assigned issues @roji 4 Milestone 6.0.1 Contributor Assigned issues @roji 5 @Brar 1 @vonzshik 1 Milestone 6.0.0 Contributor Assigned issues @roji 56 @vonzshik 2 @arontsang 1 @Isitar 1 @kislovs 1 @pafrench 1 @tiborfsk 1"
  },
  "doc/conceptual/EFCore.PG/release-notes/7.0.html": {
    "href": "doc/conceptual/EFCore.PG/release-notes/7.0.html",
    "title": "7.0 Release Notes | Npgsql Documentation",
    "keywords": "7.0 Release Notes New features ExecuteUpdate and ExecuteDelete Support has been added for the new EF Core 7.0 ExecuteUpdate and ExecuteDelete, which allow expressing arbitrary, efficient updates via LINQ. See the EF What's new section for documentation. Support for DbDataSource Npgsql 7.0 introduced <xref:Npgsql.NpgsqlDataSource>, a major improvement to how database connections and configuration are managed in System.Data. <xref:Npgsql.NpgsqlDataSource> enabled rich new configuration APIs, which are also available when using the Npgsql EF provider: // Create a data source with the configuration you want: var dataSourceBuilder = new NpgsqlDataSourceBuilder(builder.Configuration.GetConnectionString(\"MyContext\")); dataSourceBuilder .UseLoggerFactory(loggerFactory) // Configure ADO.NET logging .UsePeriodicPasswordProvider(); // Automatically rotate the password periodically await using var dataSource = dataSourceBuilder.Build(); // Pass your data source to the EF provider: builder.Services.AddDbContext<MyContext>(options => options.UseNpgsql(dataSource); Note that the data source configuration works at the Npgsql ADO.NET layer, and is distinct from EF-level configuration. More improvements are planned in 8.0, to make data source usage more streamlined, especially around type mapping plugins, enums, etc. Extensive support for aggregate function translation EF Core 7.0 added support for translating provider-specific aggregate functions, and EFCore.PG builds on top of that to translate most major aggregate functions that PostgreSQL supports. This unlocks support for: string_agg: pack a column's values into a single string, with or without a delimiter. array_agg: pack a column's values into a PostgreSQL array. This can help with efficient fetching of dependent values, avoiding the so-called \"cartesian explosion\" problem. Statistical functions: standard deviation, variance and many others. Spatial functions: ST_Union, ST_Collect, ST_Extent and ST_ConvexHull. JSON functions: load values from the database as JSON documents with json_agg/jsonb_agg and json_object_agg/jsonb_object_agg. For the PostgreSQL documentation on aggregate functions, see this page. The exact translations supported by the provider are documented in the translations page. Row value expressions The provider now supports translations which make use of row value expressions, which are conceptually similar to tuples. Row values are particularly useful for implementing keyset pagination, which is much more efficient than the common, offset-base pagination. To learn more about pagination techniques, see this documentation page. Here's an example comparing two row values as an implementation of keyset pagination: var nextPage = context.Posts .OrderBy(b => b.Date) .ThenBy(b => b.PostId) .Where(b => EF.Functions.GreaterThan( ValueTuple.Create(b.Date, b.PostId), ValueTuple.Create(lastDate, lastId))) .Take(10) .ToList(); This generates the following SQL: SELECT p.\"PostId\", p.\"Date\" FROM \"Posts\" AS p WHERE (p.\"Date\", p.\"PostId\") > (@__lastDate_1, @__lastId_2) ORDER BY p.\"Date\", p.\"PostId\" LIMIT @__p_3 To the list of row value translations, see the translations page. Other new features Support for PostgreSQL 15 non-distinct NULLs in unique indexes, causing unique constraint violations if a column contains multiple null values. See the documentation for more details. Stored procedure mappings: PostgreSQL support has been added for stored procedure mapping, which is a new feature in EF Core 7.0. See the EF What's new section for documentation. Note that PostgreSQL 14 or above is required to use this feature (for output parameters). Breaking changes Note: version 7.0 of the lower-level Npgsql ADO.NET driver, which is used by the EF provider, also has some breaking changes. It's recommended to read the release notes for that as well. Obsoleted UseXminAsConcurrencyToken Starting with version 7.0, concurrency token properties can be configured via the standard EF means, rather than the PostgreSQL-specific UseXminAsConcurrencyToken; simply configure any uint property with the IsRowVersion() Fluent API or the [Timestamp] Data Annotation. See the documentation for more details. Obsoleted default column collations Versions 6.0 and below had a mechanism that allowed defining a \"default column collation\", which is applied individually to every text column by default; this differed from the database collation, which is applied once to the database at creation time. This mechanism was introduced because in PostgreSQL, the database collation is quite limited, and for example does not allow specifying non-deterministic collations (e.g. case-insensitive ones). However, with the introduction of pre-convention model configuration in EF Core, it's now possible to use that generic mechanism for specifying the default collation. As a result, the Npgsql-specific mechanism has been obsoleted and will be removed in a future version. Default column collations involve the following code: protected override void OnModelCreating(ModelBuilder modelBuilder) { modelBuilder.UseDefaultColumnCollation(\"<collation_name>\"); } To switch to the standard EF Core API, replace the code above with the following: protected override void ConfigureConventions(ModelConfigurationBuilder configurationBuilder) { configurationBuilder.Properties<string>().UseCollation(\"<collation_name>\"); } Contributors A big thank you to all the following people who contributed to the 7.0 release! Milestone 7.0.11 Contributor Assigned issues @roji 1 Milestone 7.0.4 Contributor Assigned issues @roji 7 Milestone 7.0.3 Contributor Assigned issues @jhartmann123 1 @roji 1 Milestone 7.0.1 Contributor Assigned issues @roji 3 Milestone 7.0.0 Contributor Assigned issues @roji 35 @midgleyc 1"
  },
  "doc/conceptual/EFCore.PG/release-notes/8.0.html": {
    "href": "doc/conceptual/EFCore.PG/release-notes/8.0.html",
    "title": "8.0 Release Notes | Npgsql Documentation",
    "keywords": "8.0 Release Notes Npgsql.EntityFrameworkCore.PostgreSQL version 8.0 is out and available on nuget.org. Full support for primitive collection querying One of PostgreSQL unique features as a relational database is its rich support for array types, which can be stored just like any other basic data type and queried. The Npgsql EF provider has supported mapping .NET arrays and Lists to PostgreSQL arrays for a very long time; however, with the introducion of rich primitive collection support in EF itself, Npgsql's support for arrays was extended to include full querying capabilities. Column collections On to the code! You can now use any LINQ operator - or chain of operators - on top of a primitive array or list property. For example, the following queries for all blogs who have at least 3 tags starting with \"x\": class Blog { ... public List<string> Tags { get; set; } } var blogs = await ctx.Blogs .Where(b => b.Tags.Count(t => t.StartsWith(\"x\")) > 3) .ToListAsync(); This is now translated to the following SQL: SELECT b.\"Id\", b.\"Tags\" FROM \"Blogs\" AS b WHERE ( SELECT count(*)::int FROM unnest(b.\"Tags\") AS t(value) WHERE t.value LIKE 'x%') > 3 Such complex translations rely on the PostgreSQL unnest function, which allows expanding a PostgreSQL array to a set of rows; once that's done, the array values can be queried with regular SQL. We can go further, querying for all blogs whose tags - or any of their posts' tags - contains a certain value: var blogs = await ctx.Blogs .Where(b => b.Tags.Union(b.Posts.SelectMany(p => p.Tags)) .Contains(\"foo\")) .ToListAsync(); This uses unnest both on the blog's tags on and all its posts' tags (via a lateral join), to then do a UNION On results and check whether the value is contained. SELECT b.\"Id\", b.\"Tags\" FROM \"Blogs\" AS b WHERE 'foo' IN ( SELECT t.value FROM unnest(b.\"Tags\") AS t(value) UNION SELECT t1.value FROM \"Post\" AS p JOIN LATERAL unnest(p.\"Tags\") AS t1(value) ON TRUE WHERE b.\"Id\" = p.\"BlogId\" ) Parameter collections But that's not all... The new primitive collection support works not just on columns, but also on parameterized lists. Previously, special and limited support existed for translating Contains over a parameterized list, as follows: var ids = new[] { 1, 2, 3 }; var blogs = await ctx.Blogs .Where(b => ids.Contains(b.Id)) .ToListAsync(); This has now been extended to allow composing any LINQ operator. For example, the following query queries for any blogs which have at least one tag, where that tag starts with a list of possible prefixes: var prefixes = new[] { \"f\", \"b\" }; var blogs = await ctx.Blogs .Where(b => prefixes.Any(p => b.Tags.Any(t => t.StartsWith(p)))) .ToListAsync(); Complex, I know! This translates to the following SQL: SELECT b.\"Id\", b.\"Tags\" FROM \"Blogs\" AS b WHERE EXISTS ( SELECT 1 FROM unnest(@__prefixes_0) AS p(value) WHERE EXISTS ( SELECT 1 FROM unnest(b.\"Tags\") AS t(value) WHERE p.value IS NOT NULL AND left(t.value, length(p.value)) = p.value)) Note the two usages of unnest here: one is used to expand the Tags column, whereas the other is used to expand the @__prefixes_0 array parameter that EF sends. All this machinery works together to make your LINQ query execute correctly. Inline collections Finally, support was added to inline collections, which are collections specified inside the query. For example, let's say that the list of tag prefixes in the previous query were always the same; in that case, we'd inline the prefixes variable as follows: var blogs = await ctx.Blogs .Where(b => new[] { \"f\", \"b\" }.Any(p => b.Tags.Any(t => t.StartsWith(p)))) .ToListAsync(); This translates to the following: SELECT b.\"Id\", b.\"Tags\" FROM \"Blogs\" AS b WHERE EXISTS ( SELECT 1 FROM (VALUES ('f'::text), ('b')) AS v(\"Value\") WHERE EXISTS ( SELECT 1 FROM unnest(b.\"Tags\") AS t(value) WHERE left(t.value, length(v.\"Value\")) = v.\"Value\")) Note that the unnest over the parameter has been replaced with a standard SQL VALUES construct, which allows constructing a set of rows inline, inside the query. More information Exciting stuff! We hope this helps you use LINQ to model and better interact with your database. The primitive collection section in the EF 8.0 What's New page. .NET Blog post on primitive collections PostgreSQL-specific information on primitive collections, including some specialized translations done for PostgreSQL. EF JSON support via ToJson() Version 8.0 also brings support for EF7's JSON columns feature (ToJson), which allows mapping JSON columns via owned entity types. While the Npgsql EF provider has had POCO JSON mapping for a very long time, the full modeling of the new ToJson() allows for a more powerful mapping strategy, with more query and update features. ToJson is the recommended way to map JSON going forward. You can read more on ToJson in the Npgsql JSON doc page, and in the EF7 what's new docs. Compared to the traditional Npgsql-specific POCO approach, a much wider range of LINQ queries can now be translated. For example, you can now compose LINQ operators over collections within JSON documents: var blogs = await ctx.Customers .Where(b => b.Details.Orders.Any(o => o.Price > 8)) .ToListAsync(); Note that this composes a LINQ operator - Any - on top of Orders, which is a list nested inside a JSON document. This translates to the following SQL: SELECT c.\"Id\", c.\"Details\" FROM \"Customers\" AS c WHERE EXISTS ( SELECT 1 FROM ROWS FROM (jsonb_to_recordset(c.\"Details\" -> 'Orders') AS ( \"Price\" numeric, \"ShippingAddress\" text )) AS o WHERE o.\"Price\" > 8.0) Note the jsonb_to_recordset function, which drills into the JSON document, finds the Orders property, and then expands that out to a set of rows, which can then be queried via regular SQL. Similarly, updates via EF's SaveChanges() are better, and can leverage partial updating to only patch the part of the JSON document which has changed. For example, let's assume that we load some customer with a JSON column, and change a single property within that JSON data: var customer = await ctx.Customers.SingleAsync(c => c.Details.Name == \"Foo\"); customer.Details.Name = \"Bar\"; await ctx.SaveChangesAsync(); Rather than needlessly sending the entire JSON document back to PostgreSQL, the EF provider uses the jsonb_set function to patch the specific property which changed, which is more efficient: UPDATE \"Customers\" SET \"Details\" = jsonb_set(\"Details\", '{Name}', @p0) WHERE \"Id\" = @p1; Other new features Version 8.0 contains many other smaller features and bug fixes, both on the EF side and on the Npgsql provider side. See the 8.0.0 milestone for the full list of Npgsql EF provider issues. Breaking changes Note: version 8.0 of the lower-level Npgsql ADO.NET driver, which is used by the EF provider, also has some breaking changes. It's recommended to read the release notes for that as well. JSON POCO and other dynamic features now require an explicit opt-in Because of the NativeAOT and trimming work done for Npgsql 8.0 (release notes), certain features now require an explicit opt-in, which you must add either on your <xref:Npgsql.NpgsqlDataSourceBuilder> or on <xref:Npgsql.NpgsqlConnection.GlobalTypeMapper?displayProperty=nameWithType>: PostgreSQL type Default .NET type JSON POCO mapping, JsonNode and subtypes <xref:Npgsql.INpgsqlTypeMapperExtensions.EnableDynamicJson> Unmapped enums, ranges, multiranges <xref:Npgsql.INpgsqlTypeMapperExtensions.EnableUnmappedTypes> Read PostgreSQL records as .NET tuples <xref:Npgsql.INpgsqlTypeMapperExtensions.EnableRecordsAsTuples> Existing code using the above features will start throwing exceptions after upgrading to version 8.0 of the EF Core provider; the exceptions provide explicit guidance on how to add the opt-ins. Note that EF Core itself is not yet compatible with NativeAOT, and Npgsql can only be used in NativeAOT applications without EF Core. Default PostgreSQL compatibility mode has been bumped from 12 to 14 This means that the provider assumes at least PostgreSQL 14; if you're running against an older version, explicitly specify the PostgreSQL version when configuring your context: optionsBuilder.UseNpgsql(\"<connection string>\", o => o.SetPostgresVersion(12, 0)) Obsoleted HasPostgresArrayConversion With EF 8.0 introducing first-class support for primitive collections, the PostgreSQL driver aligned its PostgreSQL array support to use that. As a result, HasPostgresArrayConversion can no longer be used to configure value-converted arrays; instead, the new standard EF mechanism can be used. For example, the following Npgsql-specific code would configure value conversion for a property of type MyType[] to a PostgreSQL array of strings in EF Core 6 or 7: modelBuilder.Entity<Blog>().Property(b => b.ValueConvertedArray) .HasPostgresArrayConversion(x => x.ToString(), s => MyType.Parse(s)); The same can now achieved with the following standard EF 8 code: modelBuilder.Entity<Blog>().PrimitiveCollection(b => b.ValueConvertedArray) .ElementType() .HasConversion(typeof(MyConverter)); class MyConverter : ValueConverter<MyType, string> { public MyConverter() : base(x => x.ToString(), s => MyType.Parse(s)) { } } cidr now maps to NpgsqlCidr instead of ValueTuple<IPAddress, int> As part of improving Npgsql's support for the PostgreSQL network mappings, the PostgreSQL cidr type now maps to the newly-introduced <xref:NpgsqlTypes.NpgsqlCidr>, and can no longer be mapped to ValueTuple<IPAddress, int>. Contributors A big thank you to all the following people who contributed to the 8.0 release! Milestone 8.0.0 Contributor Assigned issues @roji 34 @zpaks 1"
  },
  "doc/conceptual/EFCore.PG/release-notes/9.0.html": {
    "href": "doc/conceptual/EFCore.PG/release-notes/9.0.html",
    "title": "9.0 Release Notes | Npgsql Documentation",
    "keywords": "9.0 Release Notes Npgsql.EntityFrameworkCore.PostgreSQL version 9.0 is under development; previews are available on nuget.org. Note The following release notes and breaking changes are partial. More will be added nearer to the 9.0 final release. Improved, unified configuration experience The Npgsql EF provider is built on top of the lower-level Npgsql ADO.NET provider; the configuration interface between these two layers was less than ideal, and configuration been more difficult than it should have been. For version 9.0, the configuration experience has been considerably improved. Since version 7, the Npgsql ADO.NET provider has been moving to NpgsqlDataSource as the preferred way of configuration connections and obtaining them. At the EF level, it has been possible to pass an NpgsqlDataSource instance to UseNpgsql(); but this required that the user separately configure a data source and manage it. In addition, features such as plugins and enums require support from both the EF and ADO.NET layers, forcing users to perform multiple setup actions at the different layers. With version 9, UseNpgsql() becomes a single point for configuration, for both the EF and ADO.NET levels. EF can now internally set up an NpgsqlDataSource, automatically applying all the necessary configuration to it, and also exposes an API to allow users to apply arbitrary configuration to it as well: builder.Services.AddDbContextPool<BloggingContext>(opt => opt.UseNpgsql( builder.Configuration.GetConnectionString(\"BloggingContext\"), o => o .SetPostgresVersion(13, 0) .UseNodaTime() .MapEnum<Mood>(\"mood\") .ConfigureDataSource(dataSourceBuilder => dataSourceBuilder.UseClientCertificate(certificate)))); In the above code, the following configuration gestures are performed: SetPostgresVersion() is an EF-only option to produce SQL for PostgreSQL version 13 (avoiding newer incompatible features) UseNodaTime(), adds a plugin allowing use of NodaTime for date/time type mapping. This also requires an ADO.NET NodaTime plugin which needed to be configured separately, but this is now done automatically. MapEnum() maps a .NET enum type. Like UseNodaTime(), this also used to require a separate ADO.NET configuration gesture, but is now done automatically. As an added bonus, doing this now also adds the enum to the model, causing the enum to be created in the database via EF's migrations. ConfigureDataSource() exposes an NpgsqlDataSourceBuilder, which you can use to configure arbitrary ADO.NET options. In this example, the certificate is defined for the TLS authentication process. For more information, see the getting started docs. Improved configuration for enums and plugins Previously, configuration around enums and plugins (NodaTime, NetTopologySuite) was complicated, requiring multiple setup actions at both the EF and the lower-level Npgsql layers. EF 9.0 improves the configuration story, allowing you to configure enums and plugins via a single EF gesture: builder.Services.AddPooledDbContext<MyContext>(options => options.UseNpgsql( \"<connection string>\", o => o.MapEnum<Mood>(\"mood\"))); This takes care of everything - EF configuration, lower-level Npgsql configuration and even the addition of the enum to the EF model, which ensures that the enum is created in the database in EF migrations. See the enum, NodaTime and NetTopologySuite documentation for more details. UUIDv7 GUIDs are generated by default When your entity types have a Guid key, EF Core by default generates key values for new entities client-side - in .NET - before inserting those entity types to the database; this can be better for performance in some situations. Before version 9.0, the provider generated random GUIDs (version 4) by calling the .NET Guid.NewGuid() function. Unfortunately, random GUIDs aren't ideal for database indexing and can cause performance issues. Version 9.0 of the provider now generates the recently standardized version 7 GUIDs, which is a sequential GUID type that's more appropriate for database indexes and improves their performance. This new behavior is on by default and takes effect simply by upgrading the provider version. See this post for more details and performance numbers on random vs. sequential GUIDs. Thanks to @ChrisJollyAU and @Timovzl for contributing this improvement! Breaking changes Enum mappings must now be configured at the EF level Previously, enum configuration involved mapping the enum at the lower-level Npgsql layer (either via NpgsqlDataSourceBuilder.MapEnum or via NpgsqlConnection.GlobalTypeMapper.MapEnum); the EF provider automatically picked this configuration up for the EF-level setup. Unfortunately, this design created numerous issues and bugs. As part of the improved enum configuration story in version 9.0 (see above), enums must now be configured at the EF level; although this is a breaking change for existing applications, it usually results in simplified setup code and fixes various bugs and problematic behavior. If your application calls UseNpgsql with a simple connection string (rather than an NpgsqlDataSource), it simply needs to add a MapEnum call there: builder.Services.AddDbContext<MyContext>(options => options.UseNpgsql( \"<connection string>\", o => o.MapEnum<Mood>(\"mood\"))); All other setup code - the MapEnum call on NpgsqlConnection.GlobalTypeMapper and the HasPostgresEnum call in OnModelCreating - can be removed. If your application passes an NpgsqlDataSource to UseNpgsql, it also needs to add the MapEnum call as above; but the MapEnum call on NpgsqlDataSourceBuilder must also be kept. See the enum documentation for more information. Contributors A big thank you to all the following people who contributed to the 9.0 release! Milestone 9.0.0 Contributor Assigned issues @roji 34 @ChrisJollyAU 1 @Timovzl 1"
  },
  "doc/conceptual/Npgsql/basic-usage.html": {
    "href": "doc/conceptual/Npgsql/basic-usage.html",
    "title": "Npgsql Basic Usage | Npgsql Documentation",
    "keywords": "Npgsql Basic Usage Data source Note The data source concept was introduced in Npgsql 7.0. If you're using an older version, see Connections without a data source below. Starting with Npgsql 7.0, the starting point for any database operation is <xref:Npgsql.NpgsqlDataSource>. The data source represents your PostgreSQL database, and can hand out connections to it, or support direct execution of SQL against it. The data source encapsulates the various Npgsql configuration needed to connect to PostgreSQL, as well the connection pooling which makes Npgsql efficient. The simplest way to create a data source is the following: var connectionString = \"Host=myserver;Username=mylogin;Password=mypass;Database=mydatabase\"; await using var dataSource = NpgsqlDataSource.Create(connectionString); In this code, a data source is created given a connection string, which is used to define which database to connect to, the authentication information to use, and various other connection-related parameters. The connection string consists of key/value pairs, separated with semicolons; many options are supported in Npgsql, these are documented on the connection string page. Npgsql's data source supports additional configuration beyond the connection string, such as logging, advanced authentication options, type mapping management, and more. To further configure a data source, use <xref:Npgsql.NpgsqlDataSourceBuilder> as follows: var dataSourceBuilder = new NpgsqlDataSourceBuilder(\"Host=localhost;Username=test;Password=test\"); dataSourceBuilder .UseLoggerFactory(loggerFactory) // Configure logging .UsePeriodicPasswordProvider() // Automatically rotate the password periodically .UseNodaTime(); // Use NodaTime for date/time types await using var dataSource = dataSourceBuilder.Build(); You typically build a single data source, and then use that instance throughout your application; data sources are thread-safe, and (usually) correspond to a connection pool inside Npgsql. For more information on data source configuration, consult the relevant documentation pages. Note If you're using NativeAOT and trimming and are concerned with minimizing application size, consider using <xref:Npgsql.NpgsqlSlimDataSourceBuilder>; this builder includes only the very minimum of functionality by default, and allows adding additional features via opt-ins. Basic SQL Execution Once you have a data source, an <xref:Npgsql.NpgsqlCommand> can be used to execute SQL against it: await using var command = dataSource.CreateCommand(\"SELECT some_field FROM some_table\"); await using var reader = await command.ExecuteReaderAsync(); while (await reader.ReadAsync()) { Console.WriteLine(reader.GetString(0)); } More information on executing commands is provided below. Connections In the example above, we didn't deal with a database connection; we just executed a command directly against a data source representing the database. Npgsql internally arranges for a connection on which to execute your command, but you don't need to concern yourself with that. However, in some situations it's necessary to interact with a connection, typically when some sort of state needs to persist across multiple command executions. The common example for this is a database transaction, where multiple commands need to be executed within the same transaction, on the same transaction. A data source also acts as a factory for connections, so you can do the following: await using var connection = await dataSource.OpenConnectionAsync(); At this point you have an open connection, and can execute commands against it much like we did against the data source above: await using var command = new NpgsqlCommand(\"SELECT '8'\", connection); await using var reader = await command.ExecuteReaderAsync(); // Consume the results Connections must be disposed when they are no longer needed - not doing so will result in a connection leak, which can crash your program. In the above code sample, this is done via the await using C# construct, which ensures the connection is disposed even if an exception is later thrown. It's a good idea to keep connections open for as little time a possible: database connections are scarce resources, and keeping them open for unnecessarily long times can create unnecessary load in your application and in PostgreSQL. Pooling Opening and closing physical connections to PostgreSQL is an expensive and long process. Therefore, Npgsql connections are pooled by default: closing or disposing a connection doesn't close the underlying physical connection, but rather returns it to an internal pool managed by Npgsql. The next time a connection is needed, that pooled connection is returned again. This makes open and close extremely fast operations; do not hesitate to perform them a lot if needed, rather than holding a connection needlessly open for a long time. For information on tweaking the pooling behavior (or turning it off), see the pooling section in the connection string page. Connections without a data source The data source concept is new in Npgsql 7.0, and is the recommended way to use Npgsql. When using older versions, connections are instantiated directly, rather than obtaining them from a data source: await using var conn = new NpgsqlConnection(connectionString); await conn.OpenAsync(); Direct instantiation of connection is still supported, but is discouraged for various reasons when using Npgsql 7.0 or above. Other execution methods Above, we executed SQL via ExecuteReaderAsync. There are various ways to execute a command, based on what results you expect from it: ExecuteNonQueryAsync: executes SQL which doesn't return any results, typically INSERT, UPDATE or DELETE statements. Returns the number of rows affected. ExecuteScalarAsync: executes SQL which returns a single, scalar value. ExecuteReaderAsync: execute SQL which returns a full resultset. Returns an <xref:Npgsql.NpgsqlDataReader> which can be used to access the resultset (as in the above example). For example, to execute a simple SQL INSERT which does not return anything, you can use ExecuteNonQueryAsync as follows: await using var command = dataSource.CreateCommand(\"INSERT INTO some_table (some_field) VALUES (8)\"); await command.ExecuteNonQueryAsync(); Note that each execute method involves a database roundtrip. To execute multiple SQL statements in a single roundtrip, see the batching section below. Parameters When sending data values to the database, always consider using parameters rather than including the values in the SQL as follows: await using var cmd = new NpgsqlCommand(\"INSERT INTO table (col1) VALUES ($1), ($2)\", conn) { Parameters = { new() { Value = \"some_value\" }, new() { Value = \"some_other_value\" } } }; await cmd.ExecuteNonQueryAsync(); The $1 and $2 in your SQL are parameter placeholders: they refer to the corresponding parameter in the command's parameter list, and are sent along with your query. This has the following advantages over embedding the value in your SQL: Parameters protect against SQL injection for user-provided inputs: the parameter data is sent to PostgreSQL separately from the SQL, and is never interpreted as SQL. Parameters are required to make use of prepared statements, which significantly improve performance if you execute the same SQL many times. Parameter data is sent in an efficient, binary format, rather than being represented as a string in your SQL. Note that PostgreSQL does not support parameters in arbitrary locations - you can only parameterize data values. For example, trying to parameterize a table or column name will fail - parameters aren't a simple way to stick an arbitrary string in your SQL. Positional and named placeholders Starting with Npgsql 6.0, the recommended placeholder style is positional ($1, $2); this is the native parameter style used by PostgreSQL, and your SQL can therefore be sent to the database as-is, without any manipulation. For legacy and compatibility reasons, Npgsql also supports named placeholders. This allows the above code to be written as follows: await using var cmd = new NpgsqlCommand(\"INSERT INTO table (col1) VALUES (@p1), (@p2)\", conn) { Parameters = { new(\"p1\", \"some_value\"), new(\"p2\", \"some_other_value\") } }; await cmd.ExecuteNonQueryAsync(); Rather than matching placeholders to parameters by their position, Npgsql matches these parameter by name. This can be useful when porting database code from other databases, where named placeholders are used. However, since this placeholder style isn't natively supported by PostgreSQL, Npgsql must parse your SQL and rewrite it to use positional placeholders under the hood; this rewriting has a performance price, and some forms of SQL may not be parsed correctly. It's recommended to use positional placeholders whenever possible. For more information, see this blog post. Parameter types PostgreSQL has a strongly-typed type system: columns and parameters have a type, and types are usually not implicitly converted to other types. This means you have to think about which type you will be sending: trying to insert a string into an integer column (or vice versa) will fail. In the example above, we let Npgsql infer the PostgreSQL data type from the .NET type: when Npgsql sees a .NET string, it automatically sends a parameter of PostgreSQL type text (note that this isn't the same as, say varchar). In many cases this will work just fine, and you don't need to worry. In some cases, however, you will need to explicitly set, or coerce, the parameter type. For example, although Npgsql sends .NET string as text by default, it also supports sending jsonb. For more information on supported types and their mappings, see this page. NpgsqlParameter exposes several properties that allow you to coerce the parameter's data type: DbType: a portable enum that can be used to specify database types. While this approach will allow you to write portable code across databases, it won't let you specify types that are specific to PostgreSQL. This is useful mainly if you're avoiding Npgsql-specific types, using DbConnection and DbCommand rather than <xref:Npgsql.NpgsqlConnection> and <xref:Npgsql.NpgsqlCommand>. NpgsqlDbType: an Npgsql-specific enum that contains (almost) all PostgreSQL types supported by Npgsql. DataTypeName: an Npgsql-specific string property which allows to directly set a PostgreSQL type name on the parameter. This is rarely needed - NpgsqlDbType should be suitable for the majority of cases. However, it may be useful if you're using unmapped user-defined types (enums or composites) or some PostgreSQL type which isn't included in NpgsqlDbType (because it's supported via an external plugin). Strongly-typed parameters The standard ADO.NET parameter API is unfortunately weakly-typed: parameter values are set on NpgsqlParameter.Value, which, being an object, boxes value types such as int. If you're sending lots of value types to the database, this creates large amounts of useless heap allocations and strain the garbage collector. As an alternative, you can use NpgsqlParameter<T>. This generic class has a TypedValue member, which is similar to NpgsqlParameter.Value but is strongly-typed, thus avoiding the boxing and heap allocation. Unfortunately, using nullable value types (in order to send nulls) isn't yet supported - you'll have to use the non-generic parameter API for that. Note also that this strongly-typed parameter API is entirely Npgsql-specific, and will make your code non-portable to other database. See #8955 for an issue discussing this at the ADO.NET level. Transactions Basic transactions Transactions can be started by calling the standard ADO.NET method NpgsqlConnection.BeginTransaction(): await using var connection = await dataSource.OpenConnectionAsync(); await using var transaction = await connection.BeginTransactionAsync(); await using var command1 = new NpgsqlCommand(\"...\", connection, transaction); await command1.ExecuteNonQueryAsync(); await using var command2 = new NpgsqlCommand(\"...\", connection, transaction); await command2.ExecuteNonQueryAsync(); await transaction.CommitAsync(); PostgreSQL doesn't support nested or concurrent transactions - only one transaction may be in progress at any given moment (starting a transaction while another transaction is already in progress throws an exception). Because of this, it isn't necessary to pass the NpgsqlTransaction object returned from BeginTransaction() to commands you execute - starting a transaction means that all subsequent commands will automatically participate in the transaction, until either a commit or rollback is performed. However, for maximum portability it's recommended to set the transaction on your commands. Although concurrent transactions aren't supported, PostgreSQL supports the concept of savepoints - you may set named savepoints in a transaction and roll back to them later without rolling back the entire transaction. Savepoints can be created, rolled back to, and released via NpgsqlTransaction.SaveAsync(), RollbackAsync() and Release(name) respectively. See the PostgreSQL documentation for more details.. When starting a transaction, you may optionally set the isolation level. See the docs for more details. System.Transactions and distributed transactions In addition to BeginTransactionAsync(), .NET includes System.Transactions, an alternative API for managing transactions - read the MSDN docs to understand the concepts involved. Npgsql fully supports this API, and automatically enlists if a connection is opened within an ambient TransactionScopes. When a transaction includes more than one database (or even more than one concurrent connections to the same database), the transaction is said to be distributed. .NET 7.0 brings the same distributed transaction support that .NET Framework supported, for Windows only. While Npgsql partially supports this mechanism, it does not implement the recovery parts of the distributed transaction, because of some design issues with .NET's support. While distributed transactions may work for you, it is discouraged to fully rely on them with Npgsql. Note that if you open and close connections to the same database inside an ambient transaction, without ever having two connections open at the same time, Npgsql internally reuses the same connection, avoiding the need for a distributed transaction. Batching Let's say you need to execute two SQL statements for some reason. This can naively be done as follows: await using var cmd = new NpgsqlCommand(\"INSERT INTO table (col1) VALUES ('foo')\", conn); await cmd.ExecuteNonQueryAsync(); cmd.CommandText = \"SELECT * FROM table\"; await using var reader = await cmd.ExecuteReaderAsync(); The above code needlessly performs two roundtrips to the database: your program will not send the SELECT until after the INSERT has completed and confirmation for that has been received. Network latency can make this very inefficient: as the distance between your .NET client and PostgreSQL increases, the time spent waiting for packets to cross the network can severely impact your application's performance. Instead, you can ask Npgsql to send the two SQL statements in a single roundtrip, by using batching: await using var batch = new NpgsqlBatch(conn) { BatchCommands = { new(\"INSERT INTO table (col1) VALUES ('foo')\"), new(\"SELECT * FROM table\") } }; await using var reader = await batch.ExecuteReaderAsync(); An <xref:Npgsql.NpgsqlBatch> simply contains a list of NpgsqlBatchCommands, each of which has a CommandText and a list of parameters (much like an <xref:Npgsql.NpgsqlCommand>). All statements and parameters are efficiently packed into a single packet - when possible - and sent to PostgreSQL. Note If you haven't started an explicit transaction with <xref:Npgsql.NpgsqlConnection.BeginTransaction>, a batch is automatically wrapped in an implicit transaction. That is, if a statement within the batch fails, all later statements are skipped and the entire batch is rolled back. Legacy batching Prior to Npgsql 6.0, NpgsqlBatch did not yet exist, and batching could be done as follows: await using var cmd = new NpgsqlCommand(\"INSERT INTO table (col1) VALUES ('foo'); SELECT * FROM table\", conn); await using var reader = await cmd.ExecuteReaderAsync(); This packs multiple SQL statements into the CommandText of a single NpgsqlCommand, delimiting them with semi-colons. This technique is still supported, and can be useful when porting database code from other database. However, legacy batching is generally discouraged since it isn't natively supported by PostgreSQL, forcing Npgsql to parse the SQL to find semicolons. This is similar to named parameter placeholders, see this section for more details. Stored functions and procedures PostgreSQL supports stored (or server-side) functions, and since PostgreSQL 11 also stored procedures. These can be written in SQL (similar to views), or in PL/pgSQL (PostgreSQL's procedural language), PL/Python or several other server-side languages. Once a function or procedure has been defined, calling it is a simple matter of executing a regular command: // For functions using var cmd = new NpgsqlCommand(\"SELECT my_func(1, 2)\", conn); using var reader = cmd.ExecuteReader(); // For procedures using var cmd = new NpgsqlCommand(\"CALL my_proc(1, 2)\", conn); using var reader = cmd.ExecuteReader(); You can replace the parameter values above with regular placeholders (e.g. $1), just like with a regular query. CommandType.StoredProcedure Warning Starting with Npgsql 7.0, CommandType.StoredProcedure now invokes stored procedures, and not function as before. See the release notes for more information and how to opt out of this change. In some other databases, calling a stored procedures involves setting the command's CommandType: using var command1 = new NpgsqlCommand(\"my_procedure\", connection) { CommandType = CommandType.StoredProcedure, Parameters = { new() { Value = 8 } } }; await using var reader = await command1.ExecuteReaderAsync(); Npgsql supports this mainly for portability, but this style of calling has no advantage over the regular command shown above. When CommandType.StoredProcedure is set, Npgsql will simply generate the appropriate CALL my_procedure($1) for you, nothing more. Unless you have specific portability requirements, it is recommended you simply avoid CommandType.StoredProcedure and construct the SQL yourself. Be aware that CommandType.StoredProcedure generates a CALL command, which is suitable for invoking stored procedures and not functions. Versions of Npgsql prior to 7.0 generated a SELECT command suitable for functions, and this legacy behavior can be enabled; see the 7.0 release notes Note that if CommandType.StoredProcedure is set and your parameter instances have names, Npgsql generates parameters with named notation: SELECT my_func(p1 => 'some_value'). This means that your NpgsqlParameter names must match your PostgreSQL procedure or function parameters, or the call will fail. If you omit the names on your NpgsqlParameters, positional notation will be used instead. Note that positional parameters must always come before named ones. See the PostgreSQL docs for more info. Function in/out parameters In SQL Server (and possibly other databases), functions can have output parameters, input/output parameters, and a return value, which can be either a scalar or a table (TVF). To call functions with special parameter types, the Direction property must be set on the appropriate DbParameter. PostgreSQL functions, on the hand, always return a single table - they can all be considered TVFs. Somewhat confusingly, PostgreSQL does allow your functions to be defined with input/and output parameters: CREATE FUNCTION dup(in int, out f1 int, out f2 text) AS $$ SELECT $1, CAST($1 AS text) || ' is text' $$ LANGUAGE SQL; However, the above syntax is nothing more than a definition of the function's resultset, and is identical to the following (see the PostgreSQL docs): CREATE FUNCTION dup(int) RETURNS TABLE(f1 int, f2 text) AS $$ SELECT $1, CAST($1 AS text) || ' is text' $$ LANGUAGE SQL; In other words, PostgreSQL functions don't have output parameters that are distinct from the resultset they return - output parameters are just a syntax for describing that resultset. Because of this, on the Npgsql side there's no need to think about output (or input/output) parameters: simply invoke the function and process its resultset just like you would any regular resultset. However, to help portability, Npgsql does provide support for output parameters as follows: using (var cmd = new NpgsqlCommand(\"SELECT my_func()\", conn)) { cmd.Parameters.Add(new NpgsqlParameter(\"p_out\", DbType.String) { Direction = ParameterDirection.Output }); cmd.ExecuteNonQuery(); Console.WriteLine(cmd.Parameters[0].Value); } When Npgsql sees a parameter with ParameterDirection.Output (or InputOutput), it will simply search the function's resultset for a column whose name matches the parameter, and copy the first row's value into the output parameter. This provides no value whatsoever over processing the resultset yourself, and is discouraged - you should only use output parameters in Npgsql if you need to maintain portability with other databases which require it."
  },
  "doc/conceptual/Npgsql/compatibility.html": {
    "href": "doc/conceptual/Npgsql/compatibility.html",
    "title": "Compatibility Notes | Npgsql Documentation",
    "keywords": "Compatibility Notes This page centralizes Npgsql's compatibility status with PostgreSQL and other components, and documents some important gotchas. PostgreSQL We aim to be compatible with all currently supported PostgreSQL versions, which means 5 years back. Earlier versions may still work but we don't perform continuous testing on them or commit to resolving issues on them. ADO.NET Npgsql is an ADO.NET-compatible provider, so it has the same APIs as other .NET database drivers and should behave the same. Please let us know if you notice any non-standard behavior. NativeAOT and trimming NativeAOT allows using ahead-of-time compilation to publish a fully self-contained application that has been compiled to native code. Native AOT apps have faster startup time and smaller memory footprints, and thanks to trimming can also have a much smaller size footprint on disk. Starting with version 8.0, Npgsql is fully compatible with NativeAOT and trimming. The majority of features are compatible with NativeAOT/trimming and can be used without issues, and most applications using Npgsql can be used as-is with NativeAOT/trimming without any changes. A few features which are incompatible require an explicit code opt-in, which generates a warning if used with NativeAOT/trimming enabled. .NET Framework/.NET Core/mono Npgsql 4.* targets .NET Framework 4.6.1, as well as .NET Standard 2.0 which allows it to run on .NET Core. It is also tested and runs well on mono. Npgsql 5.* targets .NET Standard 2.0 and .NET 5. Starting with this version, we no longer run regression tests on .NET Framework and mono. In addition, the Visual Studio extension (VSIX) and the MSI GAC installer have been discontinued. pgbouncer Npgsql works well with PgBouncer, but there are some quirks to be aware of. In many cases, you'll want to turn off Npgsql's internal connection pool by specifying Pooling=false on the connection string. If you decide to keep Npgsql pooling on along with PgBouncer, and are using PgBouncer's transaction or statement mode, then you need to specify No Reset On Close=true on the connection string. This disables Npgsql's connection reset logic (DISCARD ALL), which gets executed when a connection is returned to Npgsql's pool, and which makes no sense in these modes. Prior to version 3.1, Npgsql sends the statement_timeout startup parameter when it connects, but this parameter isn't supported by pgbouncer. You can get around this by specifying CommandTimeout=0 on the connection string, and then manually setting the CommandTimeout property on your NpgsqlCommand objects. Version 3.1 no longer sends statement_timeout. PgBouncer below 1.12 doesn't support SASL authentication. Amazon Redshift Amazon Redshift is a cloud-based data warehouse originally based on PostgreSQL 8.0.2. In addition, due to its nature some features have been removed and others changed in ways that make them incompatible with PostgreSQL. We try to support Redshift as much as we can, please let us know about issues you run across. First, check out Amazon's page about Redshift and PostgreSQL which contains lots of useful compatibility information. Additional known issues: If you want to connect over SSL, your connection string must contain Server Compatibility Mode=Redshift, otherwise you'll get a connection error about ssl_renegotiation_limit. Entity Framework with database-computed identity values don't work with Redshift, since it doesn't support sequences (see issue #544). DigitalOcean Managed Database DigitalOcean's Managed Database services requires you to connect to PostgreSQL over SSL. Unfortunately when you enable it in your connection string, you will get the same error regarding ssl_renegotiation_limit as Amazon Redshift. The Redshift compatibility mode setting resolves the issue on DigitalOcean."
  },
  "doc/conceptual/Npgsql/connection-string-parameters.html": {
    "href": "doc/conceptual/Npgsql/connection-string-parameters.html",
    "title": "Connection String Parameters | Npgsql Documentation",
    "keywords": "Connection String Parameters To connect to a database, the application provides a connection string which specifies parameters such as the host, the username, the password, etc. Connection strings have the form keyword1=value; keyword2=value; and are case-insensitive. Values containing special characters (e.g. semicolons) can be double-quoted. For more information, see the official doc page on connection strings. Below are the connection string parameters which Npgsql understands, as well as some standard PostgreSQL environment variables. Basic connection Parameter Description Default Host Specifies the host name - and optionally port - on which PostgreSQL is running. Multiple hosts may be specified, see the docs for more info. If the value begins with a slash, it is used as the directory for the Unix-domain socket (specifying a Port is still required). Required Port The TCP port of the PostgreSQL server. 5432 Database The PostgreSQL database to connect to. Same as Username Username The username to connect with. If not specified, the OS username will be used. PGUSER Password The password to connect with. Not required if using GSS/SSPI. PGPASSWORD Passfile Path to a PostgreSQL password file (PGPASSFILE), from which the password is taken. PGPASSFILE Security and encryption Parameter Description Default SSL Mode Controls whether SSL is used, depending on server support. See docs for possible values and more info. Prefer Trust Server Certificate Whether to trust the server certificate without validating it. See docs for more info. false SSL Certificate Location of a client certificate to be sent to the server. See docs. PGSSLCERT SSL Key Location of a client key for a client certificate to be sent to the server. PGSSLKEY SSL Password Password for a key for a client certificate. Root Certificate Location of a CA certificate used to validate the server certificate. PGSSLROOTCERT Check Certificate Revocation Whether to check the certificate revocation list during authentication. false Channel Binding Control whether channel binding is used when authenticating with SASL. Introduced in 8.0. Prefer Persist Security Info Gets or sets a Boolean value that indicates if security-sensitive information, such as the password, is not returned as part of the connection if the connection is open or has ever been in an open state. false Kerberos Service Name The Kerberos service name to be used for authentication. See docs for more info. postgres Include Realm The Kerberos realm to be used for authentication. See docs for more info. Include Error Detail When enabled, PostgreSQL error and notice details are included on <xref:Npgsql.PostgresException.Detail?displayProperty=nameWithType> and <xref:Npgsql.PostgresNotice.Detail?displayProperty=nameWithType>. These can contain sensitive data. false Log Parameters When enabled, parameter values are logged when commands are executed. false Pooling Parameter Description Default Pooling Whether connection pooling should be used. true Minimum Pool Size The minimum connection pool size. 0 Maximum Pool Size The maximum connection pool size. 100 since 3.1, 20 previously Connection Idle Lifetime The time (in seconds) to wait before closing idle connections in the pool if the count of all connections exceeds Minimum Pool Size. Introduced in 3.1. 300 Connection Pruning Interval How many seconds the pool waits before attempting to prune idle connections that are beyond idle lifetime (see Connection Idle Lifetime). Introduced in 3.1. 10 Connection Lifetime The total maximum lifetime of connections (in seconds). Connections which have exceeded this value will be destroyed instead of returned from the pool. This is useful in clustered configurations to force load balancing between a running server and a server just brought online. 0 (disabled) Timeouts and keepalive Parameter Description Default Timeout The time to wait (in seconds) while trying to establish a connection before terminating the attempt and generating an error. 15 Command Timeout The time to wait (in seconds) while trying to execute a command before terminating the attempt and generating an error. Set to zero for infinity. 30 Cancellation Timeout The time to wait (in milliseconds) while trying to read a response for a cancellation request for a timed out or cancelled query, before terminating the attempt and generating an error. -1 skips the wait, 0 means infinite wait. Introduced in 5.0. 2000 Keepalive The number of seconds of connection inactivity before Npgsql sends a keepalive query. 0 (disabled) Tcp Keepalive Whether to use TCP keepalive with system defaults if overrides isn't specified. false Tcp Keepalive Time The number of seconds of connection inactivity before a TCP keepalive query is sent. Use of this option is discouraged, use KeepAlive instead if possible. 0 (disabled) Tcp Keepalive Interval The interval, in seconds, between when successive keep-alive packets are sent if no acknowledgement is received. Tcp KeepAlive Time must be non-zero as well. value of Tcp Keepalive Time Performance Parameter Description Default Max Auto Prepare The maximum number SQL statements that can be automatically prepared at any given point. Beyond this number the least-recently-used statement will be recycled. Zero disables automatic preparation. 0 Auto Prepare Min Usages The minimum number of usages an SQL statement is used before it's automatically prepared. 5 Read Buffer Size Determines the size of the internal buffer Npgsql uses when reading. Increasing may improve performance if transferring large values from the database. 8192 Write Buffer Size Determines the size of the internal buffer Npgsql uses when writing. Increasing may improve performance if transferring large values to the database. 8192 Socket Receive Buffer Size Determines the size of socket receive buffer. System-dependent Socket Send Buffer Size Determines the size of socket send buffer. System-dependent No Reset On Close Improves performance in some cases by not resetting the connection state when it is returned to the pool, at the cost of leaking state. Use only if benchmarking shows a performance improvement false Failover and load balancing For more information, see the dedicated docs page. Parameter Description Default Target Session Attributes Determines the preferred PostgreSQL target server type. PGTARGETSESSIONATTRS, Any Load Balance Hosts Enables balancing between multiple hosts by round-robin. false Host Recheck Seconds Controls for how long the host's cached state will be considered as valid. 10 Misc Parameter Description Default Options1 Specifies any valid PostgreSQL connection options (e.g. Options=-c synchronous_commit=local). Introduced in 5.0. PGOPTIONS Application Name The optional application name parameter to be sent to the backend during connection initiation. Enlist Whether to enlist in an ambient TransactionScope. true Search Path Sets the schema search path. Client Encoding Gets or sets the client_encoding parameter. PGCLIENTENCODING, UTF8 Encoding Gets or sets the .NET encoding that will be used to encode/decode PostgreSQL string data. UTF8 Timezone Gets or sets the session timezone. PGTZ EF Template Database The database template to specify when creating a database in Entity Framework. template1 EF Admin Database The database admin to specify when creating and dropping a database in Entity Framework. template1 Load Table Composites Load table composite type definitions, and not just free-standing composite types. false Array Nullability Mode Configure the way arrays of value types are returned when requested as object instances. Possible values are: Never (arrays of value types are always returned as non-nullable arrays), Always (arrays of value types are always returned as nullable arrays) and PerInstance (the type of array that gets returned is determined at runtime). Never 1The Options connection string parameter is essentially the string of command line options that get passed to the postgres program when the process is started. It is most commonly used to set named run-time parameters via the -c option but other options can be used too (although not all of them make sense in that context). Setting multiple options is possible by separating them with a space character. Space and backslash characters in option values need to be escaped by prefixing a backslash character. Example: Options=-c default_transaction_isolation=serializable -c default_transaction_deferrable=on -c foo.bar=My\\\\ Famous\\\\\\\\Thing Compatibility Parameter Description Default Server Compatibility Mode A compatibility mode for special PostgreSQL server types. Currently \"Redshift\" is supported, as well as \"NoTypeLoading\", which will bypass the normal type loading mechanism from the PostgreSQL catalog tables and supports a hardcoded list of basic types. none Obsolete Parameter Description Default Internal Command Timeout The time to wait (in seconds) while trying to execute an internal command before terminating the attempt and generating an error. -1 uses CommandTimeout, 0 means no timeout. -1 Environment variables In addition to the connection string parameters above, Npgsql also recognizes the standard PostgreSQL environment variables below. This helps Npgsql-based applications behave similar to other, non-.NET PostgreSQL client applications. The PostgreSQL doc page on environment variables recognized by libpq can be found here. Environment variable Description PGUSER Behaves the same as the user connection parameter. PGPASSWORD Behaves the same as the password connection parameter. Use of this environment variable is not recommended for security reasons, as some operating systems allow non-root users to see process environment variables via ps; instead consider using a password file (see Section 33.15). PGPASSFILE Behaves the same as the passfile connection parameter. PGSSLCERT Behaves the same as the sslcert connection parameter. PGSSLKEY Behaves the same as the sslkey connection parameter. PGSSLROOTCERT Behaves the same as the sslrootcert connection parameter. PGCLIENTENCODING Behaves the same as the client_encoding connection parameter. PGTZ Sets the default time zone. (Equivalent to SET timezone TO ....) PGOPTIONS Behaves the same as the options connection parameter."
  },
  "doc/conceptual/Npgsql/contributing.html": {
    "href": "doc/conceptual/Npgsql/contributing.html",
    "title": "Contributing to Npgsql | Npgsql Documentation",
    "keywords": "Contributing to Npgsql As a general rule, Npgsql makes no attempt to validate what it sends to PostgreSQL. For all cases where PostgreSQL would simply return a reasonable error, we prefer that to happen rather than checking replicating validation checks client-side."
  },
  "doc/conceptual/Npgsql/copy.html": {
    "href": "doc/conceptual/Npgsql/copy.html",
    "title": "COPY | Npgsql Documentation",
    "keywords": "COPY PostgreSQL has a feature allowing efficient bulk import or export of data to and from a table. This is usually a much faster way of getting data in and out of a table than using INSERT and SELECT. See documentation for the COPY command for more details. Npgsql supports three COPY operation modes: binary, text and raw binary. Binary COPY This mode uses the efficient PostgreSQL binary format to transfer data in and out of the database. The user uses an API to read and write rows and fields, which Npgsql decodes and encodes. When you've finished, you must call Complete() to save the data; not doing so will cause the COPY operation to be rolled back when the writer is disposed (this behavior is important in case an exception is thrown). Warning It is the your responsibility to read and write the correct type! If you use COPY to write an int32 into a string field you may get an exception, or worse, silent data corruption. It is also highly recommended to use the overload of Write() which accepts an NpgsqlDbType, allowing you to unambiguously specify exactly what type you want to write. Test your code thoroughly. // Import two columns to table data using (var writer = conn.BeginBinaryImport(\"COPY data (field_text, field_int2) FROM STDIN (FORMAT BINARY)\")) { writer.StartRow(); writer.Write(\"Hello\"); writer.Write(8, NpgsqlDbType.Smallint); writer.StartRow(); writer.Write(\"Goodbye\"); writer.WriteNull(); writer.Complete(); } // Export two columns to table data using (var reader = Conn.BeginBinaryExport(\"COPY data (field_text, field_int2) TO STDOUT (FORMAT BINARY)\")) { reader.StartRow(); Console.WriteLine(reader.Read<string>()); Console.WriteLine(reader.Read<int>(NpgsqlDbType.Smallint)); reader.StartRow(); reader.Skip(); Console.WriteLine(reader.IsNull); // Null check doesn't consume the column Console.WriteLine(reader.Read<int>()); reader.StartRow(); // Last StartRow() returns -1 to indicate end of data } Text COPY This mode uses the PostgreSQL text or csv format to transfer data in and out of the database. It is the user's responsibility to format the text or CSV appropriately, Npgsql simply provides a TextReader or Writer. This mode is less efficient than binary copy, and is suitable mainly if you already have the data in a CSV or compatible text format and don't care about performance. using (var writer = conn.BeginTextImport(\"COPY data (field_text, field_int4) FROM STDIN\")) { writer.Write(\"HELLO\\t1\\n\"); writer.Write(\"GOODBYE\\t2\\n\"); } using (var reader = conn.BeginTextExport(\"COPY data (field_text, field_int4) TO STDOUT\")) { Console.WriteLine(reader.ReadLine()); Console.WriteLine(reader.ReadLine()); } Raw Binary COPY In this mode, data transfer is binary, but Npgsql does no encoding or decoding whatsoever - data is exposed as a raw .NET Stream. This mode makes sense only for bulk data and restore a table: the table is saved as a blob, which can later be restored. If you need to actually make sense of the data, you should be using regular binary mode instead (not raw). Example: int len; var data = new byte[10000]; // Export table1 to data array using (var inStream = conn.BeginRawBinaryCopy(\"COPY table1 TO STDOUT (FORMAT BINARY)\")) { // We assume the data will fit in 10000 bytes, in real usage you would read repeatedly, writine to a file. len = inStream.Read(data, 0, data.Length); } // Import data array into table2 using (var outStream = conn.BeginRawBinaryCopy(\"COPY table2 FROM STDIN (FORMAT BINARY)\")) { outStream.Write(data, 0, len); } Cancel Import operations can be cancelled at any time by disposing NpgsqlBinaryImporter without calling Complete() on it. Export operations can be cancelled as well, by calling Cancel(). Other See the CopyTests.cs test fixture for more usage samples."
  },
  "doc/conceptual/Npgsql/dev/tests.html": {
    "href": "doc/conceptual/Npgsql/dev/tests.html",
    "title": "Tests | Npgsql Documentation",
    "keywords": "Overview Npgsql has an extensive test suite to guard against regressions. The test suite is run on the official build server for the .NET Framework and .NET Core with all supported PostgreSQL backends. Continuous integration results are publicly available via Github Actions. Getting Started Setup PostgreSQL The Npgsql test suite requires a PostgreSQL backend for tests to run. By default, the test suite expects PostgreSQL to be running on the local machine with the default port (5432). Install PostgreSQL: https://www.postgresql.org/download Start the PostgreSQL backend. Create the npgsql_tests account By default, the test suite expects an account named npgsql_tests with a password of npgsql_tests. This account is used by the test suite to create a database named npgsql_tests and run the tests. $ psql -h localhost -U postgres postgres=# CREATE USER npgsql_tests PASSWORD 'npgsql_tests' SUPERUSER; postgres=# CREATE DATABASE npgsql_tests OWNER npgsql_tests; Note: superuser access is required to create and drop test databases, load extensions (e.g. hstore, postgis), etc. Clone the repository cd ~ git clone git@github.com:npgsql/npgsql.git (use ssh) git clone https://github.com/npgsql/npgsql.git (use https) Run the test suite cd ~/npgsql dotnet test ./test/Npgsql.Tests dotnet test ./test/Npgsql.PluginTests dotnet test ./test/Npgsql.Benchmarks"
  },
  "doc/conceptual/Npgsql/dev/type-representations.html": {
    "href": "doc/conceptual/Npgsql/dev/type-representations.html",
    "title": "PostgreSQL Types | Npgsql Documentation",
    "keywords": "Overview The following are notes by Emil Lenngren on PostgreSQL wire representation of types: bool: text: t or f binary: a byte: 1 or 0 bytea: text: either \\x followed by hex-characters (lowercase by default), or plain characters, where non-printable characters (between 0x20 and 0x7e, inclusive) are written as \\nnn (octal) and \\ is written as \\\\ binary: the bytes as they are char: This type holds a single char/byte. (Not to be confused with bpchar (blank-padded char) which is PostgreSQL's alias to the SQL standard's char). The char may be the null-character text: the char as a byte, encoding seems to be ignored binary: the char as a byte name: A null-padded string of NAMEDATALEN (currently 64) bytes (the last byte must be a null-character). Used in pg catalog. text: the name as a string binary: the name as a string int2/int4/int8: text: text representation in base 10 binary: binary version of the integer int2vector: non-null elements, 0-indexed, 1-dim text: 1 2 3 4 binary: same as int2[] oidvector: non-null elements, 0-indexed, 1-dim text: 1 2 3 4 binary: same as oid[] regproc: internally just an OID (UInt32) text: -, name of procedure, or numeric if not found binary: only the OID in binary regprocedure/regoper/regoperator/regclass/regconfig/regdictionary: similar to regproc text: text: the string as it is binary: the string as it is oid: A 32-bit unsigned integer used for internal object identification. text: the text-representation of this integer in base 10 binary: the UInt32 tid: tuple id Internally a tuple of a BlockNumber (UInt32) and an OffsetNumber (UInt16) text: (blockNumber,offsetNumber) binary: the block number in binary followed by offset number in binary xid: transaction id Internally just a TransactionId (UInt32) text: the number binary: the number in binary cid: command id Internally just a CommandId (UInt32) text: the number binary: the number in binary json: json text: the json an text binary: the json as text jsonb: json internally stored in an efficient binary format text: the json as text binary: An Int32 (version number, currently 1), followed by data (currently just json as text) xml: Xml. It is probably most efficient to use the text format, especially when receiving from client. text: the xml as text (when sent from the server: encoding removed, when receiving: assuming database encoding) binary: the xml as text (when sent from the server: in the client's specified encoding, when receiving: figures out itself) pg_node_tree: used as type for the column typdefaultbin in pg_type does not accept input text: text binary: text smgr: storage manager can only have the value \"magnetic disk\" text: magnetic disk binary: not available point: A tuple of two float8 text: (x,y) The floats are interpreted with the C strtod function. The floats are written with the snprintf function, with %.*g format. NaN/-Inf/+Inf can be written, but not interpretability depends on platform. The extra_float_digits setting is honored. For linux, NaN, [+-]Infinity, [+-]Inf works, but not on Windows. Windows also have other output syntax for these special numbers. (1.#QNAN for example) binary: the two floats lseg: A tuple of two points text: [(x1,y1),(x2,y2)] see point for details binary: the four floats in the order x1, y1, x2, y2 path: A boolean whether the path is opened or closed + a vector of points. text: [(x1,y1),...] for open path and ((x1,y1),...) for closed paths. See point for details. binary: first a byte indicating open (0) or close (1), then the number of points (Int32), then a vector of points box: A tuple of two points. The coordinates will be reordered so that the first is the upper right and the second is the lower left. text: (x1,y1),(x2,y2) see point for details binary: the four floats in the order x1, y1, x2, y2 (doesn't really matter since they will be reordered) polygon: Same as path but with two differences: is always closed and internally stores the bounding box. text: same as closed path binary: the number of points (Int32), then a vector of points line (version 9.4): Ax + By + C = 0. Stored with three float8. Constraint: A and B must not both be zero (only checked on text input, not binary). text: {A,B,C} see point for details about the string representation of floats. Can also use the same input format as a path with two different points, representing the line between those. binary: the three floats circle: <(x,y),r> (center point and radius), stored with three float8. text: <(x,y),r> see point for details about the string representation of floats. binary: the three floats x, y, r in that order float4/float8: text: (leading/trailing whitespace is skipped) interpreted with the C strtod function, but since it has problems with NaN, [+-]Infinity, [+-]Inf, those strings are identified (case-insensitively) separately. when outputting: NaN, [+-]Infinity is treated separately, otherwise the string is printed with snprintf %.*g and the extra_float_digits setting is honored. binary: the float abstime: A unix timestamp stored as a 32-bit signed integer with seconds-precision (seconds since 1970-01-01 00:00:00), in UTC Has three special values: Invalid (2^31-1), infinity (2^31-3), -infinity (-2^31) text: same format as timestamptz, or \"invalid\", \"infinity\", \"-infinity\" binary: Int32 reltime: A time interval with seconds-precision (stored as an 32-bit signed integer) text: same as interval binary: Int32 tinterval: Consists of a status (Int32) and two abstimes. Status is valid (1) iff both abstimes are valid, else 0. Note that the docs incorrectly states that ' is used as quote instead of \" text: [\"<abstime>\" \"<abstime>\"] binary: Int32 (status), Int32 (abstime 1), Int32 (abstime 2) unknown: text: text binary: text money: A 64-bit signed integer. For example, $123.45 is stored as the integer 12345. Number of fraction digits is locale-dependent. text: a locale-depedent string binary: the raw 64-bit integer macaddr: 6 bytes text: the 6 bytes in hex (always two characters per byte) separated by : binary: the 6 bytes appearing in the same order as when written in text inet/cidr: Struct of Family (byte: ipv4=2, ipv6=3), Netmask (byte with number of bits in the netmask), Ipaddr bytes (16) Text: The IP-address in text format and /netmask. /netmask is omitted in inet if the netmask is the whole address. Binary: family byte, netmask byte, byte (cidr=1, inet=0), number of bytes in address, bytes of the address aclitem: Access list item used in pg_class Text: Something like postgres=arwdDxt/postgres Binary: not available bpchar: Blank-padded char. The type modifier is used to blank-pad the input. text: text binary: text varchar: Variable-length char. The type modifier is used to check the input's length. text: text binary: text date: A signed 32-bit integer of a date. 0 = 2000-01-01. Infinity: INT_MAX, -Infinity: INT_MIN Text: Date only using the specified date style Binary: Int32 time: A signed 64-bit integer representing microseconds from 00:00:00.000000. (Legacy uses 64-bit float). Negative values are not allowed. Max value is 24:00:00.000000. text: hh:mm:ss or hh:mm:ss.ffffff where the fraction part is between 1 and 6 digits (trailing zeros are not written) binary: the 64-bit integer timetz: A struct of Time: A signed 64-bit integer representing microseconds from 00:00:00.000000. (Legacy uses 64-bit float). Negative values are not allowed. Max value is 24:00:00.000000. Zone: A signed 32-bit integer representing the zone (in seconds). Note that the sign is inverted. So GMT+1h is stored as -1h. text: hh:mm:ss or hh:mm:ss.ffffff where the fraction part is between 1 and 6 digits (trailing zeros are not written) binary: the 64-bit integer followed by the 32-bit integer timestamp: A signed 64-bit integer representing microseconds from 2000-01-01 00:00:00.000000 Infinity is LONG_MAX and -Infinity is LONG_MIN (Infinity would be 294277-01-09 04:00:54.775807) Earliest possible timestamp is 4714-11-24 00:00:00 BC. Even earlier would be possible, but due to internal calculations those are forbidden. text: dependent on date style binary: the 64-bit integer timestamptz: A signed 64-bit integer representing microseconds from 2000-01-01 00:00:00.000000 UTC. (Time zone is not stored). Infinity is LONG_MAX and -Infinity is LONG_MIN text: first converted to the time zone in the db settings, then printed according to the date style binary: the 64-bit integer interval: A struct of Time (Int64): all time units other than days, months and years (microseconds) Day (Int32): days, after time for alignment Month (Int32): months and years, after time for alignment text: Style dependent, but for example: \"-11 mons +15435 days -11111111:53:00\" binary: all fields in the struct bit/varbit: First a signed 32-bit integer containing the number of bits (negative length not allowed). Then all the bits in big end first. So a varbit of length 1 has the first (and only) byte set to either 0x80 or 0x00. Last byte is assumed (and is automatically zero-padded in recv) to be zero-padded. text: when sending from backend: all the bits, written with 1s and 0s. when receiving from client: (optionally b or B followed by) all the bits as 1s and 0s, or a x or X followed by hexadecimal digits (upper- or lowercase), big endian first. binary: the 32-bit length followed by the bytes containing the bits numeric: A variable-length numeric value, can be negative. text: NaN or first - if it is negative, then the digits with . as decimal separator binary: first a header of 4 16-bit integers: number of digits in the digits array that follows (unsigned integer), weight of the first digit (10000^weight), can be both negative, positive or 0, sign: negative=0x4000, positive=0x0000, NaN=0xC000 dscale: number of digits (in base 10) to print after the decimal separator then the array of digits: The digits are stored in base 10000, where each digit is a 16-bit integer. Trailing zeros are not stored in this array, to save space. The digits are stored such that, if written as base 10000, the decimal separator can be inserted between two digits in base 10000, i.e. when this is to be printed in base 10, only the first digit in base 10000 can (possibly) be printed with less than 4 characters. Note that this does not apply for the digits after the decimal separator; the digits should be printed out in chunks of 4 characters and then truncated with the given dscale. refcursor: uses the same routines as text record: Describes a tuple. Is also the \"base class\" for composite types (i.e. it uses the same i/o functions). text: ( followed by a list of comma-separated text-encoded values followed by ). Empty element means null. Quoted with \" and \" if necessary. \" is escaped with \"\" and \\ is escaped with \\\\ (this differs from arrays where \" is escaped with \\\"). Must be quoted if it is an empty string or contains one of \"\\,() or a space. binary: First a 32-bit integer with the number of columns, then for each column: An OID indicating the type of the column The length of the column (32-bit integer), or -1 if null The column data encoded as binary cstring: text/binary: all characters are sent without the trailing null-character void: Used for example as return value in SELECT * FROM func_returning_void() text: an empty string binary: zero bytes uuid: A 16-byte uuid. text: group of 8, 4, 4, 4, 12 hexadecimal lower-case characters, separated by -. The first byte is written first. It is allowed to surround it with {}. binary: the 16 bytes txid_snapshot: (txid is a UInt64) A struct of UInt32 nxip (size of the xip array) txid xmin (no values in xip is smaller than this) txid xmax (no values in xip is larger than or equal this) txid[] xip (is ordered in ascending order) text: xmin:xmax:1,2,3,4 binary: all fields in the structure tsvector: Used for text searching. Example of tsvector: 'a':1,6,10 'on':5 'and':8 'ate':9A 'cat':3 'fat':2,11 'mat':7 'rat':12 'sat':4 Max length for each lexeme string is 2046 bytes (excluding the trailing null-char) The words are sorted when parsed, and only written once. Positions are also sorted and only written once. For some reason, the unique check does not seem to be made for binary input, only text input... text: As seen above. ' is escaped with '' and \\ is escaped with \\\\. binary: UInt32 number of lexemes for each lexeme: lexeme text in client encoding, null-terminated UInt16 number of positions for each position: UInt16 WordEntryPos, where the most significant 2 bits is weight, and the 14 least significant bits is pos (can't be 0). Weights 3,2,1,0 represent A,B,C,D tsquery: A tree with operands and operators (&, |, !). Operands are strings, with optional weight (bitmask of ABCD) and prefix search (yes/no, written with *). text: the tree written in infix notation. Example: ( 'abc':*B | 'def' ) & !'ghi' binary: the tree written in prefix notation: First the number of tokens (a token is an operand or an operator). For each token: UInt8 type (1 = val, 2 = oper) followed by For val: UInt8 weight + UInt8 prefix (1 = yes / 0 = no) + null-terminated string, For oper: UInt8 oper (1 = not, 2 = and, 3 = or, 4 = phrase). In case of phrase oper code, an additional UInt16 field is sent (distance value of operator). Default is 1 for <->, otherwise the n value in '<n>'. enum: Simple text gtsvector: GiST for tsvector. Probably internal type. int4range/numrange/tsrange/tstzrange/daterange/int8range and user-defined range types: /* A range's flags byte contains these bits: */ #define RANGE_EMPTY 0x01 /* range is empty */ #define RANGE_LB_INC 0x02 /* lower bound is inclusive */ #define RANGE_UB_INC 0x04 /* upper bound is inclusive */ #define RANGE_LB_INF 0x08 /* lower bound is -infinity */ #define RANGE_UB_INF 0x10 /* upper bound is +infinity */ #define RANGE_LB_NULL 0x20 /* lower bound is null (NOT USED) */ #define RANGE_UB_NULL 0x40 /* upper bound is null (NOT USED) */ #define RANGE_CONTAIN_EMPTY 0x80/* marks a GiST internal-page entry whose * subtree contains some empty ranges */ A range has no lower bound if any of RANGE_EMPTY, RANGE_LB_INF (or RANGE_LB_NULL, not used anymore) is set. The same applies for upper bounds. text: A range with RANGE_EMPTY is just written as the string \"empty\". Inclusive bounds are written with [ and ], else ( and ) is used. The two values are comma-separated. Missing bounds are written as an empty string (without quotes). Each value is quoted with \" if necessary. Quotes are necessary if the string is either the empty string or contains \"\\,()[] or spaces. \" is escaped with \"\" and \\ is escaped with \\\\. Example: [18,21] binary: First the flag byte. Then, if has lower bound: 32-bit length + binary-encoded data. Then, if has upper bound: 32-bit length + binary-encoded data. hstore: Key/value-store. Both keys and values are strings. text: Comma-space separated string, where each item is written as \"key\"=>\"value\" or \"key\"=>NULL. \" and \\ are escaped as \\\" and \\\\. Example: \"a\"=>\"b\", \"c\"=>NULL, \"d\"=>\"q\" binary: Int32 count for each item: Int32 keylen string of the key (not null-terminated) Int32 length of item (or -1 if null) the item as a string ghstore: internal type for indexing hstore domain types: mapped types used in information_schema: cardinal_number: int4 (must be nonnegative or null) character_data: varchar sql_identifier: varchar time_stamp: timestamptz yes_or_no: varchar(3) (must be \"YES\" or \"NO\" or null) intnotnull: when an int4 is cast to this type, it is checked that the int4 is not null, but it still returns an int4 and not intnotnull..."
  },
  "doc/conceptual/Npgsql/diagnostics/exceptions_notices.html": {
    "href": "doc/conceptual/Npgsql/diagnostics/exceptions_notices.html",
    "title": "Exceptions, errors and notices | Npgsql Documentation",
    "keywords": "Exceptions, errors and notices Exception types Most exceptions thrown by Npgsql are either of type <xref:Npgsql.NpgsqlException>, or wrapped by one; this allows your application to catch NpgsqlException where appropriate, for all database-related errors. Note that NpgsqlException is a sub-class of the general System.Data.DbException, so if your application uses more than one database type, you can catch that as well. When Npgsql itself encounters an error, it typically raises that as an NpgsqlException directly, possibly wrapping an inner exception. For example, if a networking error occurs while communicating with PostgreSQL, Npgsql will raise an NpgsqlException wrapping an IOException; this allow you both to identify the root cause of the problem, while still identifying it as database-related. In other cases, PostgreSQL itself will report an error to Npgsql; Npgsql raises these by throwing a PostgresException, which is a sub-class of NpgsqlException adding important contextual information on the error. Most importantly, PostgresException exposes the SqlState property, which contains the PostgreSQL error code. This value can be consulted to identify which error type occurred. When executing multiple commands via <xref:Npgsql.NpgsqlBatch>, the <xref:Npgsql.NpgsqlException.BatchCommand> property references the command within the batch which triggered the exception. This allows you to understand exactly what happened, and access the specific SQL which triggered the error. PostgreSQL notices Finally, PostgreSQL also raises \"notices\", which contain non-critical information on command execution. Notices are not errors: they do not indicate failure and can be safely ignored, although they may contain valuable information on the execution of your commands. Npgsql logs notices in the debug logging level. To deal with notices programmatically, Npgsql also exposes the <xref:Npgsql.NpgsqlConnection.Notice> event, which you can hook into for any further processing: conn.Notice += (_, args) => Console.WriteLine(args.Notice.MessageText);"
  },
  "doc/conceptual/Npgsql/diagnostics/logging.html": {
    "href": "doc/conceptual/Npgsql/diagnostics/logging.html",
    "title": "Logging | Npgsql Documentation",
    "keywords": "Logging Note Starting with version 7.0, Npgsql supports standard .NET logging via Microsoft.Extensions.Logging. If you're using an earlier version of Npgsql, skip down to this section. Npgsql fully supports logging various events via the standard .NET Microsoft.Extensions.Logging package. These can help debug issues and understand what's going on as your application interacts with PostgreSQL. Console programs To set up logging in Npgsql, create your ILoggerFactory as usual, and then configure an NpgsqlDataSource with it. Any use of connections handed out by the data source will log via your provided logger factory. The following shows a minimal console application logging to the console via Microsoft.Extensions.Logging.Console: // Create a Microsoft.Extensions.Logging LoggerFactory, configuring it with the providers, // log levels and other desired configuration. var loggerFactory = LoggerFactory.Create(builder => builder.AddConsole()); // Create an NpgsqlDataSourceBuilder, configuring it with our LoggerFactory var dataSourceBuilder = new NpgsqlDataSourceBuilder(\"Host=localhost;Username=test;Password=test\"); dataSourceBuilder.UseLoggerFactory(loggerFactory); await using var dataSource = dataSourceBuilder.Build(); // Any connections handed out by the data source will log via the LoggerFactory: await using var connection = await dataSource.OpenConnectionAsync(); await using var command = new NpgsqlCommand(\"SELECT 1\", connection); _ = await command.ExecuteScalarAsync(); Running this program outputs the following to the console: info: Npgsql.Command[2001] Command execution completed (duration=16ms): SELECT 1 By default, Npgsql logs command executions at the Information log level, as well as various warnings and errors. To see more detailed logging, increase the log level to Debug or Trace. ASP.NET and dependency injection If you're using ASP.NET, you can use the additional Npgsql.DependencyInjection package, which provides seamless integration with dependency injection and logging: var builder = WebApplication.CreateBuilder(args); builder.Logging.AddConsole(); builder.Services.AddNpgsqlDataSource(\"Host=localhost;Username=test;Password=test\"); The AddNpgsqlDataSource arranges for a data source to be configured in the DI container, which automatically uses the logger factory configured via the standard ASP.NET means. This allows your endpoints to get injected with Npgsql connections which log to the same logger factory when used. Configuration without NpgsqlDataSource If your application doesn't use NpgsqlDataSource, you can still configure Npgsql's logger factory globally, as follows: var loggerFactory = LoggerFactory.Create(builder => builder.AddConsole()); NpgsqlLoggingConfiguration.InitializeLogging(loggerFactory); await using var conn = new NpgsqlConnection(\"Host=localhost;Username=test;Password=test\"); conn.Execute(\"SELECT 1\"); Note that you must call InitializeLogging at the start of your program, before any other Npgsql API is used. Parameter logging By default, when logging SQL statements, Npgsql does not log parameter values, since these may contain sensitive information. You can opt into parameter logging when debugging your application: Console Program ASP.NET Program Without DbDataSource dataSourceBuilder.EnableParameterLogging(); builder.Services.AddNpgsqlDataSource( \"Host=localhost;Username=test;Password=test\", builder => builder.EnableParameterLogging()); NpgsqlLoggingConfiguration.InitializeLogging(loggerFactory, parameterLoggingEnabled: true); Warning Do not leave parameter logging enabled in production, as sensitive user information may leak into your logs. Logging in older versions of Npgsql Prior to 7.0, Npgsql had its own, custom logging API. To use this, statically inject a logging provider implementing the INpgsqlLoggingProvider interface as follows: NpgsqlLogManager.Provider = new ??? Note: you must set the logging provider before invoking any other Npgsql method, at the very start of your program. It's trivial to create a logging provider that passes log messages to whatever logging framework you use, you can find such an adapter for NLog below. ConsoleLoggingProvider Npgsql comes with one built-in logging provider: ConsoleLoggingProvider. It simply dumps all log messages with a given level or above to standard output. You can set it up by including the following line at the beginning of your application: NpgsqlLogManager.Provider = new ConsoleLoggingProvider(<min level>, <print level?>, <print connector id?>); Level defaults to NpgsqlLogLevel.Info (which will only print warnings and errors). You can also have log levels and connector IDs logged. NLogLoggingProvider (or implementing your own) The following provider is used in the Npgsql unit tests to pass log messages to NLog. You're welcome to copy-paste it into your project, or to use it as a starting point for implementing your own custom provider. class NLogLoggingProvider : INpgsqlLoggingProvider { public NpgsqlLogger CreateLogger(string name) { return new NLogLogger(name); } } class NLogLogger : NpgsqlLogger { readonly Logger _log; internal NLogLogger(string name) { _log = LogManager.GetLogger(name); } public override bool IsEnabled(NpgsqlLogLevel level) { return _log.IsEnabled(ToNLogLogLevel(level)); } public override void Log(NpgsqlLogLevel level, int connectorId, string msg, Exception exception = null) { var ev = new LogEventInfo(ToNLogLogLevel(level), \"\", msg); if (exception != null) ev.Exception = exception; if (connectorId != 0) ev.Properties[\"ConnectorId\"] = connectorId; _log.Log(ev); } static LogLevel ToNLogLogLevel(NpgsqlLogLevel level) { switch (level) { case NpgsqlLogLevel.Trace: return LogLevel.Trace; case NpgsqlLogLevel.Debug: return LogLevel.Debug; case NpgsqlLogLevel.Info: return LogLevel.Info; case NpgsqlLogLevel.Warn: return LogLevel.Warn; case NpgsqlLogLevel.Error: return LogLevel.Error; case NpgsqlLogLevel.Fatal: return LogLevel.Fatal; default: throw new ArgumentOutOfRangeException(\"level\"); } } }"
  },
  "doc/conceptual/Npgsql/diagnostics/metrics.html": {
    "href": "doc/conceptual/Npgsql/diagnostics/metrics.html",
    "title": "OpenTelemetry Metrics | Npgsql Documentation",
    "keywords": "OpenTelemetry Metrics Npgsql supports reporting aggregated metrics which provide snapshots on its state and activities at a given point. These can be especially useful for diagnostics issues such as connection leaks, or doing general performance analysis Metrics are reported via the standard .NET System.Diagnostics.Metrics API; see these docs for more details. The Npgsql metrics implement the experimental OpenTelemetry semantic conventions for database metrics - adding some additional useful ones - and will evolve as that specification stabilizes. Note Npgsql versions before 8.0, as well as TFMs under net6.0, emit metrics via the older Event Counters API instead of the new OpenTelemetry ones. Metrics are usually collected and processed via tools such as Prometheus, and plotted on dashboards via tools such as Grafana. Configuring .NET to emit metrics to these tools is beyond the scope of this documentation, but you can use the command-line tool dotnet-counters to quickly test Npgsql's support. To collect metrics via dotnet-counters, install the dotnet-counters tool. Then, find out your process PID, and run it as follows: dotnet counters monitor Npgsql -p <PID> dotnet-counters will now attach to your running process and start reporting continuous counter data: [Npgsql] db.client.commands.bytes_read (By / 1 sec) pool.name=CustomersDB 1,020 db.client.commands.bytes_written (By / 1 sec) pool.name=CustomersDB 710 db.client.commands.duration (s) pool.name=CustomersDB,Percentile=50 0.001 pool.name=CustomersDB,Percentile=95 0.001 pool.name=CustomersDB,Percentile=99 0.001 db.client.commands.executing ({command}) pool.name=CustomersDB 2 db.client.commands.prepared_ratio pool.name=CustomersDB 0 db.client.connections.max ({connection}) pool.name=CustomersDB 100 db.client.connections.usage ({connection}) pool.name=CustomersDB,state=idle 3 pool.name=CustomersDB,state=used 2 Note that Npgsql emits multiple dimensions with the metrics, e.g. the connection states (idle or used). In addition, an identifier for the connection pool - or data source - is emitted with every metric, allowing you to separately track e.g. multiple databases accessed in the same applications. By default, the pool.name will be the connection string, but it can be useful to give your data sources a name for easier and more consistent tracking: var builder = new NpgsqlDataSourceBuilder(\"Host=localhost;Username=test;Password=test\") { Name = \"CustomersDB\" }; await using var dataSource = builder.Build();"
  },
  "doc/conceptual/Npgsql/diagnostics/overview.html": {
    "href": "doc/conceptual/Npgsql/diagnostics/overview.html",
    "title": "Diagnostics overview | Npgsql Documentation",
    "keywords": "Diagnostics overview Npgsql provides several ways to analyze what's going on inside Npgsql and to diagnose performance issues. Each has its own dedicated doc page: Tracing allows collecting information on which queries are executed, including precise timing information on start, end and duration. These events can be collected in a database, searched, graphically explored and otherwise analyzed. Logging generates textual information on various events within Npgsql; log levels can be adjusted to collect low-level information, helpful for diagnosing errors. Metrics generates aggregated quantitative data, useful for tracking the performance of your application in realtime and over time (e.g. how many queries are currently being executed in a particular moment). For information on the exceptions thrown by Npgsql, and on notices produced by PostgreSQL, see this page."
  },
  "doc/conceptual/Npgsql/diagnostics/tracing.html": {
    "href": "doc/conceptual/Npgsql/diagnostics/tracing.html",
    "title": "Tracing with OpenTelemetry (experimental) | Npgsql Documentation",
    "keywords": "Tracing with OpenTelemetry (experimental) Note Support for tracing via OpenTelemetry has been introduced in Npgsql 6.0. The OpenTelemetry specifications for database tracing are currently experimental, so Npgsql's support may change in upcoming releases. OpenTelemetry is a widely-adopted framework for distributed observability across many languages and components; its tracing standards allow applications and libraries to emit information on activities and events, which can be exported by the application, stored and analyzed. Activities typically have start and end times, and can encompass other activities recursively; this allows you to analyze e.g. exactly how much time was spent in the database when handling a certain HTTP call. To make Npgsql emit tracing data, reference the Npgsql.OpenTelemetry NuGet package from your application, and set up tracing as follows: using var tracerProvider = Sdk.CreateTracerProviderBuilder() .SetResourceBuilder(ResourceBuilder.CreateDefault().AddService(\"npgsql-tester\")) .SetSampler(new AlwaysOnSampler()) // This optional activates tracing for your application, if you trace your own activities: .AddSource(\"MyApp\") // This activates up Npgsql's tracing: .AddNpgsql() // This prints tracing data to the console: .AddConsoleExporter() .Build(); Once this is done, you should start seeing Npgsql trace data appearing in your application's console. At this point, you can look into exporting your trace data to a more useful destination: systems such as Zipkin or Jaeger can efficiently collect and store your data, and provide user interfaces for querying and exploring it. Setting these up in your application is quite easy - simply replace the console exporter with the appropriate exporter for the chosen system. For example, Zipkin visualizes traces in the following way: In this trace, the Npgsql query (to database testdb) took around 800ms, and was nested inside the application's work1 activity, which also had another unrelated subtask1. This allows understanding the relationships between the different activities, and where time is being spent."
  },
  "doc/conceptual/Npgsql/failover-and-load-balancing.html": {
    "href": "doc/conceptual/Npgsql/failover-and-load-balancing.html",
    "title": "Multiple Hosts, Failover and Load Balancing | Npgsql Documentation",
    "keywords": "Multiple Hosts, Failover and Load Balancing Note The functionality described in this page was introduced in Npgsql 6.0. Npgsql 6.0 allows specifying multiple hosts in your application's connection strings, allowing various failover and load balancing scenarios to be supported without the need for any additional component such as pgpool or pgbouncer. This typically requires setting up replication between your multiple PostgreSQL servers, to keep your standby servers in sync with your primary; this can be done with the help of PostgreSQL logical or physical replication, and some cloud providers provide this out of the box. Whatever the solution chosen, it's important to understand that this is out of Npgsql's scope - Npgsql is only responsible for connecting to your multiple servers as described below, and not for keeping your servers in sync. Multiple servers and failover Npgsql allows you to specify multiple servers in your connection string as follows: Npgsql 7.0+ Older versions var dataSourceBuilder = new NpgsqlDataSourceBuilder(\"Host=server1,server2;Username=test;Password=test\"); await using var dataSource = dataSourceBuilder.BuildMultiHost(); await using var connection = await dataSource.OpenConnectionAsync(); await using var connection = new NpgsqlConnection(\"Host=server1,server2;Username=test;Password=test\"); Different ports may be specified per host with the standard colon syntax: Host=server1:5432,server2:5433. By default, Npgsql will try to connect to the servers in the order in which they were specified. In the above example, server2 is only used if a connection could not be established to server1 (or if the connection pool for server1 has been exhausted). This allows a simple failover setup, where Npgsql always connects to a single, primary server, but can connect to a standby in case the primary is down; this improves the reliability of your application. In this configuration, we sometimes refer to the standby as \"warm\" - it is always up and in sync with the primary, but is only used when the primary is down. Note Using failover as described above does not mean you don't have to worry about errors when your primary server is down. When opening a connection, you may get a broken connection from the pool: Npgsql has no way of knowing whether the connection is working without actually executing something on it, which would negate the perf advantages of pooling. Also, once you have an open connection, Npgsql will never implicitly retry a failed command on a failover server, since that command may be in a transaction (or otherwise depend on some state in the first connection). In other words, you must always be prepared to catch I/O-related exceptions when interacting with the database, and possibly implement a retrying strategy, opening a new connection and re-executing the series of commands. Specifying server types In the failover scenario above, if server1 goes down, server2 is typically promoted to being the new primary. However, server1 may be brought back up and assume the role of standby - the servers will have switched roles - and Npgsql will continue to connect to server1 whenever possible. If you need to connect to a specific server type - e.g. to the primary in order to perform writes - you can do so as follows: Npgsql 7.0+ Older versions await using var connection = await dataSource.OpenConnectionAsync(TargetSessionAttributes.Primary); This makes Npgsql return connections only to the primary server, regardless of where it's located in the host list you provide. You can also get a separate data source which will only return connections to a specific server type. For example, you can create a primary-only data sources in your application startup, and use that data source as usual: // At startup: _primaryDataSource = dataSource.WithTargetSession(TargetSessionAttributes.Primary); // ... and wherever you need a connection: await using var connection = await _primaryDataSource.OpenConnectionAsync(); await using var connection = new NpgsqlConnection(\"Host=server1,server2;Username=test;Password=test;Target Session Attributes=primary\"); This makes Npgsql return connections only to the primary server, regardless of where it's located in the host list you provide. Load distribution Going a step further, it's important to understand that applications don't always make use of the database in the same way; some parts of your application only need to read data from the database, while others need to write data. If you have one or more standby servers, Npgsql can dispatch read-only queries to those servers to reduce the load on your primary. While the failover setup described above improves reliability, this technique improves performance. You can tell Npgsql that you prefer a connection to a standby: Npgsql 7.0+ Older versions // At startup: _preferStandbyDatASource = dataSource.WithTargetSession(TargetSessionAttributes.PreferStandby); // ... and wherever you need a connection: await using var connection = await _preferStandbyDatASource.OpenConnectionAsync(); await using var connection = new NpgsqlConnection(\"Host=server1,server2;Username=test;Password=test;Target Session Attributes=prefer-standby\"); With \"prefer standby\", as long as at least one standby server is available, Npgsql returns connections to that server. However, if all standby servers are down (or have exhausted their Max Pool Size setting), a connection to the primary is returned instead. The following options are supported for the target session attributes: Npgsql 7.0+ Older versions Option Description Any Any successful connection is acceptable. Primary Server must not be in hot standby mode (pg_is_in_recovery() must return false). Standby Server must be in hot standby mode (pg_is_in_recovery() must return true). PreferPrimary First try to find a primary server, but if none of the listed hosts is a primary server, try again in Any mode. PreferStandby First try to find a standby server, but if none of the listed hosts is a standby server, try again in Any mode. ReadWrite Session must accept read-write transactions by default (that is, the server must not be in hot standby mode and the default_transaction_read_only parameter must be off). ReadOnly Session must not accept read-write transactions by default (the converse). Option Description any Any successful connection is acceptable. primary Server must not be in hot standby mode (pg_is_in_recovery() must return false). standby Server must be in hot standby mode (pg_is_in_recovery() must return true). prefer-primary First try to find a primary server, but if none of the listed hosts is a primary server, try again in Any mode. prefer-standby First try to find a standby server, but if none of the listed hosts is a standby server, try again in Any mode. read-write Session must accept read-write transactions by default (that is, the server must not be in hot standby mode and the default_transaction_read_only parameter must be off). read-only Session must not accept read-write transactions by default (the converse). Npgsql detects whether a server is a primary or a standby by occasionally querying pg_is_in_recovery(), and whether a server is read-write or read-only by querying default_transaction_read_only - this is consistent with how PostgreSQL's libpq implements target_session_attributes. Servers are queried just before a connection is returned from the pool; the query intervals can be controlled via the Host Recheck Seconds parameter (10 seconds by default). Note If you choose to distribute load across multiple servers, make sure you understand what consistency guarantees are provided by PostgreSQL in your particular setup. In some cases, hot standbys lag behind their primary servers, and will therefore return slightly out-of-date results. This is usually OK, but if you require up-to-date results at all times, synchronous commit may provide a good solution (albeit with a performance cost). Load balancing We have seen how to select servers based on the type of workload we want to execute. However, in the above examples, Npgsql still attempts to return connections based on the host order specified in the connection string; this concentrates load on a single primary and possibly a single secondary, and doesn't balance load across multiple servers of the same type. You can specify Load Balance Hosts=true in the connection string to instruct Npgsql to load balance across all servers, by returning connections in round-robin fashion: Host=server1,server2,server3,server4,server5;Username=test;Password=test;Load Balance Hosts=true;Target Session Attributes=prefer-standby With this connection string, every time a connection is opened, Npgsql starts at a different point in the list. For example, in the 3rd connection attempt, Npgsql first tries to return a connection to server3; if that server is reachable and is a standby, it is selected. This allows spreading your (typically read-only) application load across all available servers, and can greatly improve your scalability."
  },
  "doc/conceptual/Npgsql/faq.html": {
    "href": "doc/conceptual/Npgsql/faq.html",
    "title": "FAQ | Npgsql Documentation",
    "keywords": "FAQ How can I call a PostgreSQL 11 stored procedure? I tried doing so with CommandType.StoredProcedure and got an error... PostgreSQL 11 stored procedures can be called, but unfortunately not with CommandType.StoredProcedure. PostgreSQL has supported stored functions for a long while, and since these have acted as replacements for non-existing procedures, Npgsql's CommandType.StoredProcedure has been implemented to invoke them; this means that CommandType.StoredProcedure translates into SELECT * FROM my_stored_function(). The new stored procedures introduce a special invocation syntax - CALL my_stored_procedure() - which is incompatible with the existing stored function syntax. On the brighter side, it's very easy to invoke stored procedures (or functions) yourself - you don't really need CommandType.StoredProcedure. Simply create a regular command and set CommandText to CALL my_stored_procedure(@p1, @p2), handling parameters like you would any other statement. In fact, with Npgsql and PostgreSQL, CommandType.StoredProcedure doesn't really have any added value over constructing the command yourself. I opened a pooled connection, and it throws right away when I use it! What gives? We know it's frustrating and seems weird, but this behavior is by-design. While your connection is idle in the pool, any number of things could happen to it - a timeout could cause it to break, or some other similar network problem. Unfortunately, with the way networking works, there is no reliable way for us to know on the client if a connection is still alive; the only thing we can do is send something to PostgreSQL, and wait for the response to arrive. Doing this whenever a connection is handed out from the pool would kill the very reason pooling exists - it would dramatically slow down pooling, which is there precisely to avoid unneeded network roundtrips. But the reality is even more grim than that. Even if Npgsql checked whether a connection is live before handing it out of the pool, there's nothing guaranteeing that the connection won't break 1 millisecond after that check - it's a total race condition. So the check wouldn't just degrade performance, it would also be largely useless. The reality of network programming is that I/O errors can occur at any point, and your code must take that into account if it has high reliability requirements. Resilience/retrying systems can help you with this; take a look at Polly as an example. One thing which Npgsql can do to help a bit, is the keepalive feature; this does a roundtrip with PostgreSQL every e.g. 1 second - including when the connection is idle in the pool - and destroys it if an I/O error occurs. However, depending on timing, you may still get a broken connection out of the pool - unfortunately there's simply no way around that. I get an exception \"The field field1 has a type currently unknown to Npgsql (OID XXXXX). You can retrieve it as a string by marking it as unknown\". Npgsql has to implement support for each PostgreSQL type, and it seems you've stumbled upon an unsupported type. First, head over to our issues page and check if an issue already exists on your type, otherwise please open one to let us know. Then, as a workaround, you can have your type treated as text - it will be up to you to parse it in your program. One simple way to do this is to append ::TEXT in your query (e.g. SELECT 3::TEXT). If you don't want to modify your query, Npgsql also includes an API for requesting types as text. The following code returns all the columns in the resultset as text: await using (var cmd = new NpgsqlCommand(...)) { cmd.AllResultTypesAreUnknown = true; await using var reader = await cmd.ExecuteReaderAsync(); // Read everything as strings } You can also specify text only for some columns in your resultset: await using (var cmd = new NpgsqlCommand(...)) { // Only the second field will be fetched as text cmd.UnknownResultTypeList = new[] { false, true }; await using var reader = await cmd.ExecuteReaderAsync(); // Read everything as strings } I'm trying to write a JSONB type and am getting 'column \"XXX\" is of type jsonb but expression is of type text' When sending a JSONB parameter, you must explicitly specify its type to be JSONB with NpgsqlDbType: await using (var cmd = new NpgsqlCommand(\"INSERT INTO foo (col) VALUES (@p)\", conn)) { cmd.Parameters.AddWithValue(\"p\", NpgsqlDbType.Jsonb, jsonText); } I'm trying to apply an Entity Framework 6 migration and I get Type is not resolved for member 'Npgsql.NpgsqlException,Npgsql' Unfortunately, a shortcoming of EF6 requires you to have Npgsql.dll in the Global Assembly Cache (GAC), otherwise you can't see migration-triggered exceptions. You can add Npgsql.dll to the GAC by opening a VS Developer Command Prompt as administator and running the command gacutil /i Npgsql.dll. You can remove it from the GAC with gacutil /u Npgsql."
  },
  "doc/conceptual/Npgsql/index.html": {
    "href": "doc/conceptual/Npgsql/index.html",
    "title": "Documentation | Npgsql Documentation",
    "keywords": "Getting Started The best way to use Npgsql is to install its nuget package. Npgsql aims to be fully ADO.NET-compatible, its API should feel almost identical to other .NET database drivers. Here's a basic code snippet to get you started: var connectionString = \"Host=myserver;Username=mylogin;Password=mypass;Database=mydatabase\"; await using var dataSource = NpgsqlDataSource.Create(connectionString); // Insert some data await using (var cmd = dataSource.CreateCommand(\"INSERT INTO data (some_field) VALUES ($1)\")) { cmd.Parameters.AddWithValue(\"Hello world\"); await cmd.ExecuteNonQueryAsync(); } // Retrieve all rows await using (var cmd = dataSource.CreateCommand(\"SELECT some_field FROM data\")) await using (var reader = await cmd.ExecuteReaderAsync()) { while (await reader.ReadAsync()) { Console.WriteLine(reader.GetString(0)); } } You can find more info about the ADO.NET API in the MSDN docs or in many tutorials on the Internet."
  },
  "doc/conceptual/Npgsql/installation.html": {
    "href": "doc/conceptual/Npgsql/installation.html",
    "title": "Installation | Npgsql Documentation",
    "keywords": "Offical Packages Official releases of Npgsql are always available on nuget.org. This is the recommended way to use Npgsql. We occasionally publish previews to nuget.org as well - these are generally quite safe for use, and can help us find issues before official packages are released. Daily Builds In addition to the official releases, we automatically publish CI packages for every build. You can use these to test new features or bug fixes that haven't been released yet. Two CI nuget feeds are available: The patch feed contains CI packages for the next hotfix/patch version. These packages are generally very stable and safe. To use it, add https://www.myget.org/F/npgsql/api/v3/index.json to your NuGet.Config. The vNext feed contains CI packages for the next minor or major versions. These are less stable and should be tested with care. To use it, add https://www.myget.org/F/npgsql-vnext/api/v3/index.json to your NuGet.Config. Older, unsupported installation methods Windows MSI Installer If you need to use Npgsql as a database provider for PowerBI, Excel or other similar systems, you need to install it into the Windows Global Assembly Cache (GAC), and do some special configuration. Npgsql versions prior to 5.0.0 provided a Windows installer (MSI) which does the installation for you, and which are still usable and maintained with critical bug fixes. Do not use the Windows MSI installer unless you're sure that your program requires GAC installation - this method is otherwise highly discouraged. The Npgsql Windows MSI installer for Npgsql 4.1.x can be found on our Github releases page: it will install Npgsql (and optionally the Entity Framework providers) into your GAC and add Npgsql's DbProviderFactory into your machine.config file. Some additional assemblies which are Npgsql dependencies will be installed into the GAC as well (e.g. System.Threading.Tasks.Extensions.dll). Visual Studio Integration Older versions of Npgsql came with a Visual Studio extension (VSIX) which integrated PostgreSQL access into Visual Studio. The extension allowed connecting to PostgreSQL from within Visual Studio's Server Explorer, creating an Entity Framework 6 model from an existing database, etc. The extension had various limitations and known issues, mainly because of problems with Visual Studio's extensibility around database. Use of the extension is no longer recommended. However, if you'd like to give it a try, it can be installed directly from the Visual Studio Marketplace page. DbProviderFactory in .NET Framework On .NET Framework, you can register Npgsql's DbProviderFactory in your applications App.Config (or Web.Config), allowing you to use general, provider-independent ADO.NET types in your application (e.g. DbConnection instead of NpgsqlConnection) - see this tutorial. To do this, add the following to your App.config: <system.data> <DbProviderFactories> <add name=\"Npgsql Data Provider\" invariant=\"Npgsql\" description=\".Net Data Provider for PostgreSQL\" type=\"Npgsql.NpgsqlFactory, Npgsql, Culture=neutral, PublicKeyToken=5d8b90d52f46fda7\"/> </DbProviderFactories> </system.data>"
  },
  "doc/conceptual/Npgsql/keepalive.html": {
    "href": "doc/conceptual/Npgsql/keepalive.html",
    "title": "Keepalive | Npgsql Documentation",
    "keywords": "Keepalive Some clients keep idle connections for long periods of time - this is especially frequent when waiting for PostgreSQL notifications. In this scenario, how can the client know the connection is still up, and hasn't been broken by a server or network outage? For this purpose, Npgsql has a keepalive feature, which makes it initiate periodic ping roundtrips. This feature is by default disabled, and must be enabled via the Keepalive connection string parameter, setting the number of seconds between each keepalive. When keepalive is enabled, Npgsql will emit an NpgsqlConnection.StateChange event if the keepalive fails. Note that you should only turn this feature on if you need it. Unless you know you'll have long-lived idle connections, and that your backend (or network equipment) will interfere with these connections, it's better to leave this off. TCP Keepalives The keepalive mechanism above is ideal for long-standing idle connections, but it cannot be used during query processing. With some PostgreSQL-like data warehouse products such as Amazon Redshift or Greenplum, it is not uncommon for a single SQL statement to take a long time to execute, and during that time it is not possible to send application-level pings. For these cases you may want to turn on TCP keepalive, which is quite different from the application-level keepalive described above. To better understand the different kinds of keepalives, see this blog post. As that article explains, TCP keepalive depends on networking stack support and might not always work, but it is your only option during query processing. On Linux, you turn keepalives simply by specifying Tcp Keepalive=true in your connection string. The default system-wide settings will be used (for interval, count...) - it is currently impossible to specify these at the application level. On Windows, you can also specify Tcp Keepalive Time and Tcp Keepalive Interval to tweak these settings."
  },
  "doc/conceptual/Npgsql/large-objects.html": {
    "href": "doc/conceptual/Npgsql/large-objects.html",
    "title": "Large Objects | Npgsql Documentation",
    "keywords": "Large Objects The Large Objects feature is a way of storing large files in a PostgreSQL database. Files can normally be stored in bytea columns but there are two downsides; a file can only be 1 GB and the backend buffers the whole file when reading or writing a column, which may use significant amounts of RAM on the backend. With the Large Objects feature, objects are instead stored in a separate system table in smaller chunks and provides a streaming API for the user. Each object is given an integral identifier that is used for accessing the object, that can, for example, be stored in a user's table containing information about this object. Example // Retrieve a Large Object Manager for this connection var manager = new NpgsqlLargeObjectManager(Conn); // Create a new empty file, returning the identifier to later access it uint oid = manager.Create(); // Reading and writing Large Objects requires the use of a transaction using (var transaction = Conn.BeginTransaction()) { // Open the file for reading and writing using (var stream = manager.OpenReadWrite(oid)) { var buf = new byte[] { 1, 2, 3 }; stream.Write(buf, 0, buf.Length); stream.Seek(0, System.IO.SeekOrigin.Begin); var buf2 = new byte[buf.Length]; stream.Read(buf2, 0, buf2.Length); // buf2 now contains 1, 2, 3 } // Save the changes to the object transaction.Commit(); } See also See the PostgreSQL documentation for more information. All functionality are implemented and wrapped in the classes NpgsqlLargeObjectManager and NpgsqlLargeObjectStream using standard .NET Stream as base class."
  },
  "doc/conceptual/Npgsql/performance.html": {
    "href": "doc/conceptual/Npgsql/performance.html",
    "title": "Performance | Npgsql Documentation",
    "keywords": "Performance Diagnostics To be able to improve performance, you first need to be able to see which queries are slow, and generally observe how your application is behaving. PostgreSQL provide some powerful features for knowing what's going on in the database; the statistics collector is a good place to start, and in particular the pg_stat_activity table, which shows which queries are being executed at any given point. Beyond PostgreSQL, Npgsql provides its own set of diagnostics features for tracing, logging and producing metrics. Tracing and metrics are particularly useful for performance analysis - consider collecting this data continuously on your production platform. These features are documented in the dedicated diagnostics page. Prepared Statements One of the most important (and easy) ways to improve your application's performance is to prepare your commands. Even if you're not coding against ADO.NET directly (e.g. using Dapper or an O/RM), Npgsql has an automatic preparation feature which allows you to benefit from the performance gains associated with prepared statements. See this blog post and/or the documentation for more details. Batching/Pipelining When you execute a command, Npgsql executes a roundtrip to the database. If you execute multiple commands (say, inserting 3 rows or performing multiple selects), you're executing multiple roundtrips; each command has to complete before the next command can start execution. Depending on your network latency, this can considerably degrade your application's performance. You can batch multiple SQL statements in a single roundtrip: var batch = new NpgsqlBatch(connection) { BatchCommands = { new(\"SELECT ...\"), new(\"SELECT ...\") } }; await using (var reader = await batch.ExecuteReaderAsync()) { while (await reader.ReadAsync()) { // Read first resultset } await reader.NextResultAsync(); while (await reader.ReadAsync()) { // Read second resultset } } Disable enlisting to TransactionScope By default, Npgsql will enlist to ambient transactions. This occurs when a connection is opened while inside a TransactionScope, and can provide a powerful programming model for working with transactions. However, this involves checking whether an ambient transaction is in progress each time a (pooled) connection is open, an operation that takes more time than you'd think. Scenarios where connections are very short-lived and open/close happens very frequently can benefit from removing this check - simply include Enlist=false in the connection string. Note that you can still enlist manually by calling NpgsqlConnection.Enlist(). Pooled Connection Reset When a pooled connection is closed, Npgsql will arrange for its state to be reset the next time it's used. This prevents leakage of state from one usage cycle of a physical connection to another one. For example, you may change certain PostgreSQL parameters (e.g. statement_timeout), and it's undesirable for this change to persist when the connection is closed. Connection reset happens via the PostgreSQL DISCARD ALL command, or, if there are any prepared statements at the time of closing, by a combination of the equivalent statements described in the docs (to prevent closing those statements). Note that these statements aren't actually sent when closing the connection - they're written into Npgsql's internal write buffer, and will be sent with the first user statement after the connection is reopened. This prevents a costly database roundtrip. If you really want to squeeze every last bit of performance from PostgreSQL, you may disable connect reset by specifying No Reset On Close on your connection string - this will slightly improve performance in scenarios where connection are very short-lived, and especially if prepared statements are in use. Reading Large Values When reading results from PostgreSQL, Npgsql first reads raw binary data from the network into an internal read buffer, and then parses that data as you call methods such as NpgsqlDataReader.GetString(). While this allows for efficient network reads, it's worth thinking about the size of this buffer, which is 8K by default. Under normal usage, Npgsql attempts to read each row into the buffer; if that entire row fits in 8K, you'll have optimal performance. However, if a row is bigger than 8K, Npgsql will allocate an \"oversize buffer\", which will be used until the connection is closed or returned to the pool. If you're not careful, this can create significant memory churn that will slow down your application. To avoid this, if you know you're going to be reading 16k rows, you can specify Read Buffer Size=18000 in your connection string (leaving some margin for protocol overhead), this will ensure that the read buffer is reused and no extra allocation occur. Another option is to pass CommandBehavior.SequentialAccess to NpgsqlCommand.ExecuteReader(). Sequential mode means that Npgsql will no longer read entire rows into its buffer, but will rather fill up the buffer as needed, reading more data only when it's empty. The same 8K read buffer will be used regardless of the row's total size, and Npgsql will take care of the details. In sequential mode, however, you must read the row's fields in the order in which you specified them; you cannot read the 2nd field and then go back to the 1st field, and trying to do so will generate an exception. Similarly, you cannot read the same field twice - once you've read a field, it has been consumed. For more information on CommandBehavior.SequentialAccess, see this page. If you decide to use this feature, be aware that it isn't used as often and may therefore contain bugs. You can also control the socket's receive buffer size (not to be confused with Npgsql's internal buffer) by setting the Socket Receive Buffer Size connection string parameter. Writing Large Values Writing is somewhat similar - Npgsql has an internal write buffer (also 8K by default). When writing your query's SQL and parameters to PostgreSQL, Npgsql always writes \"sequentially\", that is, filling up the 8K buffer and flushing it when full. You can use Write Buffer Size to control the buffer's size. You can also control the socket's send buffer size (not to be confused with Npgsql's internal buffer) by setting the Socket Send Buffer Size connection string parameter. Avoiding boxing when writing parameter values See this section. Unix Domain Socket If you're on Linux or macOS and are connecting to a PostgreSQL server on the same machine, you can boost performance a little by connecting via Unix domain socket rather than via a regular TCP/IP socket. To do this, simply specify the directory of your PostgreSQL sockets in the Host connection string parameter - if this parameter starts with a slash, it will be taken to mean a filesystem path."
  },
  "doc/conceptual/Npgsql/prepare.html": {
    "href": "doc/conceptual/Npgsql/prepare.html",
    "title": "Prepared Statements | Npgsql Documentation",
    "keywords": "Prepared Statements Introduction It's recommended that you start by reading this blog post. Most applications repeat the same SQL statements many times, passing different parameters. In such cases, it's very beneficial to prepare commands - this will send the command's statement(s) to PostgreSQL, which will parse and plan for them. The prepared statements can then be used on execution, saving valuable planning time. The more complex your queries, the more you'll notice the performance gain; but even very simple queries tend to benefit from preparation. Following is a benchmark Npgsql.Benchmarks.Prepare, which measures the execution time of the same query, executed prepared and unprepared. TablesToJoin is a parameter which increases the query complexity - it determines how many tables the query joins from. Method TablesToJoin Mean StdErr StdDev Op/s Scaled Scaled-StdDev Allocated Unprepared 0 67.1964 us 0.1586 us 0.6142 us 14881.75 1.00 0.00 1.9 kB Prepared 0 43.5007 us 0.2466 us 0.9227 us 22988.13 0.65 0.01 305 B Unprepared 1 98.8502 us 0.1278 us 0.4949 us 10116.32 1.00 0.00 1.93 kB Prepared 1 53.7518 us 0.0486 us 0.1818 us 18604.04 0.54 0.00 306 B Unprepared 2 180.0599 us 0.2990 us 1.1579 us 5553.71 1.00 0.00 2.06 kB Prepared 2 70.3609 us 0.1715 us 0.6417 us 14212.44 0.39 0.00 306 B Unprepared 5 1,084.6065 us 1.1822 us 4.2626 us 921.99 1.00 0.00 2.37 kB Prepared 5 110.0652 us 0.1098 us 0.3805 us 9085.52 0.10 0.00 308 B Unprepared 10 23,086.5956 us 37.2072 us 139.2167 us 43.32 1.00 0.00 3.11 kB Prepared 10 197.1392 us 0.3044 us 1.1790 us 5072.56 0.01 0.00 308 B As is immediately apparent, even an extremely simple scenario (TablesToJoin=0, SQL=SELECT 1), preparing the query with PostgreSQL provides a 36% speedup. As query complexity increases by adding join tables, the gap widens dramatically. The only potential disadvantage of prepared statements is that they hold server-side resources (e.g. cached plans). If you're dynamically generating SQL queries, make sure you don't overwhelm the server by preparing too much. Most reasonable applications shouldn't have to worry about this. Simple Preparation To prepare your commands, simply use the following standard ADO.NET code: var cmd = new NpgsqlCommand(...); cmd.Parameters.Add(\"param\", NpgsqlDbType.Integer); await cmd.PrepareAsync(); // Set parameters await cmd.ExecuteNonQueryAsync(); // And so on Note that all parameters must be set before calling Prepare() - they are part of the information transmitted to PostgreSQL and used to effectively plan the statement. You must also set the DbType or NpgsqlDbType on your parameters to unambiguously specify the data type (setting the value isn't support). Note that preparation happens on individual statements, and not on commands, which can contain multiple statements, batching them together. This can be important in cases such as the following: var cmd = new NpgsqlCommand(\"UPDATE foo SET bar=@bar WHERE baz=@baz; UPDATE foo SET bar=@bar WHERE baz=@baz\"); // set parameters. await cmd.PrepareAsync(); Although there are two statements in this command, the same prepared statement is used to execute since the SQL is identical. Persistency With many database drivers, prepared statements are closed when their owning command was disposed. This significantly reduces their usefulness, especially since closing a pooled connection automatically closes all prepared statements. For applications where connections are short-lived - most web applications - this effectively makes prepared statements useless. In Npgsql, all prepared statements are persistent - they don't get closed when a command or connection is closed. Npgsql keeps track of statements prepared on each physical connection; if you prepare the same SQL a second time on the same physical connection, Npgsql will simply reuse the prepared statement from the first preparation. This means that in an application with short-lived, pooled connections, prepared statements will gradually be created as the application warms up and the connections are first used. Then, opening a new pooled connection will return a physical connection that already has a prepared statement for your SQL, providing a very substantial performance boost. For example: await using (var conn = await dataSource.OpenConnectionAsync()) await using (var cmd = new NpgsqlCommand(\"<some_sql>\", conn)) { await cmd.PrepareAsync(); // First time on this physical connection, Npgsql prepares with PostgreSQL await cmd.ExecuteNonQueryAsync(); } await using (var conn = await dataSource.OpenConnectionAsync()) await using (var cmd = new NpgsqlCommand(\"<some_sql>\", conn)) { // We assume the internal connection pool returned the same physical connection used above await cmd.PrepareAsync(); // The connection already has a prepared statement for <some_sql>, this doesn't need to do anything await cmd.ExecuteNonQueryAsync(); } You can still choose to close a prepared statement by calling NpgsqlCommand.Unprepare(). You can also unprepare all statements on a given connection by calling NpgsqlConnection.UnprepareAll(). Automatic Preparation While the preparation examples shown above provide a very significant performance boost, they depend on you calling the Prepare() command. Unfortunately, if you're using some data layer above ADO.NET, such as Dapper or Entity Framework, chances are these layers don't prepare for you. While issues exist for both Dapper and Entity Framework Core, they don't take advantage of prepared statement at the moment. Npgsql supports automatic preparation. When turned on, this will make Npgsql track the statements you execute and automatically prepare them when you reach a certain threshold. When you reach that threshold, the statement is automatically prepared, and from that point on will be executed as prepared, yielding all the performance benefits discussed above. To turn on this feature, you simply need to set the Max Auto Prepare connection string parameter, which determines how many statements can be automatically prepared on the connection at any given time (this parameter defaults to 0, disabling the feature). A second parameter, Auto Prepare Min Usages, determines how many times a statement needs to be executed before it is auto-prepared (defaults to 5). Since no code changes are required, you can simply try setting Max Auto Prepare and running your application to see an immediate speed increase. Note also that, like explicitly-prepared statements, auto-prepared statements are persistent, allowing you to reap the performance benefits in short-lived connection applications. Note that if you're coding directly against Npgsql or ADO.NET, explicitly preparing your commands with Prepare() is still recommended over letting Npgsql prepare automatically. Automatic preparation does incur a slight performance cost compared to explicit preparation, because of the internal LRU cache and various book-keeping data structures. Explicitly preparing also allows you to better control exactly which statements are prepared and which aren't, and ensures your statements will always stay prepared, and never get ejected because of the LRU mechanism. Note that automatic preparation is a complex new feature which should be considered somewhat experimental; test carefully, and if you see any strange behavior or problem try turning it off."
  },
  "doc/conceptual/Npgsql/release-notes/3.0.html": {
    "href": "doc/conceptual/Npgsql/release-notes/3.0.html",
    "title": "Npgsql 3.0 Release Notes | Npgsql Documentation",
    "keywords": "Npgsql 3.0 Release Notes Migrating from 2.2 to 3.0 Version 3.0 represents a near-total rewrite of Npgsql. In addition to changing how Npgsql works internally and communicates with PostgreSQL, a conscious effort was made to better align Npgsql with the ADO.NET specs/standard and with SqlClient where that made sense. This means that you cannot expect to drop 3.0 as a replacement to 2.2 and expect things to work - upgrade cautiously and test extensively before deploying anything to production. The following is a non-exhaustive list of things that changed. If you run against a breaking change not documented here, please let us know and we'll add it. Major Support for .NET 2.0, .NET 3.5 and .NET 4.0 has been dropped - you will have to upgrade to .NET 4.5 to use Npgsql 3.0. We'll continue to do bugfixes on the 2.2 branch for a while on a best-effort basis. The Entity Framework provider packages have been renamed to align with Microsoft's new naming. The new packages are EntityFramework5.Npgsql and EntityFramework6.Npgsql. EntityFramework7.Npgsql is in alpha. A brand-new bulk copy API has been written, using binary encoding for much better performance. See the docs. Composite (custom) types aren't supported yet, but this is a high-priority feature for us. See #441. SSL Npgsql 2.2 didn't perform validation on the server's certificate by default, so self-signed certificate were accepted. The new default is to perform validation. Specify the Trust Server Certificate connection string parameter to get back previous behavior. The \"SSL\" connection string parameter has been removed, use \"SSL Mode\" instead. The \"SSL Mode\" parameter's Allow option has been removed, as it wasn't doing anything. Type Handling Previously, Npgsql allowed writing a NULL by setting NpgsqlParameter.Value to null. This is not allowed in ADO.NET and is no longer supported, set to DBNull.Value instead. In some cases, you will now be required to explicitly set a parameter's type although you didn't have to before (you'll get an error 42804 explaining this). This can happen especially in Dapper custom custom type handlers (#694). Simply set the NpgsqlDbType property on the parameter. Removed support for writing a parameter with an IEnumerable<T> value, since that would require Npgsql to enumerate it multiple times internally. IList<T> and IList are permitted. It is no longer possible to write a .NET enum to an integral PostgreSQL column (e.g. int4). Proper enum support has been added which allows writing to PostgreSQL enum columns (see the docs. To continue writing enums to integral columns as before, simply add an explicit cast to the integral type in your code. NpgsqlMacAddress has been removed and replaced by the standard .NET PhysicalAddress. Npgsql's BitString has been removed and replaced by the standard .NET BitArray. NpgsqlTime has been removed and replaced by the standard .NET TimeSpan. NpgsqlTimeZone has been removed. NpgsqlTimeTZ now holds 2 TimeSpans, rather than an NpgsqlTime and an NpgsqlTimeZone. NpgsqlTimeStamp no longer maps DateTime.{Max,Min}Value to {positive,negative} infinity. Use NpgsqlTimeStamp.Infinity and NpgsqlTimeStamp.MinusInfinity explicitly for that. You can also specify the \"Convert Infinity DateTime\" connection string parameter to retain the old behavior. Renamed NpgsqlInet's addr and mask to Address and Mask. NpgsqlPoint now holds Doubles instead of Singles (#437). NpgsqlDataReader.GetFieldType() and GetProviderSpecificFieldType() now return Array for arrays. Previously they returned int[], even for multidimensional arrays. NpgsqlDataReader.GetDataTypeName() now returns the name of the PostgreSQL type rather than its OID. Retired features Removed the \"Preload Reader\" feature, which loaded the entire resultset into memory. If you require this (inefficient) behavior, read the result into memory outside Npgsql. We plan on working on MARS support, see #462. The \"Use Extended Types\" parameter is no longer needed and isn't supported. To access PostgreSQL values that can't be represented by the standard CLR types, use the standard ADO.NET NpgsqlDataReader.GetProviderSpecificValue or even better, the generic NpgsqlDataReader.GetFieldValue<T>. Removed the feature where Npgsql automatically \"dereferenced\" a resultset of refcursors into multiple resultsets (this was used to emulate returning multiple resultsets from stored procedures). Note that if your function needs to return a single resultset, it should be simply returning a table rather than a cursor (see RETURNS TABLE). See #438. Removed the AlwaysPrepare connection string parameter Removed the Encoding connection string parameter, which was obsolete and unused anyway (UTF8 was always used regardless of what was specified) Removed the Protocol connection string parameter, which was obsolete and unused anyway (protocol 3 was always used) Removed NpgsqlDataReader.LastInsertedOID, it did not allow accessing individual OIDs in multi-statement commands. Replaced with NpgsqlDataReader.Statements, which provides OID and affected row information on a statement-by-statement basis. Removed NpgsqlDataReader.HasOrdinal, was a badly-named non-standard API without a serious use case. GetName() can be used as a workaround. Other It is no longer possible to create database entities (tables, functions) and then use them in the same multi-query command - you must first send a command creating the entity, and only then send commands using it. See #641 for more details. Previously, Npgsql set DateStyle=ISO, lc_monetary=C and extra_float_digits=3 on all connections it created. This is no longer case, if you rely on these parameters you must send them yourself. NpgsqlConnection.Clone() will now only return a new connection with the same connection string as the original. Previous versions returned an open connection if the original was open, and copied the Notice event listeners as well. Note: NpgsqlConnection.Clone() was accidentally missing from 3.0.0 and 3.0.1. Removed the obsolete NpgsqlParameterCollection.Add(name, value) method. Use AddWithValue() instead, which also exists in SqlClient. The savepoint manipulation methods on NpgsqlTransaction have been renamed from Save, and Rollback to CreateSavepoint and RollbackToSavepoint. This broke the naming conventions for these methods across other providers (SqlClient, Oracle...) and so in 3.0.2 the previous names were returned and the new names marked as obsolete. 3.1 will remove the the new names and leaves only Save and Rollback. See #738. The default CommandTimeout has changed from 20 seconds to 30 seconds, as in ADO.NET. CommandType.TableDirect now requires CommandText to contain the name of a table, as per the MSDN docs. Multiple tables (join) aren't supported. CommandType.StoredProcedure now requires CommandText contain only the name of a function, without parentheses or parameter information, as per the MSDN docs. Moved the LastInsertedOID property from NpgsqlCommand to NpgsqlReader, like the standard ADO.NET RecordsAffected. A single SQL statement may no longer have more than 65535 parameters. If you're bulk-importing a large number of rows, consider using the binary COPY API which is considerably more efficient, or an alternative method such as inserting from array parameters. Contributors Thank you very much to the following people who have contributed to the individual 3.0.x. releases. Milestone 3.0.8 Contributor Assigned issues @roji 11 Milestone 3.0.7 Contributor Assigned issues @roji 1 Milestone 3.0.6 Contributor Assigned issues @roji 10 @Emill 1 Milestone 3.0.5 Contributor Assigned issues @roji 4 @kenjiuno 1 Milestone 3.0.4 Contributor Assigned issues @roji 10 @damageboy 2 Milestone 3.0.3 Contributor Assigned issues @roji 14 Milestone 3.0.2 Contributor Assigned issues @roji 8 @Emill 4 Milestone 3.0.1 Contributor Assigned issues @roji 13 @Emill 1 Milestone 3.0.0 Contributor Assigned issues @roji 62 @Emill 3"
  },
  "doc/conceptual/Npgsql/release-notes/3.1.html": {
    "href": "doc/conceptual/Npgsql/release-notes/3.1.html",
    "title": "Npgsql 3.1 Release Notes | Npgsql Documentation",
    "keywords": "Npgsql 3.1 Release Notes Migrating from 3.0 to 3.1 CommandTimeout used to be implemented with PostgreSQL's statement_timeout parameter, but this wasn't a very reliable method and has been removed. CommandTimeout is now implemented via socket timeouts only, see #689 for more details. Note that if a socket timeout occurs, the connection is broken and must be reopened. The Persist Security Info parameter has been implemented and is false by default. This means that once a connection has been opened, you will not be able to get its password. Removed ContinuousProcessing mode, and replaced it with Wait, a simpler and less bug-prone mechanism for consuming asynchronous notifications (#1024). The Maximum Pool Size connection is parameter is now 100 default instead of 20 (this is default in SqlClient, pg_bouncer...). The Connection Lifetime parameter has been renamed to Connection Idle Lifetime, and its default has been changed from 15 to 300. Also, once the number of seconds has elapsed the connection is closed immediately; the previous behavior closed half of the connections. RegisterEnum and RegisterEnumGlobally have been renamed to MapEnum and MapEnumGlobally respectively. If you used enum mapping in 3.0, the strategy for translating between CLR and PostgreSQL type names has changed. In 3.0 Npgsql simply used the CLR name (e.g. SomeField) as the PostgreSQL name; Npgsql 3.1 uses a user-definable name translator, default to snake case (e.g. some_field). See #859. The EnumLabel attribute has been replaced by the PgName attribute (which is also used for the new composite type support). When PostgreSQL sends an error, it is no longer raised by an NpgsqlException but by a PostgresException. PostgresException is a subclass of NpgsqlException so code catching NpgsqlException should still work, but the PostgreSQL-specific exception properties will only be available on PostgresException. The Code property on NpgsqlException has been renamed to SqlState. It has also been moved to PostgresException. NpgsqlNotice has been renamed to PostgresNotice For multistatement commands, PostgreSQL parse errors will now be thrown only when the user calls NextResult() and gets to the problematic statement. It is no longer possible to dispose a prepared statement while a reader is still open. Since disposing a prepared statement includes database interaction, the connection must be idle. Removed NpgsqlConnection.SupportsHexByteFormat. Renamed NpgsqlConnection.Supports_E_StringPrefix to SupportsEStringPrefix. Contributors Thank you very much to the following people who have contributed to the individual 3.1.x. releases. Milestone 3.1.10 Contributor Assigned issues @roji 5 Milestone 3.1.9 Contributor Assigned issues @roji 10 @DaveVdE 1 @rwasef1830 1 Milestone 3.1.8 Contributor Assigned issues @roji 10 Milestone 3.1.7 Contributor Assigned issues @roji 7 Milestone 3.1.6 Contributor Assigned issues @roji 5 Milestone 3.1.5 Contributor Assigned issues @roji 4 Milestone 3.1.4 Contributor Assigned issues @roji 2 Milestone 3.1.3 Contributor Assigned issues @roji 10 Milestone 3.1.2 Contributor Assigned issues @roji 1 Milestone 3.1.10 Contributor Assigned issues @roji 5 Milestone 3.1.1 Contributor Assigned issues @roji 5 Milestone 3.1.0 Contributor Assigned issues @roji 24 @Emill 2"
  },
  "doc/conceptual/Npgsql/release-notes/3.2.html": {
    "href": "doc/conceptual/Npgsql/release-notes/3.2.html",
    "title": "Npgsql 3.2 Release Notes | Npgsql Documentation",
    "keywords": "Npgsql 3.2 Release Notes Npgsql 3.2 is out and available on nuget.org. This is a major release with substantial internal changes and should be deployed with care. For critical applications it may be advisable to wait until 3.2.1 is out. This release contains a large number of new features, but the main focus is performance - some usage scenarios may show dramatic improvements. See below for more details. Major Changes Prepared statements are now persistent (survive beyond pooled connection close/open), providing significant performance improvements for applications with short-lived connections, such as most webapps (#483). Also, statements can optionally be prepared automatically by Npgsql based on use, unlocking prepared statement performance for O/RMs and data layers which don't prepare themselves, such as Dapper or Entity Framework Core (#1237). See this blog post for more info. The internal I/O system has been overhauled to continue supporting sync and async I/O, but with a vastly better coding model. This should eliminate most protocol sync bugs, and make it much easier to maintain and write new type handlers (#1326). Kerberos login (\"integrated security\") is now support on Linux/Mac (#1079). Support for System.Transactions and distributed transactions has been rewritten, and should have fewer problems than before (#122). Performance counters have been implemented, similar to what SqlClient provides. (#619). The Visual Studio integration extension (DDEX) has been rewritten for a much better installation experience, and includes some new features as well (#1407). If your application attempts to make use of more than one connection at the same time, an \"operation already in progress\" was thrown. This exception now provides more information to help you track down the bug (#1248). Many other small changes have been made, especially with regards to performance. Here's the full list. Breaking Changes from 3.1 Connections can no longer be constructed with NpgsqlConnectionStringBuilder - only plain string connection strings are supported (#1415). The Buffer Size connection string parameter has been replaced by Read Buffer Size and Write Buffer Size. Contributors Thank you very much to the following people who have contributed to the individual 3.2.x. releases. Milestone 3.2.7 Contributor Assigned issues @roji 4 @erwaller 1 Milestone 3.2.6 Contributor Assigned issues @roji 8 Milestone 3.2.5 Contributor Assigned issues @roji 4 Milestone 3.2.4.1 Contributor Assigned issues @roji 1 Milestone 3.2.4 Contributor Assigned issues @roji 3 Milestone 3.2.3 Contributor Assigned issues @roji 9 @funny-falcon 1 @jlareo 1 @odiernod 1 Milestone 3.2.2 Contributor Assigned issues @roji 11 @kurtschelfthout 2 @Emill 1 Milestone 3.2.1 Contributor Assigned issues @roji 7 Milestone 3.2 Contributor Assigned issues @roji 33"
  },
  "doc/conceptual/Npgsql/release-notes/4.0.html": {
    "href": "doc/conceptual/Npgsql/release-notes/4.0.html",
    "title": "Npgsql 4.0 Release Notes | Npgsql Documentation",
    "keywords": "Npgsql 4.0 Release Notes Npgsql 4.0 is out and available at nuget.org. This is a major version with significant changes, upgrade with care, consult the breaking changes section below and test well before deploying to production. A special thanks goes out to @YohDeadfall for his many contributions and reviews. Thanks also (alphabetically) to @austindrenski, @Brar, @kspeakman, @rwasef1830, @shortspider, @StillLearnin, @uhayat for their valuable contributions. High performance A concentrated effort has substantially increased Npgsql performance, especially in highly concurrent, low-latency scenarios. Improvements include: Rewriting of the connection pool to be lock-free, since contention started to be an issue in highly concurrent, short-lived connection scenarios (#1839). Significant reduction of allocations through more recycling and other techniques. New API for generically providing parameters, avoiding boxing of value types (#1639). Avoiding numerous internal async calls where they weren't needed. ... many others In round 16 of the TechEmpower benchmark, .NET Core/ASP.NET Core came in 7th place running with Npgsql, making it one of the fastest mainstream web stacks available - see this blog post for more info. Please let us know how the new version works for you - both positive and negative comments are welcome. If you're interested in Npgsql performance and haven't yet seen the performance page, it's a good opportunity to check it out (it's valid also for 3.2 users). Improved spatial support (PostGIS) Previous versions have allowed basic usage of PostGIS's spatial types via built-in Npgsql types, which were limited in many ways. Thanks to a new plugin infrastructure, you can now use the Npgsql.NetTopologySuite plugin, which maps PostGIS types to the NetTopologySuite spatial library's types. NetTopologySuite's types are more complete, and support a variety of spatial operations and conversions you can perform after loading your spatial data from PostgreSQL. If you prefer to use JSON for your spatial types, the Npgsql.GeoJSON plugin maps PostGIS types to GeoJSON.NET types. GeoJSON is a standard JSON format for spatial data. Finally, if you prefer to use the previous Npgsql types (e.g. PostgisPoint), these are available via the Npgsql.LegacyPostgis plugin. Thanks to @YohDeadfall for implementing both the NetTopologySuite and GeoJSON plugins. NodaTime date/time support NodaTime is a powerful alternative to .NET's built-in date/time types, such as DateTime. The built-in types are flawed in many ways: they have problematic support for timezones, don't have a date-only or time-only types, and promote problematic programming but not making the right distinctions. If your application handles dates and times in anything but the most basic way, you should seriously consider using NodaTime. To learn more read this blog post by Jon Skeet. You can now use the new Npgsql.NodaTime to have Npgsql map PostgreSQL date/time types to NodaTime types. Json.NET Another plugin, Npgsql.Json.NET, works with Newtonsoft Json.NET to automatically serialize and deserialize PostgreSQL's jsonb and json types to your objects, providing a seamless database JSON programming experience. Instead of working with strings which you have to serialize and deserialize, Npgsql does it for you. Other improvements Fix the binary COPY API to make it interact better with exceptions (#1646). Npgsql better supports working with enums and composites, even without mapping them, and better supports new types introduced via plugins (#1792). Better \"reflection\" capabilities. Continuing work from 3.2, Npgsql now exposes more information about PostgreSQL types, allowing you to dynamically reflect on columns types returned by queries, or required as parameters (#1276, #1779). Derive parameters for queries. You can now also use NpgsqlCommandBuilder to dynamically understand which parameters and types are required for arbitrary queries (previously supported only for functions) (#1698, thanks @Brar!). Allow reading a single character from a PostgreSQL text column (#1188). Decimals read from PostgreSQL will now have the correct scale (#1925). Thanks @StillLearnin and @YohDeadfall. In addition to more documentation, several blog posts are planned to explain the above in more details (to be announced on @shayrojansky). Breaking changes from 3.2 Caution The date/time behavior has changed in the following ways: DateTime is always sent as timestamp by default, regardless of its kind. You can still specify NpgsqlDbType.TimestampTz, in which case local DateTime gets converted to UTC before sending. When reading timestamptz as a DateTimeOffset, the machine local offset will be used. Previously a DateTimeOffset in UTC was returned. It is no longer possible to read or write DateTimeOffset as timestamp, only as timestamptz. Caution The API for binary import (COPY IN) has changed substantially in a breaking way, and code from 3.2 will not work as-is on 4.0. You must now call NpgsqlBinaryImporter.Complete() to save your imported data; not doing so will roll the operation back. NpgsqlBinaryImporter.Cancel() has been removed - simply closing/disposing the importer will implicitly cancel the import. This is similar to how TransactionScope works and is necessary to prevent accidental commit of data on exception. See #1646. Caution If you're using decimal/numeric numbers (not floating-point), there's a chance your data needs to be fixed (previous versions incorrectly inserted a scale larger than 28, which is the maximum allowed by .NET decimal). If you're having trouble reading data previously inserted by Npgsql, consider running this fixup code. If your data really does contain more than 28/29 fractional digits and you need to keep that precision, see the workarounds proposed in this comment for loading these values. .NET Standard 1.3 is no longer supported. .NET Standard 2.0 is the lowest supported version. Npgsql used to use its own internal TLS/SSL due to issues with some server. As these issues have been resolved, the standard .NET SslStream is now used by default (#1482), but you can still set Use SSL Stream=false to keep using the internal implementation (please report why you need this, as it's likely the internal implementation will be removed in a future release). The reader instances returned by NpgsqlCommand.ExecuteReader() are now recycled, to reduce memory allocations (#1649). You should not keep a reference or interact with a reader after its command has been disposed (such interaction was limited in any case). The Min Pool Size parameter will no longer make the pool create new connections internally - it will only have an effect on how many connections are pruned. Previously, in various points the pool would check if the current number of connections was below Min Pool Size, and if so, automatically created new ones - this no longer happens. Parameter types have become more strict. Previous versions allowed to you pass arbitrary value types, such as writing CLR string to int columns, or anything that implemented IConvertible. Although some implicit conversions are still supported (e.g. long -> int, short -> int), some have been removed. Data type names returned from NpgsqlDataReader.GetDataTypeName() and other APIs are now more standards-conforming (e.g. integer[] instead of _int4), and properly include type modifiers (e.g. character varying(10)) (#1919). NpgsqlParameter.EnumType and NpgsqlParameter.SpecificType have been removed. See Composites and Enums for more details. Parameter names are no longer trimmed, set your names to the exact parameter name specified in your SQL. If a parameter's name isn't set, it will no longer default to Parameter1, Parameter2, etc. The following APIs \"connection capability\" APIs have been removed from NpgsqlConnection: UseConformantStrings, SupportsEStringPrefix, UseSslStream. The default name translator, NpgsqlSnakeCaseNameTranslator, has been changed to handle acronyms better. Given the property name IsJSON, the old translator algorithm would output is_j_s_o_n, while the new outputs is_json. To revert back to the old algorithm, create a NpgsqlSnakeCaseNameTranslator instance with legacyMode: true and pass it when calling the MapComposite and MapEnum methods. If you are reading tables as composites (#990), you will have to add the new Load Table Composites to your connection string. NpgsqlConnection.GetSchema() will no longer return system tables (i.e. tables in schemas pg_catalog and information_schema), #1831. You may no longer have multiple streams or text readers open on a reader (this was previously supported with non-sequential readers). Accessing a new column closes any open stream or text reader. The DateTimeOffset instances returned for PostgreSQL timetz now have their date set to 0001-01-02 instead of the previous 0001-01-01 (#1924). Contributors Thank you very much to the following people who have contributed to the individual 4.0.x. releases. Milestone 4.0.11 Contributor Assigned issues @manandre 1 @roji 1 @YohDeadfall 1 Milestone 4.0.10 Contributor Assigned issues @kYann 1 @roji 1 Milestone 4.0.9 Contributor Assigned issues @roji 2 @YohDeadfall 1 Milestone 4.0.8 Contributor Assigned issues @roji 2 @romanov-is-here 1 @thetranman 1 @YohDeadfall 1 Milestone 4.0.7 Contributor Assigned issues @roji 4 @aspaw 1 Milestone 4.0.6 Contributor Assigned issues @roji 2 @austindrenski 1 @zabulus 1 Milestone 4.0.5 Contributor Assigned issues @roji 6 @YohDeadfall 5 @austindrenski 1 Milestone 4.0.4 Contributor Assigned issues @roji 6 @YohDeadfall 3 @austindrenski 1 Milestone 4.0.3 Contributor Assigned issues @roji 6 @YohDeadfall 3 Milestone 4.0.2 Contributor Assigned issues @roji 2 @YohDeadfall 1 Milestone 4.0.11 Contributor Assigned issues @manandre 1 @roji 1 @YohDeadfall 1 Milestone 4.0.10 Contributor Assigned issues @kYann 1 @roji 1 Milestone 4.0.1 Contributor Assigned issues @roji 3 @austindrenski 2 @YohDeadfall 2 Milestone 4.0 Contributor Assigned issues @roji 34 @YohDeadfall 6 @Brar 1 @funny-falcon 1"
  },
  "doc/conceptual/Npgsql/release-notes/4.1.html": {
    "href": "doc/conceptual/Npgsql/release-notes/4.1.html",
    "title": "Npgsql 4.1 Release Notes | Npgsql Documentation",
    "keywords": "Npgsql 4.1 Release Notes Npgsql 4.1 is out and available at nuget.org. New Features The major new features of 4.1 are: Support for reading and writing your types as JSON via the new, high-perf System.Text.Json API (#2306). Support for the new async methods introduced in .NET Standard 2.1 (#2481). Expose performance statistics via the new .NET event counters (#1725). Async support for binary imports and exports (#1632). Easier and PostgreSQL standard ways to provide client certificates for authentication (#2129). Many other small improvements and performance optimizations have been introduced as well - you can track progress here. Breaking changes from 4.0 .NET Framework 4.5, 4.5.1 and 4.5.2 are no longer supported. .NET Framework 4.6.1 and .NET Standard 2.0 are the lowest supported versions. The spatial plugin, Npgsql.NetTopologySuite, has been updated to depend on NetTopologySuite 2.0.0, which is a major version introducing breaking changes. Specifically, EF Core 3.0 is the first version supporting NetTopologySuite 2.0.0; it is not possible to use EF Core 2.x with the new version of Npgsql.NetTopologySuite. The UseSslStream property of NpgsqlConnectionStringBuilder is now marked as Obselete. SslStream is always used. A fix was done to Npgsql's snake case naming converter (#2152); this could break code that relies on the previous conversion logic. Contributors Thank you very much to the following people who have contributed to the individual 4.1.x. releases. Milestone 4.1.12 Contributor Assigned issues @roji 1 Milestone 4.1.9 Contributor Assigned issues @YohDeadfall 1 Milestone 4.1.8 Contributor Assigned issues @YohDeadfall 1 Milestone 4.1.7 Contributor Assigned issues @YohDeadfall 3 @vonzshik 1 Milestone 4.1.6 Contributor Assigned issues @loop-evgeny 1 @mm3141 1 @roji 1 @vonzshik 1 @YohDeadfall 1 Milestone 4.1.5 Contributor Assigned issues @calexander3 1 @roji 1 @romanov-is-here 1 @vonzshik 1 @warcha 1 @YohDeadfall 1 Milestone 4.1.4 Contributor Assigned issues @roji 6 @YohDeadfall 6 @warcha 3 @dwat001 1 @elipatov 1 @manandre 1 @williamdenton 1 Milestone 4.1.3 Contributor Assigned issues @roji 5 @YohDeadfall 4 @NinoFloris 1 @nycdotnet 1 Milestone 4.1.2 Contributor Assigned issues @YohDeadfall 4 @NinoFloris 2 @austindrenski 1 Milestone 4.1.1 Contributor Assigned issues @roji 7 Milestone 4.1.0 Contributor Assigned issues @roji 27 @YohDeadfall 6 @austindrenski 2 @NinoFloris 2 @afkos 1 @baronfel 1 @shortspider 1 @williamdenton 1 @zabulus 1"
  },
  "doc/conceptual/Npgsql/release-notes/5.0.html": {
    "href": "doc/conceptual/Npgsql/release-notes/5.0.html",
    "title": "Npgsql 5.0 Release Notes | Npgsql Documentation",
    "keywords": "Npgsql 5.0 Release Notes Npgsql 5.0 is out and available at nuget.org. New features The full list of issues for this release is available here. PostgreSQL logical and physical replication PostgreSQL replication creating programs which receive a continuous, live stream of all updates happening in a PostgreSQL database or set of tables. It can be used for auditing purposes, continuously exporting certain (or all) changes to another database or format, or various other purposes. See the documentation for more details. This major feature was developed by @Brar, many thanks! Issue: #1520 Improved support for cancellation and timeout When the user requests cancellation (via a cancellation token or <xref:Npgsql.NpgsqlCommand.Cancel?displayProperty=nameWithType>), or when the Command Timeout expires, Npgsql now takes the following steps: Contact PostgreSQL and attempt to cancel the running command. If successful, PostgreSQL immediately stops processing the command and is free to do other work. In previous versions, queries were sometimes left running on the server. If PostgreSQL cancellation isn't successful within a short time window, the network is likely down. Npgsql forcibly closes the physical connection and raises an exception, making sure the application isn't blocked. Previous support was inconsistent across cancellation and timeout, and async I/O was only partially supported. Thanks to @vonzshik for all their valuable work on this! Issue: #3166 Connection multiplexing Multiplexing is an experimental new way for the driver to handle commands and connections: user code no longer receives an exclusive physical connection from Npgsql's internal pool, but rather submits commands to be executed on any available connection. This is a much more efficient use of connections, which can lead to: Significant reduction to the number of required PostgreSQL physical connection requirements, and removing load from the server. Continued uptime even when the maximum allowed connection count has been reached. A potentially substantial performance boost on the TechEmpower benchmark (next round to be published). Multiplexing is disabled by default, is considered experimental and must be explicitly opted into. Full documentation will be available soon. Issue: #1982 Nullable references All Npgsql APIs are now fully annotated for C# nullable references (#3120), in line with the new annotations in System.Data released in .NET 5.0. Support for arrays of nullable value types It is now possible to read PostgreSQL arrays as CLR arrays of nullable value types. For example, a PostgreSQL integer[] column can now be read as follows: var array = reader.GetFieldValue<int?>(0); Previously, only non-nullable value arrays were supported, and nulls were translated to the default value (e.g. 0). Thanks to @Brar for developing this feature. Issue: #443 Close connections past a certain age The Connection Lifetime connection string parameter can now be used to unconditionally close connections which have reached a certain age. Once such a connection is returned to the pool, it is destroyed and a new connection will be created if needed. This is useful in clustered configurations to force load balancing between a running server and a server just brought online, or to mitigate certain resource leaks. Thanks to @FlorianRainer for collaborating on this feature. Issue: #1810 Breaking changes Npgsql no longer targets .NET Framework 4.6.1. Since .NET Standard 2.0 is targeted, it is still possible to use Npgsql from .NET Framework applications; however, we no longer run regression tests on .NET Framework and will only fix bugs on a best-effort basis. In addition, the Visual Studio extension (VSIX) and the MSI GAC installer have been discontinued. #3269. Npgsql targets the netcoreapp3.1 TFM instead of netcoreapp3.0, which is out of support (#3160). When command timeout occurs, Npgsql now raises an <xref:Npgsql.NpgsqlException> wrapping a TimeoutException (#3132). Previously, an NpgsqlException wrapping an IOException was raised. Similarly, when cancellation occurs, Npgsql now raises an OperationCanceledException when a command is cancelled (both via an async cancellation token, and via <xref:Npgsql.NpgsqlCommand.Cancel?displayProperty=nameWithType>). Previously, NpgsqlCommand.Cancel caused a <xref:Npgsql.PostgresException> to be raised instead. When reading PostgreSQL arrays, if a null value is being read into a CLR arrays of a non-nullable value type, an exception is now thrown (#443). Previously, the default value was populated instead. It is now possible to read arrays of nullable value types instead. The NpgsqlTransaction.IsCompleted property has been removed. The application must itself track when a transaction is committed or rolled back. Support for unmapped PostgreSQL composite types has been removed (#2403). The ordering in which Npgsql checks alternative password sources has changed (#2695, #2657). This only affects applications which omit the password from the connection string. If the connection string specifies a PGPASS file, Npgsql now throws if that file doesn't exist (#2694). The TcpKeepAliveTime and TcpKeepAliveInterval connection string parameters are now expressed in seconds instead of milliseconds (#1936). The <xref:Npgsql.NpgsqlConnection.GetSchema?displayProperty=nameWithType> API now returns all indexes in the database (2958). Previously, only indexes in the search_path were returned. <xref:Npgsql.NpgsqlOperationInProgressException> now inherits from <xref:Npgsql.NpgsqlException>. <xref:Npgsql.NpgsqlBinaryImporter.Complete?displayProperty=nameWithType> has been changed to return a ulong reporting the number of imported rows (#2112). The Npgsql.RawPostgis plugin has been discontinued (#3201). Npgsql now allows any field to be read as raw binary data, so the plugin is no longer necessary. ~Support for non-standard-conforming strings has been dropped (#2931).~ Support for non-standard-confirming strings has been restored in 5.0.1 (except for multiplexing). Npgsql no longer supports SASL authentication with PgBouncer below 1.12. Contributors Thank you very much to the following people who have contributed to the individual 5.0.x. releases. Milestone 5.0.14 Contributor Assigned issues @vonzshik 1 Milestone 5.0.11 Contributor Assigned issues @vonzshik 4 @aromaa 1 @kislovs 1 @roji 1 Milestone 5.0.10 Contributor Assigned issues @vonzshik 11 @roji 5 @Brar 1 Milestone 5.0.7 Contributor Assigned issues @vonzshik 8 @roji 4 @Brar 1 @chrisdcmoore 1 Milestone 5.0.5 Contributor Assigned issues @vonzshik 6 @roji 3 Milestone 5.0.4 Contributor Assigned issues @vonzshik 5 @roji 2 @Brar 1 @neyromant 1 @YohDeadfall 1 Milestone 5.0.3 Contributor Assigned issues @baal2000 1 @roji 1 @vonzshik 1 Milestone 5.0.2 Contributor Assigned issues @vonzshik 4 @roji 2 @Brar 1 @cime 1 @YohDeadfall 1 Milestone 5.0.1.1 Contributor Assigned issues @vonzshik 3 @roji 2 @YohDeadfall 1 Milestone 5.0.0 Contributor Assigned issues @roji 34 @vonzshik 19 @YohDeadfall 16 @Brar 7 @warcha 5 @manandre 2 @ch-asimakopoulos 1 @chrisdcmoore 1 @FlorianRainer 1 @NinoFloris 1 @russellfoster 1 @TwentyFourMinutes 1"
  },
  "doc/conceptual/Npgsql/release-notes/6.0.html": {
    "href": "doc/conceptual/Npgsql/release-notes/6.0.html",
    "title": "Npgsql 6.0 Release Notes | Npgsql Documentation",
    "keywords": "Npgsql 6.0 Release Notes Npgsql version 6.0 has been released and is available on nuget. Npgsql 6.0 brings some major breaking changes and is not a simple in-place upgrade. Carefully read the breaking change notes below and upgrade with care. New features The full list of issues for this release is available here. Multiple hosts, load balancing and failover Npgsql now includes first-class support for managing connections to multiple PostgreSQL servers, unlocking both load balancing for better performance and failover for better reliability (#732). This long-awaited feature is an important part of using PostgreSQL in scalable and mission-critical environments. For example, consider the following connection string: Host=server1,server2,server3,server4,server5;Username=test;Password=test;Load Balance Hosts=true;Target Session Attributes=prefer-standby This will make Npgsql load balance connections across 5 different servers, in round-robbin fashion; connections to read-only standby servers is preferred in order to offload as much read-only load from the primary server, but if only the primary server is up it will still be used. The multiple hosts feature is highly configurable, see the full documentation for more details. Tracing with OpenTelemetry Npgsql can now report tracing data via OpenTelemetry; this can provide invaluable data on the queries your application is running, how long they're taking, and situate them in the larger context of your application. For example, you can visualize what percentage of your HTTP requests are spent waiting for your database query. The following is a sample timeline visualization of Npgsql-reported trace data, using Zipkin: See the full documentation in the diagnostics page. Timestamp rationalization and improvements Support for timestamp with time zone and timestamp without time zone has been rationalized and simplified, and aligned with PostgreSQL best practices. In particular, the \"UTC everywhere\" pattern is much better supported via the PostgreSQL timestamp with time zone type, which is the recommended way to handle timestamps. A detailed explanation is available in this blog post, below is a summary of the main improvements. UTC timestamps have been cleanly separated from non-UTC timestamps, aligning with the PostgreSQL types. The former are represented by timestamp with time zone and DateTime with Kind UTC, the latter by timestamp without time zone and DateTime with Kind Local or Unspecified. It is recommended to use UTC timestamps where possible. Npgsql no longer performs any implicit timezone conversions when reading or writing any timestamp value - the value in the database is what you get, and the machine timezone no longer plays any role when reading/writing values. Npgsql no longer supports date/time representations which cannot be fully round-tripped to the database. If it can't be fully stored as-is, you can't write it. A compatibility switch enables opting out of the new behavior, to maintain backwards compatibility. This change introduces significant breaking changes (see below), although a compatibility flag can be used to opt out and revert to the previous behavior. Other date/time improvements include: Support for the new .NET DateOnly and TimeOnly types (#3616). PostgreSQL tstzrange is now mapped to NodaTime Interval, and PostgreSQL daterange is now mapped to NodaTime DateInterval. Most methods on these types are translated (#3973, #4070). DateTime.MinValue and MaxValue are now mapped to PostgreSQL -infinity and infinity by default. Raw SQL mode and new batching API It is now possible to use PostgreSQL positional parameters, allowing Npgsql to skip rewriting your SQL and send it directly to PostgreSQL (#1042): SELECT * FROM blogs WHERE b.name = @p; -- Before Npgsql 6.0 SELECT * FROM blogs WHERE b.name = $1; -- Npgsql 6.0 This is more efficient, is safer (since Npgsql doesn't have to parse SQL), and aligns your SQL with the actual PostgreSQL SQL. Note that named parameters are still supported for backwards compatibility. In addition, Npgsql now supports the new ADO.NET batching API introduced in .NET 6.0 as an alternative to packing multiple statements into a single NpgsqlCommand, delimited by semicolons (#3860): // Before Npgsql 6.0 var command = new NpgsqlCommand(\"SELECT 1; SELECT 2\", connection); var reader = await command.ExecuteReaderAsync(); // Npgsql 6.0 var batch = new NpgsqlBatch(connection) { BatchCommands = { new(\"SELECT 1\"), new(\"SELECT 2\") } }; var reader = await batch.ExecuteReaderAsync(); As with positional parameters, this allow Npgsql to avoid parsing and splitting your SQL which is safer and more efficient. To learn more about positional parameters and batching, see this blog post. SSL/TLS improvements Npgsql 6.0 contains some considerable improvements to encryption support, here are some highlights (#4006): The Ssl Mode connection string parameter has been aligned with the standard PostgreSQL ssl_mode setting, and allows for more fine-grained choice in what to validate (see breaking change note below for SSL Mode=Require). Support has been added for PEM client certificates when running on .NET 5.0 and above. Support for specifying a root CA certificate that isn't installed in the machine store. See the updated security and encryption docs for more details. Improved logical replication support The recently-released PostgreSQL 14 made significant improvements to logical replication, and Npgsql 6.0 already supports them! Here's a summary of the main changes (#4050, thanks @Brar): PG14 binary logical replication - you can now read the standard .NET types from replication data; only text was possible previously (#4049). PG14 streaming replication - incremental updates for large in-progress transactions (#4047) The replication API now fully streams all data, including even columns; the previous API buffered rows, leading to lots of heap allocations for big rows (#4068). Unfortunately, some of the above required an API redesign, leading to some breaking changes. Other new features Support for the new .NET DateOnly and TimeOnly types (#3616). Full support for the PostgreSQL 14 multirange type, mapped to arrays or lists of NpgsqlRange<T> (#3868). PostgreSQL decimal can now be mapped to .NET BigInteger (#3665). Bulk import/export now includes a fully asynchronous API (#3309). Improved array nullability via a new connection string parameter (#3386). Composite types and records can now be read by getting a nested DbDataReader through NpgsqlDataReader.GetData() (#3558, thanks @Emill). This allows for efficient access without having to map composite types to .NET POCOs. Considerable work has gone in to make Npgsql more friendly to trimming/AOT, removing reflection (#3300). Npgsql still isn't fully trimming-friendly, but it's getting close. Breaking changes Changes to SSL configuration (SSL Mode=Require) To validate server certificates, use SSL Mode=VerifyFull (or alternatively SSL VerifyCA) instead of Require. To not validate server certificates (e.g. self-signed certificates), use SSL Mode=Require and Trust Server Certificate=true. In previous versions, specifying SSL Mode=Require made Npgsql validate the server certificate, but the standard PostgreSQL ssl_mode setting does not. As part of aligning Npgsql with other PostgreSQL drivers and tools, 6.0 now has SSL Mode=VerifyFull which corresponds to the previous Require behavior. Stopping validation for Require would align Npgsql with the PostgreSQL behavior, but would silently turn off validation for current users and expose them to potential security issues. As a result, we now require Trust Server Certificate=true to be specified with Require: this forces users to explicitly opt out of validation. In a future version, we'll remove the requirement to specify Trust Server Certificate for Require (and possibly the parameter itself). Major changes to timestamp mapping Note It is possible to opt out of these changes to maintain backwards compatibility, see below. Quick summary In many cases, is makes sense to store UTC timestamps in the database. To do this, migrate your timestamp without time zone columns to timestamp with time zone (see migration notes below), and always use either DateTime with Kind=Utc or DateTimeOffset with offset 0. If using NodaTime (recommended), use either Instant or ZonedDateTime with time zone UTC. To store non-UTC timestamps, use DateTime with Kind=Unspecified. If using NodaTime (recommended), use LocalDateTime (no explicit column configuration is required). If you're using Dapper, use version 2.0.123 or above. Earlier versions will fail when trying to send a UTC DateTime. Detailed notes The below notes will use the PostgreSQL aliases timestamptz to refer to timestamp with time zone, and timestamp to refer to timestamp without time zone. Note that timestamp with time zone represents a UTC timestamp and does not store a timezone in the database. UTC DateTime is now strictly mapped to timestamptz, while Local/Unspecified DateTime is now strictly mapped to timestamp. DateTime with Kind=UTC are now written as PostgreSQL timestamptz; previously, DateTime was always written as timestamp. It is no longer possible to write UTC DateTime as timestamp, or Local/Unspecified DateTime as timestamptz. This was previously allowed, with Npgsql performing implicit timezone conversions. Note that if you write a UTC DateTime to a PostgreSQL timestamp column, PostgreSQL will implicitly convert the timestamptz value sent by Npgsql, performing a timezone conversion based on the TimeZone parameter. timestamptz values are now read back as DateTime with Kind=UTC, without any conversions; these were previously returned as local DateTime, converted to the local machine's timezone. When reading timestamptz values as DateTimeOffset, UTC values (offset 0) are always returned. DbType.DateTime now maps to timestamptz, not timestamp. DbType.DateTime2 continues to map to timestamp, and DbType.DateTimeOffset continues to map to timestamptz, as before. Unless you're writing cross-database applications, consider using NpgsqlDbType instead of DbType to specify precise PostgreSQL types, or simply let Npgsql infer the types by not setting either. It is no longer possible to write DateTimeOffset with offsets other than 0 (UTC), since these cannot be represented in PostgreSQL. These were previously implicitly converted to UTC before sending. It is no longer possible to read or write timetz as DateTime or TimeSpan, as these don't have a timezone. This was previously allowed, with the offset being stripped. See the EF Core provider docs for additional changes at the EF level. NodaTime changes Instant is now sent as a timestamptz value, and not as a timestamp, since they represent a universally agreed-upon point in time. To send a timestamp, use LocalDateTime. When reading timestamptz as ZonedDateTime or OffsetDateTime, UTC values are always returned. Previously, local values based on the PostgreSQL TimeZone parameter were returned. Note In most cases, storing UTC timestamps is the recommended practice. If this is what you're doing, it's strongly recommended to migrate all relevant columns from timestamp columns to timestamptz. See below for how to do this. Migrating columns from timestamp to timestamptz Migrating timestamp columns to timestamptz is a simple procedure, but care must be taken, depending on the current contents of your column. As a starting point, let's assume your existing timestamp column has the timestamp 2020-01-01 12:00:00: SELECT \"created_on\", pg_typeof(\"created_on\") AS type FROM \"Blogs\"; Results in: created_on | type ---------------------+----------------------------- 2020-01-01 12:00:00 | timestamp without time zone The following SQL will change the column's type to timestamptz: ALTER TABLE blogs ALTER COLUMN created_on TYPE timestamp with time zone; When converting the timestamp without time zone column to timestamp with time zone, PostgreSQL will assume that existing values are local timestamps, and will convert them to UTC based on the TimeZone parameter. Performing the above query will result in something like: CreatedOn | type ------------------------+-------------------------- 2020-01-01 12:00:00+02 | timestamp with time zone This means that your new timestamptz column now contains 10:00 UTC, which is probably not what you want: if the original values were in fact UTC values, you need them to be preserved as-is, changing only the column type. To do this, set TimeZone to UTC before executing the ALTER TABLE: SET TimeZone='UTC'; ALTER TABLE blogs ALTER COLUMN created_on TYPE timestamp with time zone; This will ensure that no time zone conversions will be applied when converting the columns: CreatedOn | type ------------------------+-------------------------- 2020-01-01 14:00:00+02 | timestamp with time zone Opting out of the new timestamp mapping logic The changes described above are far-reaching, and may break applications in various ways. You can upgrade to version 6.0 but opt out of the new mapping by enabling the Npgsql.EnableLegacyTimestampBehavior AppContext switch. To do this and revert to the legacy timestamp behavior, add the following at the start of your application, before any Npgsql operations are invoked: AppContext.SetSwitch(\"Npgsql.EnableLegacyTimestampBehavior\", true); NodaTime: tstzrange and daterange are mapped to Interval and DateInterval by default When using NodaTime, reading a PostgreSQL tstzrange returns Interval instead of NpgsqlRange<Instant> (#4070), and reading a PostgreSQL daterange returns DateInterval instead of NpgsqlRange<LocalDate> (#3973). It is still possible to read NpgsqlRange by via reader.GetFieldValue<NpgsqlRange<Instant>> and reader.GetFieldValue<NpgsqlRange<LocalDate>>. Date/time min/max values are now converted to PostgreSQL infinity values by default PostgreSQL has special infinity and -infinity values for timestamps and dates, which are later and earlier than other value. Npgsql has supported mapping DateTime.MaxValue and MinValue to these infinity values via an Convert Infinity DateTime connection string parameter, which was disabled by default. This behavior is now on by default, since DateTime.MaxValue and MinValue are very rarely used as actual timestamps/dates, and the Convert Infinity DateTime parameter has been removed. To disable infinity conversions, add the following at the start of your application: AppContext.SetSwitch(\"Npgsql.DisableDateTimeInfinityConversions\", true); See the date/time documentation for more details. PG intervals with months/years can no longer be read as TimeSpan The PostgreSQL interval type can contain months and years, which are time units without a fixed, absolute duration (different months have different numbers of days). In contrast, .NET TimeSpan is always an absolute duration, and does not support months or years. Previously, when reading intervals as TimeSpan, Npgsql read months as 30-day units, which was incorrect and could yield wrong results. To read intervals with month/year components, consider using NodaTime's Period type, which is perfectly suited for representing intervals and is supported by Npgsql. Otherwise, you can read intervals as <xref:NpgsqlTypes.NpgsqlInterval>, provides direct, raw access to the PostgreSQL data which Npgsql receives. Finally, if you want to continue treating months as 30-day units, consider changing your interval data in the database, so that e.g. '1 month 20 days' becomes '50 days'. NpgsqlStatement and PostgresException.Statement have been removed Npgsql versions before 6.0 exposed an NpgsqlStatement type, which contained information on individual statements within an NpgsqlCommand batch. Npgsql introduces support for the new standardized ADO.NET DbBatch type, which has numerous advantages compared to the previous support. While executing multiple statements in a single NpgsqlCommand is still supported for backwards compatibility, NpgsqlCommand no longer exposes per-statement information via NpgsqlStatement as before. Consider using the new NpgsqlBatch for all batched executions instead: it exposes the BatchCommands property which is similar to the previous Statements property. Accordingly, PostgresException.Statement has been removed, and replaced by BatchCommand which is only populated when executing via NpgsqlBatch. The default SSL Mode is now Prefer When SSL Mode isn't specified on the connection string, it used to default to Disable; it is now Prefer. This means that SSL will be used when the server supports it, and if a custom certificate validation callback is set, failure to validate would cause the connection to fail. See the Security and encryption page for more details. The logical replication API has been redone As part of the redesign around logical replication support, some major changes to the API had to be done. Applications will have to be changed to use the new API. Arrays/lists over ranges are mapped to PG14 multiranges PostgreSQL 14 introduced a new multirange type, which is very similar to an array of ranges but supports various range-related operations efficiently. The provider now maps arrays and lists of NpgsqlRange to these new types by default. You can still read old-style arrays over ranges by explicitly specifying the type: var arrayOverRange = reader.GetFieldValue<NpgsqlRange<int>[]>(0); You can also write arrays over ranges by explicitly specifying the NpgsqlDbType: var parameter = new NpgsqlParameter { Value = new NpgsqlRange<int>[] { ... }, NpgsqlDbType = NpgsqlDbType.IntegerRange | NpgsqlDbType.Array }; DBNull no longer permitted in arrays Previously, it was possible to write an object[] containing DBNull as a way of indicating null; this is no longer supported. Instead, use C# null. Parameter name matching changed around case-sensitivity When using named parameter placeholders, Npgsql now only does a single pass to match placeholders to parameters, taking the first parameter whose name passes a case-insensitive match. Npgsql previously did two passes - a case-insensitive pass, followed by a case-sensitive one if the first pass failed to match; this was detrimental to performance in some scenarios. This behavioral change is only visible when using two parameters with names that are identical except for case; this is generally discouraged. To revert to the legacy two-pass behavior, add the following at the start of your application, before any Npgsql operations are invoked: AppContext.SetSwitch(\"Npgsql.EnableLegacyCaseInsensitiveDbParameters\", true); Note: version 6.0.0 and 6.0.1 changed parameter matching to be case-sensitive. This change was rolled back in 6.0.2 since it could lead to data loss in some scenarios. The provider-specific date/time types have been obsoleted Npgsql contains provider-specific NpgsqlDateTime, NpgsqlDate and NpgsqlTimeSpan types, which were designed to provide the same APIs as the corresponding built-in BCL types, but to support the full range of the PostgreSQL types. These types were buggy and inefficient in many ways, and have been obsoleted; they will be removed in Npgsql 7.0. Instead of the obsoleted types, use the following techniques: NodaTime can be used to interact with values which are out-of-range for the BCL types. To support values which are out-of-range for NodaTime, PostgreSQL timestamps can now be read/written as long, and dates can be read/written as int. These are the raw PostgreSQL representations, with no operations - they simply provide an \"escape hatch\" in case users need to interact with out-of-range values. For interval, a new NpgsqlInterval type has been introduced, which again contains the raw PostgreSQL data (months, days, time). Npgsql.LegacyPostgis has been removed The Npgsql.LegacyPostgis plugin, which mapped legacy Npgsql types to PostGIS, has been removed and no longer ships (#3962). Use Npgsql.NetTopologySuite to work with PostGIS. Contributors Thank you very much to the following people who have contributed to the individual 6.0.x. releases. Milestone 6.0.8 Contributor Assigned issues @roji 1 @vonzshik 1 Milestone 6.0.7 Contributor Assigned issues @vonzshik 4 @roji 2 @Brar 1 Milestone 6.0.6 Contributor Assigned issues @vonzshik 3 Milestone 6.0.5 Contributor Assigned issues @roji 4 @alitas 1 @davidhunt135 1 @vonzshik 1 Milestone 6.0.4 Contributor Assigned issues @roji 13 @vonzshik 10 Milestone 6.0.3 Contributor Assigned issues @roji 8 @vonzshik 3 @Brar 2 @NinoFloris 2 Milestone 6.0.2 Contributor Assigned issues @roji 3 @NinoFloris 1 Milestone 6.0.1 Contributor Assigned issues @vonzshik 5 @roji 4 @NinoFloris 1 @zitmen 1 Milestone 6.0.0 Contributor Assigned issues @roji 42 @vonzshik 34 @Brar 7 @NinoFloris 3 @Emill 2 @manandre 2 @mdalepiane 1 @mintsoft 1 @Seltzer 1 @TwentyFourMinutes 1"
  },
  "doc/conceptual/Npgsql/release-notes/7.0.html": {
    "href": "doc/conceptual/Npgsql/release-notes/7.0.html",
    "title": "Npgsql 7.0 Release Notes | Npgsql Documentation",
    "keywords": "Npgsql 7.0 Release Notes Npgsql version 7.0 has been released and is available on nuget. New features The full list of issues for this release is available here. DbDataSource A major improvement in Npgsql 7.0 is <xref:Npgsql.NpgsqlDataSource>, which implements the new DbDataSource abstraction in .NET System.Data. A data source represents your PostgreSQL database, and can hand out connections to it or support direct execution of SQL to it. Instead of directly instantiating an NpgsqlConnection and then executing commands against it, you now create a data source once, and then use that throughout your application: await using var dataSource = NpgsqlDataSource.Create(connectionString); // Execute a command directly against the data source, no NpgsqlConnection needed: await using var command = dataSource.CreateCommand(\"INSERT INTO some_table (some_field) VALUES (8)\"); await command.ExecuteNonQueryAsync(); // Open a connection in order to e.g. start a transaction on it: await using var connection = await dataSource.OpenConnectionAsync(); Since the data source encapsulates all the necessary configuration for connecting to a database (e.g. the connection string, authentication callbacks...), it can be registered in dependency injection or passed around as needed, without needing any additional information. The new <xref:Npgsql.NpgsqlDataSourceBuilder> also provides the ideal API point for various configuration when building a data source: var dataSourceBuilder = new NpgsqlDataSourceBuilder(\"Host=localhost;Username=test;Password=test\"); dataSourceBuilder .UseLoggerFactory(loggerFactory) // Configure logging .UsePeriodicPasswordProvider() // Automatically rotate the password periodically .UseNodaTime(); // Use NodaTime for date/time types await using var dataSource = dataSourceBuilder.Build(); If you're using dependency injection (e.g. in an ASP.NET application), see the Npgsql.DependencyInjection package for an easy way to set up your data source. Improved logging with Microsoft.Extensions.Logging Previous versions had a custom logging implementation which required special adapters and was hard to use. Npgsql 7.0 fully supports the standard .NET Microsoft.Extensions.Logging - just provide Npgsql with your ILoggerFactory and you're ready to go. If you using ASP.NET, things are even easier with the new Npgsql.DependencyInjection, which takes care of seamlessly picking up the ASP.NET logging configuration from DI: var builder = WebApplication.CreateBuilder(args); builder.Logging.AddConsole(); builder.Services.AddNpgsqlDataSource(\"Host=localhost;Username=test;Password=test\"); For more details, see the updated logging documentation page. Support for version 3 of the logical replication protocol PostgreSQL 15 introduced improvements to logical replication, in particular around streaming large, in-progress transactions. Npgsql 7.0 adds support for improvements - see #4216 for more information (thanks @Brar!). Breaking changes CommandType.StoredProcedure now invokes procedures instead of functions When NpgsqlCommand.CommandType is set to CommandType.StoredProcedure, Npgsql now generates SQL for invoking a PostgreSQL stored procedure, and not a function, as before. To opt out of this breaking change and continue to invoke functions as before, enable the Npgsql.EnableStoredProcedureCompatMode AppContext switch as follows: AppContext.SetSwitch(\"Npgsql.EnableStoredProcedureCompatMode\", true); For context, PostgreSQL originally only supported functions, and did not support the standard SQL concept of stored procedures; because of this, CommandType.StoredProcedure was implemented to invoke functions. PostgreSQL 11 then introduced stored procedures, which have various advantages over functions in some scenarios (e.g. the ability to use transactions). The 7.0 release changes CommandType.StoredProcedure to invoke procedures as its naming suggests, and aligns Npgsql with other database providers for better compatibility. Note that with Npgsql, there is no advantage in using CommandType.StoredProcedure over simply invoking your function or procedure via SQL. Doing so is in fact recommended: // Invoke a procedure using var command1 = new NpgsqlCommand(\"CALL some_procedure($1, $2)\", connection) { new() { Value = \"some_value\" }, new() { Value = \"some_other_value\" } }; // Invoke a function using var command2 = new NpgsqlCommand(\"SELECT * FROM some_function($1, $2)\", connection) { new() { Value = \"some_value\" }, new() { Value = \"some_other_value\" } }; For more information on calling procedures and functions, see this doc section. Managing type mappings at the connection level is no longer supported Previous versions of Npgsql allowed mapping custom types (enums/composites) and configuring plugins (NetTopologySuite, NodaTime) at the connection level; the type mapping change would persist only for the lifetime of the connection, and would be reverted when the connection closed. This mechanism was inefficient - connections get opened and closed a lot - and added significant maintenance burden internally. With the introduction of NpgsqlDataSource, Npgsql now has a natural API point for managing type mappings: var dataSourceBuilder = new NpgsqlDataSourceBuilder(\"Host=localhost;Username=test;Password=test\"); dataSourceBuilder.MapEnum<MyEnum>(); dataSourceBuilder.UseNodaTime(); await using var dataSource = dataSourceBuilder.Build(); All connections handed out by the data source will use the configured type mappings. Note that managing type mappings globally via NpgsqlConnection.GlobalTypeMapper is supported as before, but has been marked as obsolete; although we do not plan on removing global type mappings any time soon, <xref:Npgsql.NpgsqlDataSourceBuilder> is now the recommended way to manage type mappings. Global type mappings must now be done before any usage Previously, any type mapping configuration done via NpgsqlConnection.GlobalTypeMapper would take effect for any new connection opened after the change. Starting with 7.0, global type mappings must be done before the data source or pool is created. To ensure correct functioning, do any global type mappings at the very start of your program, before using any other Npgsql API. NpgsqlDataReader.Dispose no longer swallows exceptions Previously, when an unconsumed <xref:Npgsql.NpgsqlDataReader> was disposed, any exceptions that occurred while consuming the remaining results were swallowed and Dispose completed successfully. These exceptions are no longer swallowed, and are thrown from Dispose. This also affects scenarios where <xref:Npgsql.NpgsqlDataReader> is used in a C# using statement. In most cases in .NET, throwing from Dispose is discouraged. If the instance being disposed is used in a using statement and some exception is thrown, that exception triggers Dispose being called (as per using), and if Dispose throws an exception of its own, then that exception bubbles up. As a result, the original exception is hidden, making it difficult to understand exactly what happened. As a result, it is common for Dispose to catch any exceptions internally and swallow them, allowing the original exception to bubble up. However, in the Npgsql case, swallowing exceptions in Dispose can have very problematic consequences. A reader can be disposed before it's entirely consumed, after only part of the result set has been read; when this happens, Npgsql consumes the rest of the result set as part of the disposal. If any errors occur past this point, then those exceptions were previously swallowed; this meant that the application could be unaware that part of the command failed, since no exception was raised. Throwing these exceptions from dispose ensures that the application is aware of any failures that occurred. For more context on this change, see the discussion in #4377. The logging API has been replaced by Microsoft.Extensions.Logging Npgsql previously had its own logging API, requiring special adapters to the standard logging libraries. This API has been removed in 7.0, and replaced with support for the standard Microsoft.Extensions.Logging package. See Logging for more information. The obsoleted NpgsqlDateTime, NpgsqlDate and NpgsqlTimeSpan have been removed NpgsqlDateTime, NpgsqlDate and NpgsqlTimeSpan were \"provider-specific\" types, designed to expose the full range of the PostgreSQL date/time types, which can represent values beyond the built-in .NET types (e.g. DateTime). However, these types were problematic in many ways, and were seldom used. The types were obsoleted in Npgsql 6.0, and have been removed in 7.0. To deal with date/time values outside the range of the corresponding .NET types, see Date and Time Handling. NpgsqlConnection.Settings has been removed The connection's connection string is still exposed via <xref:Npgsql.NpgsqlConnection.ConnectionString?displayProperty=nameWithType>; this can be parsed with <xref:Npgsql.NpgsqlConnectionStringBuilder>. Replication APIs now return UTC DateTime Previously, replication APIs returned DateTime instances of Kind Unspecified; this has been changed to Utc to reflect the actual type of data sent by PostgreSQL. Contributors Thank you very much to the following people who have contributed to the individual 7.0.x. releases. Milestone 7.0.6 Contributor Assigned issues @vonzshik 10 @roji 1 Milestone 7.0.4 Contributor Assigned issues @vonzshik 3 @roji 2 Milestone 7.0.2 Contributor Assigned issues @vonzshik 5 @manandre 2 @roji 1 Milestone 7.0.1 Contributor Assigned issues @roji 3 @Brar 1 @vonzshik 1 Milestone 7.0.0 Contributor Assigned issues @roji 37 @Brar 9 @vonzshik 4 @0xced 1 @baal2000 1"
  },
  "doc/conceptual/Npgsql/release-notes/8.0.html": {
    "href": "doc/conceptual/Npgsql/release-notes/8.0.html",
    "title": "Npgsql 8.0 Release Notes | Npgsql Documentation",
    "keywords": "Npgsql 8.0 Release Notes Npgsql version 8.0 is out and available on nuget.org. Note Npgsql 8.0 will be the last version to support .NET Framework (via .NET Standard 2.0). Starting with 9.0, Npgsql will only target .NET TFMs supported at release time (i.e. net6.0). NativeAOT and trimming support Npgsql 8.0 now has 1st-class support for NativeAOT and trimming; the entire library has been properly annotated and is safe for use in applications. The majority of features have been made compatible with NativeAOT/trimming and can be used without issues, and most applications using Npgsql can be used as-is with NativeAOT/trimming without any changes. A few features which are incompatible require an explicit code opt-in, which generates a warning if used with NativeAOT/trimming enabled (see breaking change note). Considerable effort has gone into reducing Npgsql's size footprint; a minimal Npgsql application using NativeAOT and trimming now takes only around 5MB of disk space. To allow users to achieve a minimal size footprint, <xref:Npgsql.NpgsqlSlimDataSourceBuilder> has been introduced; unlike the standard <xref:Npgsql.NpgsqlDataSourceBuilder>, this builder includes only the very minimum of functionality by default, and allows adding additional features via opt-ins. This allows a pay-per-play approach to application size, where developers can choose only the features they actually need for optimal size. For more information, see <xref:Npgsql.NpgsqlSlimDataSourceBuilder>. Making Npgsql NativeAOT/trimming-compatible was a far-reaching effort, affecting many parts of the driver and involving a rewrite of large parts of Npgsql's internals (leading to many other internal improvements). This huge task was done mainly by Nino Floris, with considerable contributions by Nikita Kazmin. OpenTelemetry metrics Npgsql has emitted metrics for several versions, which provided aggregated metrics on various Npgsql interals; for example, it was possible to follow the state of the connection pool, or to track how many commands are being executed per second. Npgsql 8.0 improves on that by switching from the older EventCounter API to the newer System.Diagnostics.Metrics API, implementing OpenTelemetry metrics. To understand more about the different kinds of metrics APIs in .NET, see these docs. The new metrics have several advantages over the old one; for one thing, they allow associating multiple dimensions to metrics, e.g. allowing Npgsql to cleanly emit pool-related metrics separately for each data source (or connection string) used in the application. The Npgsql metrics implement the experimental OpenTelemetry semantic conventions for database metrics - adding some additional useful ones - and will evolve as that specification stabilizes. For more information, see the doc page on metrics. Register NpgsqlDataSource as a keyed DI service .NET 8.0 introduced keyed services for dependency injection, allowing multiple services with the same CLR type to be registered in a single DI service provider. This is particularly useful when needing to contact multiple databases from your DI-enabled application: var builder = WebApplication.CreateBuilder(args); builder.Services .AddNpgsqlDataSource(\"Host=localhost;Database=CustomersDB;Username=test;Password=test\", serviceKey: DatabaseType.CustomerDb) .AddNpgsqlDataSource(\"Host=localhost;Database=OrdersDB;Username=test;Password=test\", serviceKey: DatabaseType.OrdersDb); var app = builder.Build(); app.MapGet(\"/\", async ([FromKeyedServices(DatabaseType.OrdersDb)] NpgsqlConnection connection) => connection.ConnectionString); app.Run(); enum DatabaseType { CustomerDb, OrdersDb } In this ASP.NET Minimal API application, two Npgsql data sources are registered in DI - one for a customers database, and another for an orders database. When a data source - or connections - needs to be injected somewhere, an enum is used as the service key, to distinguish which database is being requested (note that connections to both databases can be requested by the same function!). For more information on registering Npgsql services in DI, see the documentation for Npgsql.DependencyInjection. Other features Allow using nullable value types with the generic NpgsqlParameter<T>, e.g. NpgsqlParameter<int?>. Introduce a non-caching password provider callback via <xref:Npgsql.NpgsqlDataSourceBuilder.UsePasswordProvider?displayProperty=nameWithType>. Allow customizing System.Text.Json JsonSerializationOptions via <xref:Npgsql.NpgsqlDataSourceBuilder.ConfigureJsonOptions?displayProperty=nameWithType>. Improvements and cleanup for networking type mappings: In addition to .NET <xref:System.Net.IPAddress>, PostgreSQL inet can also mapped to be mapped to <xref:NpgsqlTypes.NpgsqlInet>, which is an immutable struct containing both IP and netmask components. PostgreSQL cidr is now mapped to the newly-introduced <xref:NpgsqlTypes.NpgsqlCidr>. The mapping to ValueTuple<IPAddress, int> has been removed. Allow providing the root certificate programmatically via the new <xref:Npgsql.NpgsqlDataSourceBuilder.UseRootCertificate?displayProperty=nameWithType> Version 8.0 contains many other smaller features and bug fixes, see the 8.0.0 milestone for the full list of issues. Breaking changes JSON POCO and other dynamic features now require an explicit opt-in Npgsql 8.0 is fully compatible with NativeAOT and trimming (see above). While most driver capabilities have been made to work in those profiles, certain features involve dynamic coding practices and are incompatible with NativeAOT and/or trimming - at least for now. As a result, these features now require explicit opt-ins (annotated to be incompatible with NativeAOT/trimming), which you must add either on your <xref:Npgsql.NpgsqlDataSourceBuilder> or on <xref:Npgsql.NpgsqlConnection.GlobalTypeMapper?displayProperty=nameWithType>: PostgreSQL type Default .NET type JSON POCO mapping, JsonNode and subtypes <xref:Npgsql.INpgsqlTypeMapperExtensions.EnableDynamicJson> Unmapped enums, ranges, multiranges <xref:Npgsql.INpgsqlTypeMapperExtensions.EnableUnmappedTypes> Read PostgreSQL records as .NET tuples <xref:Npgsql.INpgsqlTypeMapperExtensions.EnableRecordsAsTuples> Existing code using the above features will start throwing exceptions after upgrading to Npgsql 8.0; the exceptions provide explicit guidance on how to add the opt-ins. SSL Mode=Require no longer validates certificates tl;dr use SSL Mode=VerifyCA or VerifyFull in order to validate certificates provided by PostgreSQL. In versions of Npgsql older than 6.0, specifying SSL Mode=Require made Npgsql validate the SSL/TLS certificate provided by PostgreSQL. This did not align with the meaning of \"require\" in PostgreSQL and other clients, where it simply means that SSL/TLS is required, but without certificate validation. To align with the standard PostgreSQL meaning, starting with Npgsql 6.0 VerifyCA or VerifyFull must be specified to validate the certificate. To prevent existing usage of Require to silently stop validating, Npgsql 6.0 and 7.0 forced Trust Server Certificate=true to be specified; this made users aware of the change, guiding them to either switch to VerifyCA/VerifyFull (if they want validation) or to add Trust Server Certificate=true (if they don't). After two major versions, we are now removing the requirement to specify Trust Server Certificate=true with SSL Mode=Require; the latter will behave in the standard PostgreSQL way and will not verify certificates. For more context, see #3988. IList<T> mapping now requires a generic NpgsqlParameter<T> Previous versions of Npgsql allowed writing arbitrary list types as PostgreSQL array, as long as they implemented the IList<T> interface: await using var command = new NpgsqlCommand(\"SELECT $1\", conn) { Parameters = { new NpgsqlParameter { Value = new ReadOnlyCollection<int>(new List<int> { 1, 2, 3 }) } } }; await using var reader = await command.ExecuteReaderAsync(); This capability has been removed; supporting it required a costly reflection check, which also would be difficult to implement with trimming enabled, potentially increasing binary size in an unacceptable way. As a mitigation, you can instead use the generic NpgsqlParameter<T> - typed with IList<T> - to do the same: await using var command = new NpgsqlCommand(\"SELECT $1\", conn) { Parameters = { new NpgsqlParameter<IList<int>> { Value = new ReadOnlyCollection<int>(new List<int> { 1, 2, 3 }) } } }; await using var reader = await command.ExecuteReaderAsync(); cidr now maps to NpgsqlCidr instead of ValueTuple<IPAddress, int> As part of improving Npgsql's support for the PostgreSQL network mappings (see above), the PostgreSQL cidr type now maps to the newly-introduced <xref:NpgsqlTypes.NpgsqlCidr>, and can no longer be mapped to ValueTuple<IPAddress, int>. Obsoletions and obsolete API removals NpgsqlTsVector.Parse() and NpgsqlTsQuery.Parse() are now obsolete. These methods attempted to mimic the behavior of the PostgreSQL to_tsvector and to_tsquery functions, but could only do so partially and in problematic ways. Use the PostgreSQL functions instead. The parsing functions on the built-in geometry types (NpgsqlPoint, NpgsqlBox etc.) have been removed; similarly, they partially replicated PostgreSQL parsing functionality client-side and had issues. NpgsqlLargeObjectManager and NpsgqlLargeObjectStream are now obsolete. These types were very rarely-used, provided only a thin wrapper over easily-accessible PostgreSQL large-object functions, and limited usage in various ways (e.g. they didn't allow batching). Call the PostgreSQL large-object functions directly. The Internal Command Timeout connection string parameter has been obsoleted. NpgsqlDbType.TimestampTZ and NpgsqlDbType.TimeTZ were obsoleted many releases ago, and were finally removed. Use NpgsqlDbType.TimestampTz and NpgsqlDbType.TimeTz instead. Executing a void-returning function returns .NET null instead of DBNull Previously, executing a void-returning returned DBNull.Value: var command = new NpgsqlCommand(\"SELECT pg_sleep(10)\", connection); var result = await command.ExecuteScalarAsync(); Before 8.0, result had the value DBNull.Value; this has been changed in 8.0 to be .NET null. This is more correct (as there are no results, rather than a result containing NULL), aligns with ADO.NET standard practices and with other drivers. Plugin APIs have been changed for NativeAOT/trimming support As part of the effort to make Npgsql compatible with NativeAOT and trimming, the plugin API was changed in fundamental, breaking ways. Although this API never had the stability guarantees of a true public API (it was and still is in an Internal namespace), external plugins which were developed with it will require adjustments. Contributors Thank you very much to the following people who have contributed to the individual 8.0.x. releases. Milestone 8.0.0 Contributor Assigned issues @NinoFloris 43 @vonzshik 23 @roji 20 @manandre 4 @BogdanYarotsky 1 @Brar 1 @erikdesj 1 @SoftStoneDevelop 1 @sonquer 1 @yucelkivanc-hepsiburada 1"
  },
  "doc/conceptual/Npgsql/replication.html": {
    "href": "doc/conceptual/Npgsql/replication.html",
    "title": "Logical and Physical Replication | Npgsql Documentation",
    "keywords": "Logical and Physical Replication Replication allows a client to receive a continuous stream of updates from a PostgreSQL database, providing a near-realtime view of all changes as they occur. While this feature was originally developed to keep PostgreSQL standby replicas in sync with a primary, it can be used by arbitrary client applications. Replication can be used anywhere where a constant change feed of database changes is required; for example, an external application can be notified in near-realtime of any changes that occurred in a particular database table. This can be useful for external auditing purposes, for replicating certain data somewhere else, for implement the outbox pattern (see Additional resources below), and various other usages. Npgsql provides a 1st-class API for writing .NET replication clients, detailed below. While PostgreSQL supports both logical and physical replication, in the majority of cases .NET applications will want to use logical replication. Logical replication Logical replication is a means to stream messages generated by PostgreSQL logical decoding plugins to a client. The default implementation that is used by PostgreSQL itself to perform logical server to server replication is the Logical Streaming Replication Protocol which uses the pgoutput plugin, but PostgreSQL supports streaming messages generated by other plugins too and Npgsql supports receiving those. General setup To set up logical replication, follow the quick setup instructions in the PostgreSQL docs (note that a SUBSCRIPTION isn't required since the client isn't PostgreSQL): Enable logical replication in your postgresql.conf file: wal_level = logical Set up a replication user in your pg_hba.conf file: host replication repuser 0.0.0.0/0 md5 The user repuser must exist in your cluster and either be a superuser or have the replication attribute set. See CREATE ROLE docs. Logical Streaming Replication Protocol (pgoutput plugin) The modern, recommended way to perform logical replication was introduced in PostgreSQL 10 - see the PostgreSQL documentation. This method, using the built-in pgoutput replication plugin, streams efficient, binary messages to represent database updates such as INSERT, UPDATE and DELETE (see the full list); Npgsql exposes these messages as an IAsyncEnumerable which can easily be enumerated and consumed. Create a publication, which defines the group of tables in the database you wish to replicate: CREATE PUBLICATION blog_pub FOR TABLE blogs; Create a replication slot, which will hold the state of the replication stream: SELECT * FROM pg_create_logical_replication_slot('blog_slot', 'pgoutput'); If your application goes down, the slot persistently records the last data streamed to it, and allows resuming the application at the point where it left off. At this point, everything is ready to start replicating! Create this simple .NET program with Npgsql: await using var conn = new LogicalReplicationConnection(\"<connection_string>\"); await conn.Open(); var slot = new PgOutputReplicationSlot(\"blog_slot\"); // The following will loop until the cancellation token is triggered, and will print message types coming from PostgreSQL: var cancellationTokenSource = new CancellationTokenSource(); await foreach (var message in conn.StartReplication( slot, new PgOutputReplicationOptions(\"blog_pub\", 1), cancellationTokenSource.Token)) { Console.WriteLine($\"Received message type: {message.GetType().Name}\"); // Always call SetReplicationStatus() or assign LastAppliedLsn and LastFlushedLsn individually // so that Npgsql can inform the server which WAL files can be removed/recycled. conn.SetReplicationStatus(message.WalEnd); } For example, if you insert a new row into your blogs table, you should see the following output: Received message type: BeginMessage Received message type: RelationMessage Received message type: InsertMessage Received message type: CommitMessage Warning Npgsql internally recycles the message instances it hands out. It is an error to use a message received from StartReplication once the next message has been read. The above was just a minimal \"getting started\" guide for logical replication - many additional configuration options and modes exist as well. Consult the PostgreSQL documentation for more details. Test decoding (test_decoding plugin) An additional logical replication plugin which Npgsql supports is test_decoding. This plugin outputs textual representations of events, which are less efficient and need to be parsed; it is meant for testing that replication works rather than for building robust production apps. However, it can still be useful in some scenarios, especially in older PostgreSQL versions where pgoutput wasn't yet introduced. To use test_decoding, first create a logical replication slot with test_decoding as the plugin type. SELECT * FROM pg_create_logical_replication_slot('blog_slot', 'test_decoding'); After that use the following: await using var conn = new LogicalReplicationConnection(\"Host=localhost;Username=test;Password=test\"); await conn.Open(); var slot = new TestDecodingReplicationSlot(\"blog_slot\"); // The following will loop until the cancellation token is triggered, and will print message types coming from PostgreSQL: var cancellationTokenSource = new CancellationTokenSource(); await foreach (var message in conn.StartReplication(slot, cancellationTokenSource.Token)) { Console.WriteLine($\"Message: {message.Data}\"); // Always call SetReplicationStatus() or assign LastAppliedLsn and LastFlushedLsn individually // so that Npgsql can inform the server which WAL files can be removed/recycled. conn.SetReplicationStatus(message.WalEnd); } Inserting a row will produce the following string messages: Message: BEGIN 230413 Message: table public.blogs: INSERT: id[integer]:2 name[text]:'blog1' Message: COMMIT 230413 Warning Npgsql internally recycles the message instances it hands out. It is an error to use a message received from StartReplication once the next message has been read. Physical replication Finally, PostgreSQL also supports physical replication, which streams raw block data rather than logical events on changes. While useful for synchronizing PostgreSQL replicas and supported by Npgsql, this mode is unlikely to be useful for a typical .NET program client. Additional resources See here for a great post on implementing the outbox pattern via PostgreSQL logical replication. The outbox pattern guarantees delivery of an event from the database to e.g. a queue."
  },
  "doc/conceptual/Npgsql/security.html": {
    "href": "doc/conceptual/Npgsql/security.html",
    "title": "Security and Encryption | Npgsql Documentation",
    "keywords": "Security and Encryption Password management The simplest way to log into PostgreSQL is by specifying a Username and a Password in your connection string. Depending on how your PostgreSQL is configured (in the pg_hba.conf file), Npgsql will send the password in MD5 or in cleartext (not recommended). If a Password is not specified and your PostgreSQL is configured to request a password, Npgsql will look for a standard PostgreSQL password file. If you specify the Passfile connection string parameter, the file it specifies will be used. If that parameter isn't defined, Npgsql will look under the path taken from PGPASSFILE environment variable. If the environment variable isn't defined, Npgsql will fall back to the system-dependent default directory which is $HOME/.pgpass for Unix and %APPDATA%\\postgresql\\pgpass.conf for Windows. Auth token rotation and dynamic password In some cloud scenarios, logging into PostgreSQL is done with an auth token that is rotated every time interval (e.g. one hour). Npgsql has a built-in periodic password provider mechanism, which allows using an up-to-date access token with zero effort: var dataSourceBuilder = new NpgsqlDataSourceBuilder(...); dataSourceBuilder.UsePasswordProvider( passwordProvider: _ => throw new NotSupportedException(), passwordProviderAsync: (builder, token) => /* code to fetch the new access token */); await using var dataSource = dataSourceBuilder.Build(); Every time a new physical connection needs to be opened to PostgreSQL, either the synchronous passwordProvider or the asynchronous passwordProviderAsync will be called (depending whether you used Open() or OpenAsync()). Since modern .NET applications are encouraged to always use asynchronous I/O, it's good practice to simply throw in the synchronous password provider, as above. Note that since the password provider is invoked every time a physical connection is opened, it shouldn't take too long; typically, this would call into a cloud provider API (e.g. Azure Managed Identity), which itself implements a caching mechanism. However, if no such caching is done and the code could take a while, you can instead instruct Npgsql to cache the auth token for a given amount of time: dataSourceBuilder.UsePeriodicPasswordProvider( (settings, cancellationToken) => /* async code to fetch the new access token */, TimeSpan.FromMinutes(55), // Interval for refreshing the token TimeSpan.FromSeconds(5)); // Interval for retrying after a refresh failure Finally, if you already have code running when the auth token changes, you can simply inject it manually at any time into a working data source: dataSource.Password = <new password>; Any physical connection that's opened after this point will use the newly-injected password. Encryption (SSL/TLS) By default PostgreSQL connections are unencrypted, but you can turn on SSL/TLS encryption if you wish. First, you have to set up your PostgreSQL to receive SSL/TLS connections as described here. Once that's done, specify SSL Mode in your connection string as detailed below. Version 6.0+ Older versions Starting with 6.0, the following SSL Mode values are supported (see the PostgreSQL docs for more details): SSL Mode Eavesdropping protection Man-in-the-middle protection Statement Disable No No I don't care about security, and I don't want to pay the overhead of encryption. Allow Maybe No I don't care about security, but I will pay the overhead of encryption if the server insists on it. Prefer (default) Maybe No I don't care about encryption, but I wish to pay the overhead of encryption if the server supports it. Require1 Yes No I want my data to be encrypted, and I accept the overhead. I trust that the network will make sure I always connect to the server I want. VerifyCA Yes Depends on CA policy I want my data encrypted, and I accept the overhead. I want to be sure that I connect to a server that I trust. VerifyFull Yes Yes I want my data encrypted, and I accept the overhead. I want to be sure that I connect to a server I trust, and that it's the one I specify. 1 Prior to Npgsql 8.0, SSL Mode=Require required explicitly setting Trust Server Certificate=true as well, to make it explicit that the server certificate isn't validated. Starting with 8.0, Trust Server Certificate=true is no longer required and does nothing. The default mode in 6.0+ is Prefer, which allows SSL but does not require it, and does not validate certificates. Versions prior to 6.0 supported the following SSL Mode values: SSL Mode Eavesdropping protection Man-in-the-middle protection Statement Disable No No I don't care about security, and I don't want to pay the overhead of encryption. Prefer Maybe Maybe I don't care about encryption, but I wish to pay the overhead of encryption if the server supports it. Require Yes Yes I want my data encrypted, and I accept the overhead. I want to be sure that I connect to a server I trust, and that it's the one I specify. The default mode prior to 6.0 was Disable. To disable certificate validation when using Require, set Trust Server Certificate to true; this allows connecting to servers with e.g. self-signed certificates, while still requiring encryption. Advanced server certificate validation If the root CA of the server certificate isn't installed in your machine's CA store, validation will fail. Either install the certificate in your machine's CA store, or point to it via the Root Certificate connection string parameter or via the PGSSLROOTCERT environment variable. Note that Npgsql does not perform certificate revocation validation by default, since this is an optional extension not implemented by all providers and CAs. To turn on certificate revocation validation, specify Check Certificate Revocation=true on the connection string. Finally, if the above options aren't sufficient for your scenario, you can call <xref:Npgsql.NpgsqlDataSourceBuilder.UseUserCertificateValidationCallback(System.Net.Security.RemoteCertificateValidationCallback)?displayProperty=nameWithType> to provide your custom server certificate validation logic (this gets set on the underlying .NET SslStream). Client certificates PostgreSQL may be configured to require valid certificates from connecting clients for authentication. Npgsql automatically sends client certificates specified in the following places: The SSL Certificate connection string parameter. The PGSSLCERT environment variable. The default locations of ~/.postgresql/postgresql.crt (on Unix) or %APPDATA%\\postgresql\\postgresql.crt (on Windows) To provide a password for a client certificate, set either the SSL Password (6.0 and higher) or Client Certificate Key (5.0 and lower) connection string parameter. Finally, you can call <xref:Npgsql.NpgsqlDataSourceBuilder.UseClientCertificate(System.Security.Cryptography.X509Certificates.X509Certificate)?displayProperty=nameWithType>, <xref:Npgsql.NpgsqlDataSourceBuilder.UseClientCertificates(System.Security.Cryptography.X509Certificates.X509CertificateCollection)?displayProperty=nameWithType> or <xref:Npgsql.NpgsqlDataSourceBuilder.UseClientCertificatesCallback(System.Action{System.Security.Cryptography.X509Certificates.X509CertificateCollection})?displayProperty=nameWithType> to programmatically provide a certificate, multiple certificates or a callback which returns certificates (this works like on the underlying .NET SslStream). Note Npgsql supports .PFX and .PEM certificates starting with 6.0. Previously, only .PFX certificates were supported. Password-less authentication (GSS/SSPI/Kerberos) Logging in with a username and password may not be ideal, since your application must have access to your password, and raise questions around secret management. An alternate way of authenticating is to use GSS or SSPI to negotiate Kerberos. The advantage of this method is that authentication is handed off to your operating system, using your already-open login session. Your application never needs to handle a password. You can use this method for a Kerberos login, Windows Active Directory or a local Windows session. Instructions on setting up Kerberos and SSPI are available in the PostgreSQL auth methods docs. Some more instructions for SSPI are available here. Once your PostgreSQL is configured correctly, it will require GSS/SSPI authentication from Npgsql at login, and you can simply drop the Password parameter from the connection string. However, Npgsql must still send a username to PostgreSQL. If you specify a Username connection string parameter, Npgsql will send that as usual. If you omit it, Npgsql will attempt to detect your system username, including the Kerberos realm. Note that by default, PostgreSQL expects your Kerberos realm to be sent in your username (e.g. username@REALM); you can have Npgsql detect the realm by setting Include Realm to true in your connection string. Alternatively, you can disable add include_realm=0 in your PostgreSQL's pg_hba.conf entry, which will make it strip the realm. You always have the possibility of explicitly specifying the username sent to PostgreSQL yourself. Note that in versions of Npgsql prior to 8.0, use of GSS/SSPI authentication requires that Integrated Security=true be specified on the connection string. This requirement has been removed in Npgsql 8.0."
  },
  "doc/conceptual/Npgsql/types/basic.html": {
    "href": "doc/conceptual/Npgsql/types/basic.html",
    "title": "Supported Types and their Mappings | Npgsql Documentation",
    "keywords": "Supported Types and their Mappings The following lists the built-in mappings when reading and writing CLR types to PostgreSQL types. Note that in addition to the below, enum and composite mappings are documented in a separate page. Note also that several plugins exist to add support for more mappings (e.g. spatial support for PostGIS), these are listed in the Types menu. Read mappings The following shows the mappings used when reading values. The default type is returned when using NpgsqlCommand.ExecuteScalar(), NpgsqlDataReader.GetValue() and similar methods. You can read as other types by calling NpgsqlDataReader.GetFieldValue<T>(). Provider-specific types are returned by NpgsqlDataReader.GetProviderSpecificValue(). PostgreSQL type Default .NET type Non-default .NET types boolean bool smallint short byte, sbyte, int, long, float, double, decimal integer int byte, short, long, float, double, decimal bigint long long, byte, short, int, float, double, decimal real float double double precision double numeric decimal byte, short, int, long, float, double, BigInteger (6.0+) money decimal text string char[] character varying string char[] character string char[] citext string char[] json string char[] jsonb string char[] xml string char[] uuid Guid bytea byte[] timestamp without time zone DateTime (Unspecified) timestamp with time zone DateTime (Utc1) DateTimeOffset (Offset=0)2 date DateTime DateOnly (6.0+) time without time zone TimeSpan TimeOnly (6.0+) time with time zone DateTimeOffset interval TimeSpan3 <xref:NpgsqlTypes.NpgsqlInterval> cidr <xref:NpgsqlTypes.NpgsqlCidr>4 inet IPAddress <xref:NpgsqlTypes.NpgsqlInet> macaddr PhysicalAddress tsquery NpgsqlTsQuery tsvector NpgsqlTsVector bit(1) bool BitArray bit(n) BitArray bit varying BitArray point NpgsqlPoint lseg NpgsqlLSeg path NpgsqlPath polygon NpgsqlPolygon line NpgsqlLine circle NpgsqlCircle box NpgsqlBox hstore Dictionary<string, string> oid uint xid uint cid uint oidvector uint[] name string char[] (internal) char char byte, short, int, long geometry (PostGIS) PostgisGeometry record object[] composite types T range types NpgsqlRange<TElement> multirange types (PG14) NpgsqlRange<TElement>[] enum types TEnum array types Array (of element type) 1 In versions prior to 6.0 (or when Npgsql.EnableLegacyTimestampBehavior is enabled), reading a timestamp with time zone returns a Local DateTime instead of Utc. See the breaking change note for more info. 2 In versions prior to 6.0 (or when Npgsql.EnableLegacyTimestampBehavior is enabled), reading a timestamp with time zone as a DateTimeOffset returns a local offset based on the timezone of the server where Npgsql is running. 3 PostgreSQL intervals with month or year components cannot be read as TimeSpan. Consider using NodaTime's Period type, or <xref:NpgsqlTypes.NpgsqlInterval>. 4 Prior to version 8.0, the default mapping for cidr was ValueTuple<IPAddress, int>. The Default .NET type column specifies the data type NpgsqlDataReader.GetValue() will return. NpgsqlDataReader.GetProviderSpecificValue will return a value of a data type specified in the Provider-specific type column, or the Default .NET type if there is no specialization. Finally, the third column specifies other CLR types which Npgsql supports for the PostgreSQL data type. These can be retrieved by calling NpgsqlDataReader.GetBoolean(), GetByte(), GetDouble() etc. or via GetFieldValue<T>(). Write mappings There are three rules that determine the PostgreSQL type sent for a parameter: If the parameter's NpgsqlDbType is set, it is used. If the parameter's DataType is set, it is used. If the parameter's DbType is set, it is used. If none of the above is set, the backend type will be inferred from the CLR value type. PostgreSQL type Default .NET types Non-default .NET types NpgsqlDbType DbType boolean bool Boolean Boolean smallint short, byte, sbyte Smallint Int16 integer int Integer Int32 bigint long Bigint Int64 real float Real Single double precision double Double Double numeric decimal, BigInteger (6.0+) Numeric Decimal, VarNumeric money decimal Money Currency text string, char[], char Text String, StringFixedLength, AnsiString, AnsiStringFixedLength character varying string, char[], char Varchar character string, char[], char Char citext string, char[], char Citext json string, char[], char Json jsonb string, char[], char Jsonb xml string, char[], char Xml uuid Guid Uuid bytea byte[] ArraySegment<byte>, Stream (7.0+) Bytea Binary timestamp with time zone DateTime (Utc)1, DateTimeOffset TimestampTz DateTime2, DateTimeOffset timestamp without time zone DateTime (Local/Unspecified)1 Timestamp DateTime2 date DateOnly (6.0+) DateTime Date Date time without time zone TimeOnly (6.0+) TimeSpan Time Time time with time zone DateTimeOffset TimeTz interval TimeSpan <xref:NpgsqlTypes.NpgsqlInterval> Interval cidr ValueTuple<IPAddress, int>, IPAddress Cidr inet IPAddress ValueTuple<IPAddress, int> Inet macaddr PhysicalAddress MacAddr tsquery NpgsqlTsQuery TsQuery tsvector NpgsqlTsVector TsVector bit bool, BitArray, string Bit bit varying BitArray bool, BitArray, string Varbit point NpgsqlPoint Point lseg NpgsqlLSeg LSeg path NpgsqlPath Path polygon NpgsqlPolygon Polygon line NpgsqlLine Line circle NpgsqlCircle Circle box NpgsqlBox Box hstore IDictionary<string, string> Hstore oid uint Oid xid uint Xid cid uint Cid oidvector uint[] Oidvector name string, char[], char Name (internal) char byte InternalChar composite types Pre-mapped type Composite range types NpgsqlRange<TSubtype> Range | NpgsqlDbType enum types Pre-mapped type Enum array types T[], List<T> Array | NpgsqlDbType 1 UTC DateTime is written as timestamp with time zone, Local/Unspecified DateTimes are written as timestamp without time zone. In versions prior to 6.0 (or when Npgsql.EnableLegacyTimestampBehavior is enabled), DateTime is always written as timestamp without time zone. 2In versions prior to 6.0 (or when Npgsql.EnableLegacyTimestampBehavior is enabled), DbType.DateTime is mapped to timestamp without time zone. Notes when using Range and Array, bitwise-or NpgsqlDbType.Range or NpgsqlDbType.Array with the child type. For example, to construct the NpgsqlDbType for a int4range, write NpgsqlDbType.Range | NpgsqlDbType.Integer. To construct the NpgsqlDbType for an int[], write NpgsqlDbType.Array | NpgsqlDbType.Integer. For information about enums, see the Enums and Composites page."
  },
  "doc/conceptual/Npgsql/types/datetime.html": {
    "href": "doc/conceptual/Npgsql/types/datetime.html",
    "title": "Date and Time Handling | Npgsql Documentation",
    "keywords": "Date and Time Handling Warning Npgsql 6.0 introduced some important changes to how timestamps are mapped, see the release notes for more information. Note The recommended way of working with date/time types is the NodaTime plugin: the NodaTime types are much better-designed, avoid the flaws in the built-in BCL types, and are fully supported by Npgsql. Handling date and time values usually isn't hard, but you must pay careful attention to differences in how the .NET types and PostgreSQL represent dates. It's worth reading the PostgreSQL date/time type documentation to familiarize yourself with PostgreSQL's types. .NET types and PostgreSQL types The .NET and PostgreSQL types differ in the resolution and range they provide; the .NET type usually have a higher resolution but a lower range than the PostgreSQL types: PostgreSQL type Precision/Range .NET Native Type Precision/Range timestamp with time zone 1 microsecond, 4713BC-294276AD DateTime (UTC) 100 nanoseconds, 1AD-9999AD timestamp without time zone 1 microsecond, 4713BC-294276AD DateTime (Unspecified) 100 nanoseconds, 1AD-9999AD date 1 day, 4713BC-5874897AD DateOnly (6.0+), DateTime 100 nanoseconds, 1AD-9999AD time without time zone 1 microsecond, 0-24 hours TimeOnly (6.0+), TimeSpan 100 nanoseconds, -10,675,199 - 10,675,199 days time with time zone 1 microsecond, 0-24 hours DateTimeOffset (ignore date) 100 nanoseconds, 1AD-9999AD interval 1 microsecond, -178000000-178000000 years TimeSpan 100 nanoseconds, -10,675,199 - 10,675,199 days For almost all applications, the range of the .NET native types (or the NodaTime types) are more than sufficient. In the rare cases where you need to access values outside these ranges, timestamps can be accessed as long, dates as int, and intervals as NpgsqlInterval. These are the raw PostgreSQL binary representations of these type, so you'll have to deal with encoding/decoding yourself. Timestamps and timezones Warning A common mistake is for users to think that the PostgreSQL timestamp with time zone type stores the timezone in the database. This is not the case: only a UTC timestamp is stored. There is no single PostgreSQL type that stores both a date/time and a timezone, similar to .NET DateTimeOffset. To store a timezone in the database, add a separate text column containing the timezone ID. In PostgreSQL, timestamp with time zone represents a UTC timestamp, while timestamp without time zone represents a local or unspecified time zone. Starting with 6.0, Npgsql maps UTC DateTime to timestamp with time zone, and Local/Unspecified DateTime to timestamp without time zone; trying to send a non-UTC DateTime as timestamptz will throw an exception, etc. Npgsql also supports reading and writing DateTimeOffset to timestamp with time zone, but only with Offset=0. Prior to 6.0, timestamp with time zone would be converted to a local timestamp when read - see below for more details. The precise improvements and breaking changes are detailed in the 6.0 breaking changes; to revert to the pre-6.0 behavior, add the following at the start of your application, before any Npgsql operations are invoked: AppContext.SetSwitch(\"Npgsql.EnableLegacyTimestampBehavior\", true); Use of the time with time zone type is discouraged, see the PostgreSQL documentation. You can use a DateTimeOffset to read and write values - the date component will be ignored. Infinity values PostgreSQL supports the special values -infinity and infinity for the timestamp and date types (see docs); these can be useful to represent a value which is earlier or later than any other value. Starting with Npgsql 6.0, these special values are mapped to the MinValue and MaxValue value on the corresponding .NET types (DateTime and DateOnly, NodaTime Instant and LocalDate). To opt out of this behavior, set the following AppContext switch at the start of your application: AppContext.SetSwitch(\"Npgsql.DisableDateTimeInfinityConversions\", true); Note: in versions prior to 6.0, the connection string parameter Convert Infinity DateTime could be used to opt into these infinity conversions. That connection string parameter has been removed. Detailed Behavior: Reading values from the database PostgreSQL type Default .NET type Non-default .NET types timestamp with time zone DateTime (Utc1) DateTimeOffset (Offset=0)2 timestamp without time zone DateTime (Unspecified) date DateTime DateOnly (6.0+) time without time zone TimeSpan TimeOnly (6.0+) time with time zone DateTimeOffset interval TimeSpan (3) <xref:NpgsqlTypes.NpgsqlInterval> 1 In versions prior to 6.0 (or when Npgsql.EnableLegacyTimestampBehavior is enabled), reading a timestamp with time zone returns a Local DateTime instead of Utc. See the breaking change note for more info. 2 In versions prior to 6.0 (or when Npgsql.EnableLegacyTimestampBehavior is enabled), reading a timestamp with time zone as a DateTimeOffset returns a local offset based on the timezone of the server where Npgsql is running. 3 PostgreSQL intervals with month or year components cannot be read as TimeSpan. Consider using NodaTime's Period type, or <xref:NpgsqlTypes.NpgsqlInterval>. Detailed Behavior: Sending values to the database PostgreSQL type Default .NET types Non-default .NET types NpgsqlDbType DbType timestamp with time zone DateTime (Utc)1, DateTimeOffset TimestampTz DateTime2, DateTimeOffset timestamp without time zone DateTime (Local/Unspecified)1 Timestamp DateTime2 date DateOnly (6.0+) DateTime Date Date time without time zone TimeOnly (6.0+) TimeSpan Time Time time with time zone DateTimeOffset TimeTz interval TimeSpan Interval 1 UTC DateTime is written as timestamp with time zone, Local/Unspecified DateTimes are written as timestamp without time zone. In versions prior to 6.0 (or when Npgsql.EnableLegacyTimestampBehavior is enabled), DateTime is always written as timestamp without time zone. 2In versions prior to 6.0 (or when Npgsql.EnableLegacyTimestampBehavior is enabled), DbType.DateTime is mapped to timestamp without time zone."
  },
  "doc/conceptual/Npgsql/types/enums_and_composites.html": {
    "href": "doc/conceptual/Npgsql/types/enums_and_composites.html",
    "title": "PostgreSQL enums and composites | Npgsql Documentation",
    "keywords": "PostgreSQL enums and composites PostgreSQL supports enum types and composite types as database columns, and Npgsql supports reading and writing these. This allows you to seamlessly read and write enum and composite values to the database without worrying about conversions. Creating your types Let's assume you've created some enum and composite types in PostgreSQL: CREATE TYPE mood AS ENUM ('sad', 'ok', 'happy'); CREATE TYPE inventory_item AS ( name text, supplier_id integer, price numeric ); To use these types with Npgsql, you must first define corresponding CLR types that will be mapped to the PostgreSQL types: public enum Mood { Sad, Ok, Happy } public class InventoryItem { public string Name { get; set; } = \"\"; public int SupplierId { get; set; } public decimal Price { get; set; } } Mapping your CLR types Once your types are defined both in PostgreSQL and in C#, you can now configure the mapping between them with Npgsql. NpgsqlDataSource Global mapping Connection mapping Note NpgsqlDataSource was introduced in Npgsql 7.0, and is the recommended way to manage type mapping. If you're using an older version, see the other methods. var dataSourceBuilder = new NpgsqlDataSourceBuilder(...); dataSourceBuilder.MapEnum<Mood>(); dataSourceBuilder.MapComposite<InventoryItem>(); await using var dataSource = dataSourceBuilder.Build(); If you're using an older version of Npgsql which doesn't yet support NpgsqlDataSource, you can configure mappings globally for all connections in your application: NpgsqlConnection.GlobalTypeMapper.MapEnum<Mood>(); NpgsqlConnection.GlobalTypeMapper.MapComposite<InventoryItem>(); For this to work, you must place this code at the beginning of your application, before any other Npgsql API is called. Note that in Npgsql 7.0, global type mappings are obsolete (but still supported) - NpgsqlDataSource is the recommended way to manage type mappings. Note This mapping method has been removed in Npgsql 7.0. Older versions of Npgsql supported configuring a type mapping on an individual connection, as follows: var conn = new NpgsqlConnection(...); conn.TypeMapper.MapEnum<Mood>(); conn.TypeMapper.MapComposite<InventoryItem>(); Whatever the method used, your CLR types Mood and InventoryItem are now mapped to the PostgreSQL types mood and inventory_item. Using your mapped types Once your mapping is in place, you can read and write your CLR types as usual: // Writing await using (var cmd = new NpgsqlCommand(\"INSERT INTO some_table (my_enum, my_composite) VALUES ($1, $2)\", conn)) { cmd.Parameters.Add(new() { Value = Mood.Happy }); cmd.Parameters.Add(new() { Value = new InventoryItem { ... } }); cmd.ExecuteNonQuery(); } // Reading await using (var cmd = new NpgsqlCommand(\"SELECT my_enum, my_composite FROM some_table\", conn)) await using (var reader = cmd.ExecuteReader()) { reader.Read(); var enumValue = reader.GetFieldValue<Mood>(0); var compositeValue = reader.GetFieldValue<InventoryItem>(1); } Note that your PostgreSQL enum and composites types (mood and inventory_data in the sample above) must be defined in your database before the first connection is created (see CREATE TYPE). If you're creating PostgreSQL types within your program, call NpgsqlConnection.ReloadTypes() to make sure Npgsql becomes properly aware of them. Name translation CLR type and field names are usually Pascal case (e.g. InventoryData), whereas in PostgreSQL they are snake case (e.g. inventory_data). To help make the mapping for enums and composites seamless, pluggable name translators are used translate all names. The default translation scheme is NpgsqlSnakeCaseNameTranslator, which maps names like SomeType to some_type, but you can specify others. The default name translator can be set for all your connections via NpgsqlConnection.GlobalTypeMapper.DefaultNameTranslator, or for a specific connection for NpgsqlConnection.TypeMapper.DefaultNameTranslator. You also have the option of specifying a name translator when setting up a mapping: NpgsqlConnection.GlobalTypeMapper.MapComposite<InventoryData>(\"inventory_data\", new NpgsqlNullNameTranslator()); Finally, you may control mappings on a field-by-field basis via the [PgName] attribute. This overrides the name translator. public enum Mood { [PgName(\"depressed\")] Sad, Ok, [PgName(\"ebullient\")] Happy } Reading and writing unmapped enums In some cases, it may be desirable to interact with PostgreSQL enums without a pre-existing CLR enum type - this is useful mainly if your program doesn't know the database schema and types in advance, and needs to interact with any enum/composite type. Npgsql allows reading and writing enums as simple strings: // Writing enum as string await using (var cmd = new NpgsqlCommand(\"INSERT INTO some_table (my_enum) VALUES ($1)\", conn)) { cmd.Parameters.Add(new() { Value = \"Happy\" DataTypeName = \"mood\" }); cmd.ExecuteNonQuery(); } // Reading enum as string await using (var cmd = new NpgsqlCommand(\"SELECT my_enum FROM some_table\", conn)) await using (var reader = cmd.ExecuteReader()) { reader.Read(); var enumValue = reader.GetFieldValue<string>(0); }"
  },
  "doc/conceptual/Npgsql/types/geojson.html": {
    "href": "doc/conceptual/Npgsql/types/geojson.html",
    "title": "PostGIS/GeoJSON Type Plugin | Npgsql Documentation",
    "keywords": "PostGIS/GeoJSON Type Plugin The Npgsql.GeoJSON plugin makes Npgsql read and write PostGIS spatial types as GeoJSON (RFC7946) types, via the GeoJSON.NET library. As an alternative, you can use Npgsql.NetTopologySuite, which is a full-fledged .NET spatial library with many features. Setup To avoid forcing a dependency on the GeoJSON library for users not using spatial, GeoJSON support is delivered as a separate plugin. To use the plugin, simply add a dependency on Npgsql.GeoJSON and set it up in one of the following ways: NpgsqlDataSource Global mapping Connection mapping Note NpgsqlDataSource was introduced in Npgsql 7.0, and is the recommended way to manage type mapping. If you're using an older version, see the other methods. var dataSourceBuilder = new NpgsqlDataSourceBuilder(...); dataSourceBuilder.UseGeoJson(); await using var dataSource = dataSourceBuilder.Build(); If you're using an older version of Npgsql which doesn't yet support NpgsqlDataSource, you can configure mappings globally for all connections in your application: NpgsqlConnection.GlobalTypeMapper.UseGeoJson(); For this to work, you must place this code at the beginning of your application, before any other Npgsql API is called. Note that in Npgsql 7.0, global type mappings are obsolete (but still supported) - NpgsqlDataSource is the recommended way to manage type mappings. Note This mapping method has been removed in Npgsql 7.0. Older versions of Npgsql supported configuring a type mapping on an individual connection, as follows: var conn = new NpgsqlConnection(...); conn.TypeMapper.UseGeoJson(); Reading and Writing Geometry Values When reading PostGIS values from the database, Npgsql will automatically return the appropriate GeoJSON types: Point, LineString, and so on. Npgsql will also automatically recognize GeoJSON's types in parameters, and will automatically send the corresponding PostGIS type to the database. The following code demonstrates a roundtrip of a GeoJSON Point to the database: conn.ExecuteNonQuery(\"CREATE TEMP TABLE data (geom GEOMETRY)\"); await using (var cmd = new NpgsqlCommand(\"INSERT INTO data (geom) VALUES ($1)\", conn)) { cmd.Parameters.Add(new() { Value = new Point(new Position(51.899523, -2.124156)) }); await cmd.ExecuteNonQueryAsync(); } await using (var cmd = new NpgsqlCommand(\"SELECT geom FROM data\", conn)) await using (var reader = await cmd.ExecuteReaderAsync()) { await reader.ReadAsync(); var point2 = reader.GetFieldValue<Point>(0); } You may also explicitly specify a parameter's type by setting NpgsqlDbType.Geometry. Geography (geodetic) Support PostGIS has two types: geometry (for Cartesian coordinates) and geography (for geodetic or spherical coordinates). You can read about the geometry/geography distinction in the PostGIS docs or in this blog post. In a nutshell, geography is much more accurate when doing calculations over long distances, but is more expensive computationally and supports only a small subset of the spatial operations supported by geometry. Npgsql uses the same GeoJSON types to represent both geometry and geography - the Point type represents a point in either Cartesian or geodetic space. You usually don't need to worry about this distinction because PostgreSQL will usually cast types back and forth as needed. However, it's worth noting that Npgsql sends Cartesian geometry by default, because that's the usual requirement. You have the option of telling Npgsql to send geography instead by specifying NpgsqlDbType.Geography: using (var cmd = new NpgsqlCommand(\"INSERT INTO data (geog) VALUES ($1)\", conn)) { cmd.Parameters.Add(new() { Value = point, NpgsqlDbType = NpgsqlDbType.Geography }); await cmd.ExecuteNonQueryAsync(); } If you prefer to use geography everywhere by default, you can also specify that when setting up the plugin: dataSourceBuilder.UseGeoJson(geographyAsDefault: true);"
  },
  "doc/conceptual/Npgsql/types/json.html": {
    "href": "doc/conceptual/Npgsql/types/json.html",
    "title": "Mapping JSON | Npgsql Documentation",
    "keywords": "Mapping JSON Note If you're using EF Core, please read the page on JSON support in the EF provider. EF has specialized support for JSON beyond what is supported at the lower-level Npgsql layer. PostgreSQL has rich, built-in support for storing JSON columns and efficiently performing complex queries operations on them. Newcomers can read more about the PostgreSQL support on the JSON types page, and on the functions and operators page. Note that the below mapping mechanisms support both the jsonb and json types, although the former is almost always preferred for efficiency and functionality reasons. Npgsql allows you to map PostgreSQL JSON columns in three different ways: As simple strings As strongly-typed user-defined types (POCOs) As System.Text.Json DOM types (JsonDocument or JsonElement, see docs) High-performance JSON parsing with Utf8JsonReader Newtonsoft Json.NET String mapping The simplest form of mapping to JSON is as a regular .NET string: // Write a string to a json column: await using var command1 = new NpgsqlCommand(\"INSERT INTO test (data) VALUES ($1)\", conn) { Parameters = { new() { Value = \"\"\"{ \"a\": 8, \"b\": 9 }\"\"\", NpgsqlDbType = NpgsqlDbType.Jsonb } } }; await command1.ExecuteNonQueryAsync(); // Read jsonb data as a string: await using var command2 = new NpgsqlCommand(\"SELECT data FROM test\", conn); await using var reader = await command2.ExecuteReaderAsync(); while (await reader.ReadAsync()) { Console.WriteLine(reader.GetString(0)); } Note Note that when writing a string parameter as jsonb, you must specify NpgsqlDbType.Jsonb, otherwise Npgsql sends a text parameter which is incompatible with JSON. With this mapping style, you're fully responsible for serializing/deserializing the JSON data yourself (e.g. with System.Text.Json) - Npgsql simply passes your strings to and from PostgreSQL. POCO mapping Warning As of Npgsql 8.0, POCO mapping is incompatible with NativeAOT. We plan to improve this, please upvote this issue if you're interested. If your column JSON contains documents with a stable schema, you can map them to your own .NET types (or POCOs). The provider will use System.Text.Json APIs under the hood to serialize instances of your types to JSON documents before sending them to the database, and to deserialize documents coming back from the database. This effectively allows mapping an arbitrary .NET type - or object graph - to a single column in the database. Starting with Npgsql 8.0, to use this feature, you must first enable it by calling <xref:Npgsql.INpgsqlTypeMapperExtensions.EnableDynamicJson> on your <xref:Npgsql.NpgsqlDataSourceBuilder>, or, if you're not yet using data sources, on NpgsqlConnection.GlobalTypeMapper: NpgsqlDataSource Global mapping Note NpgsqlDataSource was introduced in Npgsql 7.0, and is the recommended way to manage type mapping. If you're using an older version, see the other methods. var dataSourceBuilder = new NpgsqlDataSourceBuilder(...); dataSourceBuilder.EnableDynamicJson(); await using var dataSource = dataSourceBuilder.Build(); If you're not yet using NpgsqlDataSource, you can configure mappings globally for all connections in your application: NpgsqlConnection.GlobalTypeMapper.EnableDynamicJson(); For this to work, you must place this code at the beginning of your application, before any other Npgsql API is called. Note that in Npgsql 7.0, global type mappings are obsolete (but still supported) - NpgsqlDataSource is the recommended way to manage type mappings. Once you've enabled the feature, you can simply read and write instances of your POCOs directly; when writing, specify NpgsqlDbType.Jsonb to let Npgsql know you intend for it to get sent as JSON data: // Write a POCO to a jsonb column: var myPoco1 = new MyPoco { A = 8, B = 9 }; await using var command1 = new NpgsqlCommand(\"INSERT INTO test (data) VALUES ($1)\", conn) { Parameters = { new() { Value = myPoco1, NpgsqlDbType = NpgsqlDbType.Jsonb } } }; await command1.ExecuteNonQueryAsync(); // Read jsonb data as a POCO: await using var command2 = new NpgsqlCommand(\"SELECT data FROM test\", conn); await using var reader = await command2.ExecuteReaderAsync(); while (await reader.ReadAsync()) { var myPoco2 = reader.GetFieldValue<MyPoco>(0); Console.WriteLine(myPoco2.A); } class MyPoco { public int A { get; set; } public int B { get; set; } } This mapping method is quite powerful, allowing you to read and write nested graphs of objects and arrays to PostgreSQL without having to deal with serialization yourself. System.Text.Json DOM types There are cases in which mapping JSON data to POCOs isn't appropriate; for example, your JSON column may not contain a fixed schema and must be inspected to see what it contains; for these cases, Npgsql supports mapping JSON data to JsonDocument or JsonElement (see docs): var jsonDocument = JsonDocument.Parse(\"\"\"{ \"a\": 8, \"b\": 9 }\"\"\"); // Write a JsonDocument: await using var command1 = new NpgsqlCommand(\"INSERT INTO test (data) VALUES ($1)\", conn) { Parameters = { new() { Value = jsonDocument } } }; await command1.ExecuteNonQueryAsync(); // Read jsonb data as a JsonDocument: await using var command2 = new NpgsqlCommand(\"SELECT data FROM test\", conn); await using var reader = await command2.ExecuteReaderAsync(); while (await reader.ReadAsync()) { var document = reader.GetFieldValue<JsonDocument>(0); Console.WriteLine(document.RootElement.GetProperty(\"a\").GetInt32()); } High-performance JSON parsing with Utf8JsonReader If you're writing a very performance-sensitive application, using System.Text.Json to deserialize to POCOs or JsonDocument may incur too much overhead. If that's the case, you can use System.Text.Json's Utf8JsonReader to parse JSON data from the database. Utf8JsonReader provides a low-level, forward-only API to parse the JSON data, one token at a time. Utf8JsonReader requires JSON data as raw, UTF8-encoded binary data; fortunately, Npgsql allows reading jsonb as binary data, and if your PostgreSQL client_encoding is set to UTF8 (the default), you can feed data directly from PostgreSQL to Utf8JsonReader: await using var command2 = new NpgsqlCommand(\"SELECT data FROM test\", conn); await using var reader = await command2.ExecuteReaderAsync(); while (await reader.ReadAsync()) { ParseJson(reader.GetFieldValue<byte[]>(0)); } void ParseJson(byte[] utf8Data) { var jsonReader = new Utf8JsonReader(utf8Data); // ... parse the data with jsonReader } Note that the above works well for small JSON columns; if you have large columns (above ~8k), consider streaming the JSON data instead. This can be done by passing CommandBehavior.SequentialAccess to ExecuteReaderAsync, and then calling reader.GetStream() on NpgsqlDataReader instead of GetFieldValue<byte[]>. To process streaming data with Utf8JsonReader, see these docs. Newtonsoft.JSON System.Text.Json is the built-in, standard way to handle JSON in modern .NET. However, some users still prefer using Newtonoft Json.NET, and Npgsql includes support for that. To use Json.NET, add the Npgsql.Json.NET package to your project, and enable the plugin as follows: NpgsqlDataSource Global mapping Note NpgsqlDataSource was introduced in Npgsql 7.0, and is the recommended way to manage type mapping. If you're using an older version, see the other methods. var dataSourceBuilder = new NpgsqlDataSourceBuilder(...); dataSourceBuilder.UseJsonNet(); await using var dataSource = dataSourceBuilder.Build(); If you're using an older version of Npgsql which doesn't yet support NpgsqlDataSource, you can configure mappings globally for all connections in your application: NpgsqlConnection.GlobalTypeMapper.UseJsonNet(); For this to work, you must place this code at the beginning of your application, before any other Npgsql API is called. Note that in Npgsql 7.0, global type mappings are obsolete (but still supported) - NpgsqlDataSource is the recommended way to manage type mappings. Once you've enabled the feature, you can simply read and write instances of your POCOs directly; when writing, specify NpgsqlDbType.Jsonb to let Npgsql know you intend for it to get sent as JSON data: // Write a POCO to a jsonb column: var myPoco1 = new MyPoco { A = 8, B = 9 }; await using var command1 = new NpgsqlCommand(\"INSERT INTO test (data) VALUES ($1)\", conn) { Parameters = { new() { Value = myPoco1, NpgsqlDbType = NpgsqlDbType.Jsonb } } }; await command1.ExecuteNonQueryAsync(); // Read jsonb data as a POCO: await using var command2 = new NpgsqlCommand(\"SELECT data FROM test\", conn); await using var reader = await command2.ExecuteReaderAsync(); while (await reader.ReadAsync()) { var myPoco2 = reader.GetFieldValue<MyPoco>(0); Console.WriteLine(myPoco2.A); } class MyPoco { public int A { get; set; } public int B { get; set; } } The plugin also allows you to read JObject/JArray for weakly-typed DOM mapping."
  },
  "doc/conceptual/Npgsql/types/nodatime.html": {
    "href": "doc/conceptual/Npgsql/types/nodatime.html",
    "title": "NodaTime Type Plugin | Npgsql Documentation",
    "keywords": "NodaTime Type Plugin Npgsql provides a plugin that allows mapping the NodaTime date/time library; this is the recommended way to interact with PostgreSQL date/time types, rather than the built-in .NET types. What is NodaTime By default, the PostgreSQL date/time types are mapped to the built-in .NET types (DateTime, TimeSpan). Unfortunately, these built-in types are flawed in many ways. The NodaTime library was created to solve many of these problems, and if your application handles dates and times in anything but the most basic way, you should consider using it. To learn more read this blog post by Jon Skeet. Beyond NodaTime's general advantages, some specific advantages NodaTime for PostgreSQL date/time mapping include: NodaTime's types map very cleanly to the PostgreSQL types. For example Instant corresponds to timestamptz, and LocalDateTime corresponds to timestamp without time zone. The BCL's DateTime can correspond to both, depending on its type; this can create confusion and errors. Period is much more suitable for mapping PostgreSQL interval than TimeSpan. NodaTime types can fully represent PostgreSQL's microsecond precision, and can represent dates outside the BCL's date limit (1AD-9999AD). Setup To avoid forcing a dependency on the NetTopologySuite library for users not using spatial, NodaTime support is delivered as a separate plugin. To use the plugin, simply add a dependency on Npgsql.NodaTime and set it up in one of the following ways: NpgsqlDataSource Global mapping Connection mapping Note NpgsqlDataSource was introduced in Npgsql 7.0, and is the recommended way to manage type mapping. If you're using an older version, see the other methods. var dataSourceBuilder = new NpgsqlDataSourceBuilder(...); dataSourceBuilder.UseNodaTime(); await using var dataSource = dataSourceBuilder.Build(); If you're using an older version of Npgsql which doesn't yet support NpgsqlDataSource, you can configure mappings globally for all connections in your application: NpgsqlConnection.GlobalTypeMapper.UseNodaTime(); For this to work, you must place this code at the beginning of your application, before any other Npgsql API is called. Note that in Npgsql 7.0, global type mappings are obsolete (but still supported) - NpgsqlDataSource is the recommended way to manage type mappings. Note This mapping method has been removed in Npgsql 7.0. Older versions of Npgsql supported configuring a type mapping on an individual connection, as follows: var conn = new NpgsqlConnection(...); conn.TypeMapper.UseNodaTime(); Reading and Writing Values Once the plugin is set up, you can transparently read and write NodaTime objects: // Write NodaTime Instant to PostgreSQL \"timestamp with time zone\" (UTC) await using (var cmd = new NpgsqlCommand(@\"INSERT INTO mytable (my_timestamptz) VALUES ($1)\", conn)) { cmd.Parameters.Add(new() { Value = Instant.FromUtc(2011, 1, 1, 10, 30) }); await cmd.ExecuteNonQueryAsync(); } // Read timestamp back from the database as an Instant await using (var cmd = new NpgsqlCommand(@\"SELECT my_timestamptz FROM mytable\", conn)) await using (var reader = await cmd.ExecuteReaderAsync()) { await reader.ReadAsync(); var instant = reader.GetFieldValue<Instant>(0); } Mapping Table Warning A common mistake is for users to think that the PostgreSQL timestamp with time zone type stores the timezone in the database. This is not the case: only a UTC timestamp is stored. There is no single PostgreSQL type that stores both a date/time and a timezone, similar to .NET DateTimeOffset. To store a timezone in the database, add a separate text column containing the timezone ID. PostgreSQL Type Default NodaTime Type Additional NodaTime Type Notes timestamp with time zone Instant ZonedDateTime1, OffsetDateTime1 A UTC timestamp in the database. Only UTC ZonedDateTime and OffsetDateTime are supported. timestamp without time zone LocalDateTime2 A timestamp in an unknown or implicit time zone. date LocalDate A simple date with no timezone or offset information. time without time zone LocalTime A simple time-of-day, with no timezone or offset information. time with time zone OffsetTime A type that stores a time and an offset. It's use is generally discouraged. interval Period Duration An interval of time, from sub-second units to years. NodaTime Duration is supported for intervals with days and smaller, but not with years or months (as these have no absolute duration). Period can be used with any interval unit. tstzrange Interval NpgsqlRange<Instant> etc. An interval between two instants in time (start and end). tsrange NpgsqlRange<LocalDateTime> An interval between two timestamps in an unknown or implicit time zone. daterange DateInterval NpgsqlRange<LocalDate> etc. An interval between two dates. 1 In versions prior to 6.0 (or when Npgsql.EnableLegacyTimestampBehavior is enabled), writing or reading ZonedDateTime or OffsetDateTime automatically converted to or from UTC. See the breaking change note for more info. 2 In versions prior to 6.0 (or when Npgsql.EnableLegacyTimestampBehavior is enabled), timestamp without time zone was mapped to Instant by default, instead of LocalDateTime. See the breaking change note for more info. Infinity values PostgreSQL supports the special values -infinity and infinity for the timestamp and date types (see docs); these can be useful to represent a value which is earlier or later than any other value. Starting with Npgsql 6.0, these special values are mapped to the MinValue and MaxValue value on the corresponding .NET types (Instant and LocalDate). To opt out of this behavior, set the following AppContext switch at the start of your application: AppContext.SetSwitch(\"Npgsql.DisableDateTimeInfinityConversions\", true); Note: in versions prior to 6.0, the connection string parameter Convert Infinity DateTime could be used to opt into these infinity conversions. That connection string parameter has been removed."
  },
  "doc/conceptual/Npgsql/types/nts.html": {
    "href": "doc/conceptual/Npgsql/types/nts.html",
    "title": "PostGIS/NetTopologySuite Type Plugin | Npgsql Documentation",
    "keywords": "PostGIS/NetTopologySuite Type Plugin PostgreSQL supports spatial data and operations via the PostGIS extension, which is a mature and feature-rich database spatial implementation. .NET doesn't provide a standard spatial library, but NetTopologySuite is a leading spatial library. Npgsql has a plugin which allows you to map the NTS types PostGIS columns, and even translate many useful spatial operations to SQL. This is the recommended way to interact with spatial types in Npgsql. PostgreSQL provides support for spatial types (geometry/geography) via the powerful PostGIS extension; this allows you to store points and other spatial constructs in the database, and efficiently perform operations and searches on them. Npgsql supports the PostGIS types via NetTopologySuite, which is the leading spatial library in the .NET world: the NTS types can be read and written directly to their corresponding PostGIS types. This is the recommended way to work with spatial types in Npgsql. Setup To avoid forcing a dependency on the NetTopologySuite library for users not using spatial, NTS support is delivered as a separate plugin. To use the plugin, simply add a dependency on Npgsql.NetTopologySuite and set it up in one of the following ways: NpgsqlDataSource Global mapping Connection mapping Note NpgsqlDataSource was introduced in Npgsql 7.0, and is the recommended way to manage type mapping. If you're using an older version, see the other methods. var dataSourceBuilder = new NpgsqlDataSourceBuilder(...); dataSourceBuilder.UseNetTopologySuite(); await using var dataSource = dataSourceBuilder.Build(); If you're using an older version of Npgsql which doesn't yet support NpgsqlDataSource, you can configure mappings globally for all connections in your application: NpgsqlConnection.GlobalTypeMapper.UseNetTopologySuite(); For this to work, you must place this code at the beginning of your application, before any other Npgsql API is called. Note that in Npgsql 7.0, global type mappings are obsolete (but still supported) - NpgsqlDataSource is the recommended way to manage type mappings. Note This mapping method has been removed in Npgsql 7.0. Older versions of Npgsql supported configuring a type mapping on an individual connection, as follows: var conn = new NpgsqlConnection(...); conn.TypeMapper.UseNetTopologySuite(); By default the plugin handles only ordinates provided by the DefaultCoordinateSequenceFactory of GeometryServiceProvider.Instance. If GeometryServiceProvider is initialized automatically the X and Y ordinates are handled. To change the behavior specify the handleOrdinates parameter like in the following example: dataSourceBuilder.UseNetTopologySuite(handleOrdinates: Ordinates.XYZ); To process the M ordinate, you must initialize GeometryServiceProvider.Instance to a new NtsGeometryServices instance with coordinateSequenceFactory set to a DotSpatialAffineCoordinateSequenceFactory. Or you can specify the factory when calling UseNetTopologySuite. // Place this at the beginning of your program to use the specified settings everywhere (recommended) GeometryServiceProvider.Instance = new NtsGeometryServices( new DotSpatialAffineCoordinateSequenceFactory(Ordinates.XYM), new PrecisionModel(PrecisionModels.Floating), -1); // Or specify settings for Npgsql only dataSourceBuilder.UseNetTopologySuite.UseNetTopologySuite( new DotSpatialAffineCoordinateSequenceFactory(Ordinates.XYM)); Reading and Writing Geometry Values When reading PostGIS values from the database, Npgsql will automatically return the appropriate NetTopologySuite types: Point, LineString, and so on. Npgsql will also automatically recognize NetTopologySuite's types in parameters, and will automatically send the corresponding PostGIS type to the database. The following code demonstrates a roundtrip of a NetTopologySuite Point to the database: await conn.ExecuteNonQueryAsync(\"CREATE TEMP TABLE data (geom GEOMETRY)\"); await using (var cmd = new NpgsqlCommand(\"INSERT INTO data (geom) VALUES ($1)\", conn)) { cmd.Parameters.Add(new() { Value = new Point(new Coordinate(1d, 1d)) }); await cmd.ExecuteNonQueryAsync(); } await using (var cmd = new NpgsqlCommand(\"SELECT geom FROM data\", conn)) await using (var reader = await cmd.ExecuteReaderAsync()) { await reader.ReadAsync(); var point = reader.GetFieldValue<Point>(0); } You may also explicitly specify a parameter's type by setting NpgsqlDbType.Geometry. Geography (geodetic) Support PostGIS has two types: geometry (for Cartesian coordinates) and geography (for geodetic or spherical coordinates). You can read about the geometry/geography distinction in the PostGIS docs or in this blog post. In a nutshell, geography is much more accurate when doing calculations over long distances, but is more expensive computationally and supports only a small subset of the spatial operations supported by geometry. Npgsql uses the same NetTopologySuite types to represent both geometry and geography - the Point type represents a point in either Cartesian or geodetic space. You usually don't need to worry about this distinction because PostgreSQL will usually cast types back and forth as needed. However, it's worth noting that Npgsql sends Cartesian geometry by default, because that's the usual requirement. You have the option of telling Npgsql to send geography instead by specifying NpgsqlDbType.Geography: await using (var cmd = new NpgsqlCommand(\"INSERT INTO data (geog) VALUES ($1)\", conn)) { cmd.Parameters.Add(new() { Value = point, NpgsqlDbType = NpgsqlDbType.Geography }); await cmd.ExecuteNonQueryAsync(); } If you prefer to use geography everywhere by default, you can also specify that when setting up the plugin: dataSourceBuilder.UseNetTopologySuite(geographyAsDefault: true);"
  },
  "doc/conceptual/Npgsql/wait.html": {
    "href": "doc/conceptual/Npgsql/wait.html",
    "title": "Waiting for Notifications | Npgsql Documentation",
    "keywords": "Waiting for Notifications Note: This functionality replaces Npgsql 3.0's \"Continuous processing mode\". PostgreSQL Asynchronous messages PostgreSQL has a feature whereby arbitrary notification messages can be sent between clients. For example, one client may wait until it is notified by another client of a task that it is supposed to perform. Notifications are, by their nature, asynchronous - they can arrive at any point. For more detail about this feature, see the PostgreSQL NOTIFY command. Some other asynchronous message types are notices (e.g. database shutdown imminent) and parameter changes, see the PostgreSQL protocol docs for more details. Note that despite the word \"asynchronous\", this page has nothing to do with ADO.NET async operations (e.g. ExecuteReaderAsync). Processing of Notifications Npgsql exposes notification messages via the Notification event on NpgsqlConnection. Since asynchronous notifications are rarely used and processing can be complex, Npgsql only processes notification messages as part of regular (synchronous) query interaction. That is, if an asynchronous notification is sent, Npgsql will only process it and emit an event to the user the next time a command is sent and processed. To receive notifications outside a synchronous request-response cycle, call NpgsqlConnection.Wait(). This will make your thread block until a single notification is received (note that a version with a timeout as well as an async version exist). Note that the notification is still delivered via the Notification event as before. var conn = new NpgsqlConnection(ConnectionString); conn.Open(); conn.Notification += (o, e) => Console.WriteLine(\"Received notification\"); using (var cmd = new NpgsqlCommand(\"LISTEN channel_name\", conn)) { cmd.ExecuteNonQuery(); } while (true) { conn.Wait(); // Thread will block here } Keepalive You may want to turn on keepalives."
  },
  "doc/dev/build-server.html": {
    "href": "doc/dev/build-server.html",
    "title": "Build Server Notes | Npgsql Documentation",
    "keywords": "This page describes the steps used to set up the Npgsql build server. If you're upgrading the TeamCity version, see \"Give agent service start/stop permissions\" below. Install all supported versions of the Postgresql backend At the time of writing, this means 9.1, 9.2, 9.3, 9.4, 9.5. They are configured on ports 5491, 5492, 5493, 5494, 5495. For SSPI/GSS tests, you need to set up a user with the same name as the user that will be running the tests (i.e. teamcity_agent). You must also add the following lines at the top of each PG's pg_hba.conf to set up SSPI/GSS for that user: host all teamcity_agent 127.0.0.1/32 sspi include_realm=0 host all teamcity_agent ::1/128 sspi include_realm=0 See this page on SSPI. Install a TeamCity-dedicated Postgresql cluster TeamCity itself requires an SQL database, but we don't want it to run in the same environment as that used for the unit tests. So choosing the latest stable Postgresql version (9.6 at time of writing), we create a new Postgresql cluster: initdb -U postgres -W c:\\dev\\TeamcityPostgresData Next we set up a Windows service that starts up the new cluster: pg_ctl register -N postgresql-9.6-teamcity -U teamcity -P <password> -D c:\\dev\\TeamcityPostgresData Finally, create a a user and database and point TeamCity to it. Install .NET SDKs for all supported .NET versions .NET 4.0 (Windows 7 SDK): http://www.microsoft.com/en-us/download/details.aspx?id=8279 .NET 4.5 (Windows 8 SDK): http://msdn.microsoft.com/en-us/windows/hardware/hh852363.aspx .NET 4.5.1 (Windows 8.1 SDK): http://msdn.microsoft.com/en-us/windows/hardware/bg162891.aspx While installing the SDK for .NET 4.0, I had this problem: http://support.microsoft.com/kb/2717426 Give agent service start/stop permissions When upgrading TeamCity, the agent needs to be able to stop and start the Windows service. This is how you can grant a normal user specific permissions on specific services: Download and install subinacl from http://www.microsoft.com/en-us/download/details.aspx?id=23510 cd C:\\Program Files (x86)\\Windows Resource Kits\\Tools\\ subinacl /service TCBuildAgent /grant=teamcity_agent=TO Update build status back in github Download the plugin from https://github.com/jonnyzzz/TeamCity.GitHub, get the ZIP Drop the ZIP in the TeamCity content dir's plugins subdir Add the Build Feature \"Report change status to GitHub\". Configure everything appropriately, and be sure the user you set up has push access to the repository! Install assorted dev utilities GitVersion (with Chocolatey) WiX toolset (v3.10.1 at time of writing) Install WiX WiX 3.10 has a dependency on .NET Framework 3.5, but there's some issue blocking its installation on Windows Server 2012 R2 (at least on Azure). A good workaround is to simply install via Powershell (Add-WindowsFeature NET-Framework-Core), see https://msdn.microsoft.com/en-us/library/dn169001(v=nav.70).aspx#InstallNET35. Note that ICE validation is disabled because apparently it requires an interactive account or admin privileges, which doesn't work in continuous integration."
  },
  "doc/dev/index.html": {
    "href": "doc/dev/index.html",
    "title": "| Npgsql Documentation",
    "keywords": "Tests We maintain a large regression test suite, if you're planning to submit code, please provide a test that reproduces the bug or tests your new feature. See this page for information on the Npgsql test suite. Build Server We have a TeamCity build server running continuous integration builds on commits pushed to our github repository. The Npgsql testsuite is executed over all officially supported PostgreSQL versions to catch errors as early as possible. CI NuGet packages are automatically pushed to our unstable feed at MyGet. For some information about the build server setup, see this page. Thanks to Dave Page at PostgreSQL for donating a VM for this! Release Checklist These are the steps needed to publish release 3.0.6: Merge --no-ff hotfix/3.0.6 into master Tag master with v3.0.6 Push both master and v3.0.6 to Github Wait for the build to complete In TeamCity, go to the artifacts for the build and download them all as a single ZIP Nuget push the packages Write release notes on npgsql.org, publish Create release on github, pointing to npgsql.org Upload MSI to the github release Delete hotfix/3.0.6 both locally and on github Create new branch hotfix/3.0.7 off of master, push to github Close the Github 3.0.6 milestone, create new 3.0.7 milestone Twitter Other stuff Emil compiled a list of PostgreSQL types and their wire representations."
  },
  "doc/dev/tests.html": {
    "href": "doc/dev/tests.html",
    "title": "Tests | Npgsql Documentation",
    "keywords": "Overview Npgsql comes with an extensive test suite to make sure no regressions occur. All tests are run on our build server on all supported .NET versions (including a recent version of mono) and all supported PostgreSQL backends. There is also a growing suite of speed tests to be able to measure performance. These tests are currently marked [Explicit] and aren't executed automatically. Simple setup The Npgsql test suite requires a PostgreSQL backend to test against. Simply use the latest version of PostgreSQL on your dev machine on the default port (5432). By default, all tests will be run using user npgsql_tests, and password npgsql_tests. Npgsql will automatically create a database called npgsql_tests and run its tests against this. To set this up, connect to PostgreSQL as the admin user as follows: psql -h localhost -U postgres <enter the admin password> create user npgsql_tests password 'npgsql_tests' superuser; And you're done. Superuser access is needed for some tests, e.g. loading the hstore extension, creating and dropping test databases in the Entity Framework tests..."
  },
  "doc/dev/types.html": {
    "href": "doc/dev/types.html",
    "title": "PostgreSQL Types | Npgsql Documentation",
    "keywords": "Overview The following are notes by Emil Lenngren on PostgreSQL wire representation of types: bool: text: t or f binary: a byte: 1 or 0 bytea: text: either \\x followed by hex-characters (lowercase by default), or plain characters, where non-printable characters (between 0x20 and 0x7e, inclusive) are written as \\nnn (octal) and \\ is written as \\\\ binary: the bytes as they are char: This type holds a single char/byte. (Not to be confused with bpchar (blank-padded char) which is PostgreSQL's alias to the SQL standard's char). The char may be the null-character text: the char as a byte, encoding seems to be ignored binary: the char as a byte name: A null-padded string of NAMEDATALEN (currently 64) bytes (the last byte must be a null-character). Used in pg catalog. text: the name as a string binary: the name as a string int2/int4/int8: text: text representation in base 10 binary: binary version of the integer int2vector: non-null elements, 0-indexed, 1-dim text: 1 2 3 4 binary: same as int2[] oidvector: non-null elements, 0-indexed, 1-dim text: 1 2 3 4 binary: same as oid[] regproc: internally just an OID (UInt32) text: -, name of procedure, or numeric if not found binary: only the OID in binary regprocedure/regoper/regoperator/regclass/regconfig/regdictionary: similar to regproc text: text: the string as it is binary: the string as it is oid: A 32-bit unsigned integer used for internal object identification. text: the text-representation of this integer in base 10 binary: the UInt32 tid: tuple id Internally a tuple of a BlockNumber (UInt32) and an OffsetNumber (UInt16) text: (blockNumber,offsetNumber) binary: the block number in binary followed by offset number in binary xid: transaction id Internally just a TransactionId (UInt32) text: the number binary: the number in binary cid: command id Internally just a CommandId (UInt32) text: the number binary: the number in binary json: json text: the json an text binary: the json as text jsonb: json internally stored in an efficient binary format text: the json as text binary: An Int32 (version number, currently 1), followed by data (currently just json as text) xml: Xml. It is probably most efficient to use the text format, especially when receiving from client. text: the xml as text (when sent from the server: encoding removed, when receiving: assuming database encoding) binary: the xml as text (when sent from the server: in the client's specified encoding, when receiving: figures out itself) pg_node_tree: used as type for the column typdefaultbin in pg_type does not accept input text: text binary: text smgr: storage manager can only have the value \"magnetic disk\" text: magnetic disk binary: not available point: A tuple of two float8 text: (x,y) The floats are interpreted with the C strtod function. The floats are written with the snprintf function, with %.*g format. NaN/-Inf/+Inf can be written, but not interpretability depends on platform. The extra_float_digits setting is honored. For linux, NaN, [+-]Infinity, [+-]Inf works, but not on Windows. Windows also have other output syntax for these special numbers. (1.#QNAN for example) binary: the two floats lseg: A tuple of two points text: [(x1,y1),(x2,y2)] see point for details binary: the four floats in the order x1, y1, x2, y2 path: A boolean whether the path is opened or closed + a vector of points. text: [(x1,y1),...] for open path and ((x1,y1),...) for closed paths. See point for details. binary: first a byte indicating open (0) or close (1), then the number of points (Int32), then a vector of points box: A tuple of two points. The coordinates will be reordered so that the first is the upper right and the second is the lower left. text: (x1,y1),(x2,y2) see point for details binary: the four floats in the order x1, y1, x2, y2 (doesn't really matter since they will be reordered) polygon: Same as path but with two differences: is always closed and internally stores the bounding box. text: same as closed path binary: the number of points (Int32), then a vector of points line (version 9.4): Ax + By + C = 0. Stored with three float8. Constraint: A and B must not both be zero (only checked on text input, not binary). text: {A,B,C} see point for details about the string representation of floats. Can also use the same input format as a path with two different points, representing the line between those. binary: the three floats circle: <(x,y),r> (center point and radius), stored with three float8. text: <(x,y),r> see point for details about the string representation of floats. binary: the three floats x, y, r in that order float4/float8: text: (leading/trailing whitespace is skipped) interpreted with the C strtod function, but since it has problems with NaN, [+-]Infinity, [+-]Inf, those strings are identified (case-insensitively) separately. when outputting: NaN, [+-]Infinity is treated separately, otherwise the string is printed with snprintf %.*g and the extra_float_digits setting is honored. binary: the float abstime: A unix timestamp stored as a 32-bit signed integer with seconds-precision (seconds since 1970-01-01 00:00:00), in UTC Has three special values: Invalid (2^31-1), infinity (2^31-3), -infinity (-2^31) text: same format as timestamptz, or \"invalid\", \"infinity\", \"-infinity\" binary: Int32 reltime: A time interval with seconds-precision (stored as an 32-bit signed integer) text: same as interval binary: Int32 tinterval: Consists of a status (Int32) and two abstimes. Status is valid (1) iff both abstimes are valid, else 0. Note that the docs incorrectly states that ' is used as quote instead of \" text: [\"<abstime>\" \"<abstime>\"] binary: Int32 (status), Int32 (abstime 1), Int32 (abstime 2) unknown: text: text binary: text money: A 64-bit signed integer. For example, $123.45 is stored as the integer 12345. Number of fraction digits is locale-dependent. text: a locale-depedent string binary: the raw 64-bit integer macaddr: 6 bytes text: the 6 bytes in hex (always two characters per byte) separated by : binary: the 6 bytes appearing in the same order as when written in text inet/cidr: Struct of Family (byte: ipv4=2, ipv6=3), Netmask (byte with number of bits in the netmask), Ipaddr bytes (16) Text: The IP-address in text format and /netmask. /netmask is omitted in inet if the netmask is the whole address. Binary: family byte, netmask byte, byte (cidr=1, inet=0), number of bytes in address, bytes of the address aclitem: Access list item used in pg_class Text: Something like postgres=arwdDxt/postgres Binary: not available bpchar: Blank-padded char. The type modifier is used to blank-pad the input. text: text binary: text varchar: Variable-length char. The type modifier is used to check the input's length. text: text binary: text date: A signed 32-bit integer of a date. 0 = 2000-01-01. Infinity: INT_MAX, -Infinity: INT_MIN Text: Date only using the specified date style Binary: Int32 time: A signed 64-bit integer representing microseconds from 00:00:00.000000. (Legacy uses 64-bit float). Negative values are not allowed. Max value is 24:00:00.000000. text: hh:mm:ss or hh:mm:ss.ffffff where the fraction part is between 1 and 6 digits (trailing zeros are not written) binary: the 64-bit integer timetz: A struct of Time: A signed 64-bit integer representing microseconds from 00:00:00.000000. (Legacy uses 64-bit float). Negative values are not allowed. Max value is 24:00:00.000000. Zone: A signed 32-bit integer representing the zone (in seconds). Note that the sign is inverted. So GMT+1h is stored as -1h. text: hh:mm:ss or hh:mm:ss.ffffff where the fraction part is between 1 and 6 digits (trailing zeros are not written) binary: the 64-bit integer followed by the 32-bit integer timestamp: A signed 64-bit integer representing microseconds from 2000-01-01 00:00:00.000000 Infinity is LONG_MAX and -Infinity is LONG_MIN (Infinity would be 294277-01-09 04:00:54.775807) Earliest possible timestamp is 4714-11-24 00:00:00 BC. Even earlier would be possible, but due to internal calculations those are forbidden. text: dependent on date style binary: the 64-bit integer timestamptz: A signed 64-bit integer representing microseconds from 2000-01-01 00:00:00.000000 UTC. (Time zone is not stored). Infinity is LONG_MAX and -Infinity is LONG_MIN text: first converted to the time zone in the db settings, then printed according to the date style binary: the 64-bit integer interval: A struct of Time (Int64): all time units other than days, months and years (microseconds) Day (Int32): days, after time for alignment Month (Int32): months and years, after time for alignment text: Style dependent, but for example: \"-11 mons +15435 days -11111111:53:00\" binary: all fields in the struct bit/varbit: First a signed 32-bit integer containing the number of bits (negative length not allowed). Then all the bits in big end first. So a varbit of length 1 has the first (and only) byte set to either 0x80 or 0x00. Last byte is assumed (and is automatically zero-padded in recv) to be zero-padded. text: when sending from backend: all the bits, written with 1s and 0s. when receiving from client: (optionally b or B followed by) all the bits as 1s and 0s, or a x or X followed by hexadecimal digits (upper- or lowercase), big endian first. binary: the 32-bit length followed by the bytes containing the bits numeric: A variable-length numeric value, can be negative. text: NaN or first - if it is negative, then the digits with . as decimal separator binary: first a header of 4 16-bit signed integers: number of digits in the digits array that follows (can be 0, but not negative), weight of the first digit (10000^weight), can be both negative, positive or 0, sign: negative=0x4000, positive=0x0000, NaN=0xC000 dscale: number of digits (in base 10) to print after the decimal separator then the array of digits: The digits are stored in base 10000, where each digit is a 16-bit integer. Trailing zeros are not stored in this array, to save space. The digits are stored such that, if written as base 10000, the decimal separator can be inserted between two digits in base 10000, i.e. when this is to be printed in base 10, only the first digit in base 10000 can (possibly) be printed with less than 4 characters. Note that this does not apply for the digits after the decimal separator; the digits should be printed out in chunks of 4 characters and then truncated with the given dscale. refcursor: uses the same routines as text record: Describes a tuple. Is also the \"base class\" for composite types (i.e. it uses the same i/o functions). text: ( followed by a list of comma-separated text-encoded values followed by ). Empty element means null. Quoted with \" and \" if necessary. \" is escaped with \"\" and \\ is escaped with \\\\ (this differs from arrays where \" is escaped with \\\"). Must be quoted if it is an empty string or contains one of \"\\,() or a space. binary: First a 32-bit integer with the number of columns, then for each column: An OID indicating the type of the column The length of the column (32-bit integer), or -1 if null The column data encoded as binary cstring: text/binary: all characters are sent without the trailing null-character void: Used for example as return value in SELECT * FROM func_returning_void() text: an empty string binary: zero bytes uuid: A 16-byte uuid. text: group of 8, 4, 4, 4, 12 hexadecimal lower-case characters, separated by -. The first byte is written first. It is allowed to surround it with {}. binary: the 16 bytes txid_snapshot: (txid is a UInt64) A struct of UInt32 nxip (size of the xip array) txid xmin (no values in xip is smaller than this) txid xmax (no values in xip is larger than or equal this) txid[] xip (is ordered in ascending order) text: xmin:xmax:1,2,3,4 binary: all fields in the structure tsvector: Used for text searching. Example of tsvector: 'a':1,6,10 'on':5 'and':8 'ate':9A 'cat':3 'fat':2,11 'mat':7 'rat':12 'sat':4 Max length for each lexeme string is 2046 bytes (excluding the trailing null-char) The words are sorted when parsed, and only written once. Positions are also sorted and only written once. For some reason, the unique check does not seem to be made for binary input, only text input... text: As seen above. ' is escaped with '' and \\ is escaped with \\\\. binary: UInt32 number of lexemes for each lexeme: lexeme text in client encoding, null-terminated UInt16 number of positions for each position: UInt16 WordEntryPos, where the most significant 2 bits is weight, and the 14 least significant bits is pos (can't be 0). Weights 3,2,1,0 represent A,B,C,D tsquery: A tree with operands and operators (&, |, !). Operands are strings, with optional weight (bitmask of ABCD) and prefix search (yes/no, written with *). text: the tree written in infix notation. Example: ( 'abc':*B | 'def' ) & !'ghi' binary: the tree written in prefix notation: First the number of tokens (a token is an operand or an operator). For each token: UInt8 type (1 = val, 2 = oper) followed by For val: UInt8 weight + UInt8 prefix (1 = yes / 0 = no) + null-terminated string, For oper: UInt8 oper (1 = not, 2 = and, 3 = or, 4 = phrase). In case of phrase oper code, an additional UInt16 field is sent (distance value of operator). Default is 1 for <->, otherwise the n value in '<n>'. enum: Simple text gtsvector: GiST for tsvector. Probably internal type. int4range/numrange/tsrange/tstzrange/daterange/int8range and user-defined range types: /* A range's flags byte contains these bits: */ #define RANGE_EMPTY 0x01 /* range is empty */ #define RANGE_LB_INC 0x02 /* lower bound is inclusive */ #define RANGE_UB_INC 0x04 /* upper bound is inclusive */ #define RANGE_LB_INF 0x08 /* lower bound is -infinity */ #define RANGE_UB_INF 0x10 /* upper bound is +infinity */ #define RANGE_LB_NULL 0x20 /* lower bound is null (NOT USED) */ #define RANGE_UB_NULL 0x40 /* upper bound is null (NOT USED) */ #define RANGE_CONTAIN_EMPTY 0x80/* marks a GiST internal-page entry whose * subtree contains some empty ranges */ A range has no lower bound if any of RANGE_EMPTY, RANGE_LB_INF (or RANGE_LB_NULL, not used anymore) is set. The same applies for upper bounds. text: A range with RANGE_EMPTY is just written as the string \"empty\". Inclusive bounds are written with [ and ], else ( and ) is used. The two values are comma-separated. Missing bounds are written as an empty string (without quotes). Each value is quoted with \" if necessary. Quotes are necessary if the string is either the empty string or contains \"\\,()[] or spaces. \" is escaped with \"\" and \\ is escaped with \\\\. Example: [18,21] binary: First the flag byte. Then, if has lower bound: 32-bit length + binary-encoded data. Then, if has upper bound: 32-bit length + binary-encoded data. hstore: Key/value-store. Both keys and values are strings. text: Comma-space separated string, where each item is written as \"key\"=>\"value\" or \"key\"=>NULL. \" and \\ are escaped as \\\" and \\\\. Example: \"a\"=>\"b\", \"c\"=>NULL, \"d\"=>\"q\" binary: Int32 count for each item: Int32 keylen string of the key (not null-terminated) Int32 length of item (or -1 if null) the item as a string ghstore: internal type for indexing hstore domain types: mapped types used in information_schema: cardinal_number: int4 (must be nonnegative or null) character_data: varchar sql_identifier: varchar time_stamp: timestamptz yes_or_no: varchar(3) (must be \"YES\" or \"NO\" or null) intnotnull: when an int4 is cast to this type, it is checked that the int4 is not null, but it still returns an int4 and not intnotnull..."
  },
  "doc/index.html": {
    "href": "doc/index.html",
    "title": "Npgsql - .NET Access to PostgreSQL | Npgsql Documentation",
    "keywords": "Npgsql - .NET Access to PostgreSQL About Npgsql is an open source ADO.NET Data Provider for PostgreSQL, it allows programs written in C#, Visual Basic, F# to access the PostgreSQL database server. It is implemented in 100% C# code, is free and is open source. An Entity Framework Core provider is also available, and exposes some features unique to the PostgreSQL database to EF Core users. Finally, a legacy Entity Framework 6.x (non-Core) provider is also available, but is no longer being actively maintained. Getting Help The best way to get help for Npgsql is to post a question to Stack Overflow and tag it with the npgsql tag. If you think you've encountered a bug or want to request a feature, open an issue in the appropriate project's github repository. License Npgsql is licensed under the PostgreSQL License, a liberal OSI-approved open source license. Contributors Current active contributors to Npgsql are: Shay Rojansky (@roji) Nikita Kazmin (@vonzshik) Nino Floris (@NinoFloris) Brar Piening (@Brar) Past contributors to Npgsql: Emmanuel Andr Jon Asher Raif Atef Josh Cooley Yoh Deadfall Austin Drenski Francisco Figueiredo Jr. (@franciscojunior) Npgsql creator and former lead developer for versions 0.xx, 1.xx and 2.xx Federico Di Gregorio Jon Hanna Emil Lenngren Chris Morgan Dave Page Glen Parker Hiroshi Saito Kenji Uno Warcha Thanks A special thanks to Jetbrains for donating licenses to the project."
  }
}